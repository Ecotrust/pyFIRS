{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/storage/anaconda3/envs/pyFIRS/lib/python3.6/site-packages/tqdm/autonotebook/__init__.py:14: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  \" (e.g. in jupyter console)\", TqdmExperimentalWarning)\n"
     ]
    }
   ],
   "source": [
    "import os, shutil, glob, platform, subprocess\n",
    "import geopandas as gpd, pandas as pd\n",
    "import rasterio\n",
    "from matplotlib import pyplot as plt\n",
    "from pyFIRS.wrappers import lastools, fusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyFIRS.utils import clean_dir, clean_buffer_polys, clip_tile_from_shp, convert_project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask.distributed import Client, progress, LocalCluster\n",
    "from dask import delayed\n",
    "import dask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting up parallel computing using `dask.distributed`\n",
    "`LAStools` offers native multi-core processing as an optional argument (`cores`) supplied to its command-line tools. `FUSION` command line tools do not. To enable parallel processing of `FUSION` commands, we'll use `dask.distributed` to schedule the processing of tiles in asynchronous parallel batches. This approach also offers us the ability to track progress using a progress bar.\n",
    "\n",
    "You'll first need to launch a parallel computing cluster. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster=LocalCluster(scheduler_port=7001, diagnostics_port=7002)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = Client(cluster)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point, you should also be able to view an interactive dashboard on port 7002. If you're executing this on a remote server, you'll need to set up port forward so you can view the dashboard on your local machine's browser. Once you've done that, or if you're processing on your own machine, you can view the dashboard at [http://localhost:7002/status](http://localhost:7002/status)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Enough already, let's get to work with some lidar data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll define where we can find the binary executables for LAStools and FUSION command line tools."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "las = lastools.useLAStools('/storage/lidar/LAStools/bin')\n",
    "fus = fusion.useFUSION('/storage/lidar/FUSION/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll create working directories for raw (imported with modest clean-up from source files), interim, and processed data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# where the raw lidar data is currently stored\n",
    "src_tiles = '/storage/lidar/Swinomish_Lidar_2016/source/*.laz'\n",
    "workdir = os.path.abspath('/storage/lidar/Swinomish_Lidar_2016')\n",
    "\n",
    "# define data handling directories\n",
    "raw, interim, processed = os.path.join(workdir,'raw'), os.path.join(workdir,'interim'), os.path.join(workdir,'processed')\n",
    "\n",
    "# push our working directories and wrapper classes to the workers on the cluster as well\n",
    "c.scatter([raw, interim, processed, las, fus], broadcast=True);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wine_prefix = ['/storage/wine/.wine-{}'.format(x) for x in range(len(c.ncores()))]\n",
    "# c.scatter(wine_prefix, broadcast=False);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Get the raw data into our working directory\n",
    "First, move the tiles over to our working directory.\n",
    "\n",
    "When we define functions using the `dask.delayed` decorator, the function will have 'lazy' instead of 'eager' execution. That means we can map the function to a list of inputs and it will not execute for any of them until we ask for results to be computed. When we use the `compute()` method for the client managing the scheduler that sends jobs to the workers, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e148e53ad29c4468a192f0d5803dab2b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "@dask.delayed\n",
    "def import_tiles(infile): # the function we'll map to a list of inputs\n",
    "    return las.las2las(i=infile,\n",
    "                       odir=raw,\n",
    "                       drop_withheld=True, # drop any points flagged as withheld by vendor\n",
    "                       drop_class=(7,18), # drop any points classified as noise by vendor\n",
    "                       olaz=True)\n",
    "\n",
    "infiles = glob.glob(src_tiles) # input files we'll map the function to\n",
    "imp_results = c.compute([import_tiles(file) for file in infiles]) # begin computation\n",
    "progress(imp_results) # monitor progress"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, create spatial indexes for the input files to allow fast spatial queries (which we'll use for adding buffers)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "45aa11d1af98402f94f3c4f52bbad9b8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "@dask.delayed\n",
    "def make_index(infile): # the function we'll map to a list of inputs\n",
    "    return las.lasindex(i=infile) \n",
    "\n",
    "infiles = glob.glob(os.path.join(raw, '*.laz'))\n",
    "index_results = c.compute([make_index(file) for file in infiles])\n",
    "progress(index_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Retile the data to add buffers for avoiding edge effects during processing.\n",
    "\n",
    "In practice, executing the `lastile` command on individual tiles in parallel is likely to corrupt your output files. I suspect this is because the dynamic re-tiling of input files means that many output tiles are likely to require inputs from multiple input files, and that parallel processing outside of LAStools may result in collisions writing data from multiple inputs to these output tiles. So, for this case, we'll let `lastile` handle the parallelism under the hood. We won't have a progress bar, but this shouldn't take more than 5-10 minutes.\n",
    "\n",
    "**THERE ARE ARGUMENTS IN THE FOLLOWING COMMAND THAT DEPEND UPON THE UNITS OF THE DATA.**\n",
    "\n",
    "The workflow demonstrated here is working in units of US feet on a dataset in Washington State Plane (South). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 9s, sys: 12.3 s, total: 1min 22s\n",
      "Wall time: 3min 46s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CompletedProcess(args=['wine', '/storage/lidar/LAStools/bin/lastile.exe', '-i', '/storage/lidar/Swinomish_Lidar_2016/raw/*.laz', '-tile_size', '4000', '-buffer', '100', '-flag_as_withheld', '-olaz', '-odir', '/storage/lidar/Swinomish_Lidar_2016/interim/retiled', '-cores', '32'], returncode=0, stdout=b'', stderr=b'removing /storage/lidar/Swinomish_Lidar_2016/interim/retiled\\\\1128000_1112000.laz\\r\\nWARNING: written 0 points but expected 0 points\\r\\nremoving /storage/lidar/Swinomish_Lidar_2016/interim/retiled\\\\1132000_1112000.laz\\r\\nWARNING: written 0 points but expected 0 points\\r\\nremoving /storage/lidar/Swinomish_Lidar_2016/interim/retiled\\\\1136000_1112000.laz\\r\\nWARNING: written 0 points but expected 0 points\\r\\nremoving /storage/lidar/Swinomish_Lidar_2016/interim/retiled\\\\1140000_1112000.laz\\r\\nWARNING: written 0 points but expected 0 points\\r\\nremoving /storage/lidar/Swinomish_Lidar_2016/interim/retiled\\\\1128000_1116000.laz\\r\\nWARNING: written 0 points but expected 0 points\\r\\nremoving /storage/lidar/Swinomish_Lidar_2016/interim/retiled\\\\1132000_1116000.laz\\r\\nWARNING: written 0 points but expected 0 points\\r\\nremoving /storage/lidar/Swinomish_Lidar_2016/interim/retiled\\\\1128000_1120000.laz\\r\\nWARNING: written 0 points but expected 0 points\\r\\nremoving /storage/lidar/Swinomish_Lidar_2016/interim/retiled\\\\1136000_1116000.laz\\r\\nWARNING: written 0 points but expected 0 points\\r\\nremoving /storage/lidar/Swinomish_Lidar_2016/interim/retiled\\\\1132000_1120000.laz\\r\\nWARNING: written 0 points but expected 0 points\\r\\nremoving /storage/lidar/Swinomish_Lidar_2016/interim/retiled\\\\1136000_1120000.laz\\r\\nWARNING: written 0 points but expected 0 points\\r\\nremoving /storage/lidar/Swinomish_Lidar_2016/interim/retiled\\\\1128000_1124000.laz\\r\\nWARNING: written 0 points but expected 0 points\\r\\nremoving /storage/lidar/Swinomish_Lidar_2016/interim/retiled\\\\1132000_1124000.laz\\r\\nWARNING: written 0 points but expected 0 points\\r\\nremoving /storage/lidar/Swinomish_Lidar_2016/interim/retiled\\\\1128000_1128000.laz\\r\\nWARNING: written 0 points but expected 0 points\\r\\nremoving /storage/lidar/Swinomish_Lidar_2016/interim/retiled\\\\1136000_1124000.laz\\r\\nWARNING: written 0 points but expected 0 points\\r\\nWARNING: written 86943 points but expected 0 points\\r\\nWARNING: written 53289 points but expected 0 points\\r\\nremoving /storage/lidar/Swinomish_Lidar_2016/interim/retiled\\\\1156000_1112000.laz\\r\\nWARNING: written 0 points but expected 0 points\\r\\nremoving /storage/lidar/Swinomish_Lidar_2016/interim/retiled\\\\1156000_1116000.laz\\r\\nWARNING: written 0 points but expected 0 points\\r\\nremoving /storage/lidar/Swinomish_Lidar_2016/interim/retiled\\\\1160000_1112000.laz\\r\\nWARNING: written 0 points but expected 0 points\\r\\nremoving /storage/lidar/Swinomish_Lidar_2016/interim/retiled\\\\1156000_1120000.laz\\r\\nWARNING: written 0 points but expected 0 points\\r\\nremoving /storage/lidar/Swinomish_Lidar_2016/interim/retiled\\\\1160000_1120000.laz\\r\\nWARNING: written 0 points but expected 0 points\\r\\nremoving /storage/lidar/Swinomish_Lidar_2016/interim/retiled\\\\1160000_1116000.laz\\r\\nWARNING: written 0 points but expected 0 points\\r\\nremoving /storage/lidar/Swinomish_Lidar_2016/interim/retiled\\\\1156000_1124000.laz\\r\\nWARNING: written 0 points but expected 0 points\\r\\nWARNING: written 15544 points but expected 0 points\\r\\nWARNING: written 591540 points but expected 0 points\\r\\nWARNING: written 69709 points but expected 0 points\\r\\nWARNING: written 2311342 points but expected 0 points\\r\\nWARNING: written 2010995 points but expected 0 points\\r\\nremoving /storage/lidar/Swinomish_Lidar_2016/interim/retiled\\\\1160000_1124000.laz\\r\\nWARNING: written 0 points but expected 0 points\\r\\nWARNING: written 2814140 points but expected 0 points\\r\\nWARNING: written 5657924 points but expected 0 points\\r\\nWARNING: written 2293581 points but expected 0 points\\r\\nWARNING: written 7001898 points but expected 0 points\\r\\nWARNING: written 8564492 points but expected 0 points\\r\\nWARNING: written 6600334 points but expected 0 points\\r\\nWARNING: written 3850757 points but expected 0 points\\r\\nWARNING: written 4474897 points but expected 0 points\\r\\nWARNING: written 13078343 points but expected 0 points\\r\\nWARNING: written 12221079 points but expected 0 points\\r\\nWARNING: written 15819765 points but expected 0 points\\r\\nWARNING: written 14489531 points but expected 0 points\\r\\nWARNING: written 17312928 points but expected 0 points\\r\\nWARNING: written 20059903 points but expected 0 points\\r\\nWARNING: written 17614464 points but expected 0 points\\r\\nWARNING: written 21441265 points but expected 0 points\\r\\nWARNING: written 12345859 points but expected 0 points\\r\\nWARNING: written 25219972 points but expected 0 points\\r\\nWARNING: written 21392815 points but expected 0 points\\r\\nWARNING: written 22202882 points but expected 0 points\\r\\nWARNING: written 25205713 points but expected 0 points\\r\\nWARNING: written 14396591 points but expected 0 points\\r\\nremoving /storage/lidar/Swinomish_Lidar_2016/interim/retiled\\\\1128000_1160000.laz\\r\\nWARNING: written 0 points but expected 0 points\\r\\nremoving /storage/lidar/Swinomish_Lidar_2016/interim/retiled\\\\1132000_1160000.laz\\r\\nWARNING: written 0 points but expected 0 points\\r\\nWARNING: written 2386909 points but expected 0 points\\r\\nWARNING: written 812497 points but expected 0 points\\r\\nWARNING: written 27801864 points but expected 0 points\\r\\nremoving /storage/lidar/Swinomish_Lidar_2016/interim/retiled\\\\1128000_1164000.laz\\r\\nWARNING: written 0 points but expected 0 points\\r\\nremoving /storage/lidar/Swinomish_Lidar_2016/interim/retiled\\\\1132000_1164000.laz\\r\\nWARNING: written 0 points but expected 0 points\\r\\nremoving /storage/lidar/Swinomish_Lidar_2016/interim/retiled\\\\1136000_1164000.laz\\r\\nWARNING: written 0 points but expected 0 points\\r\\nremoving /storage/lidar/Swinomish_Lidar_2016/interim/retiled\\\\1140000_1164000.laz\\r\\nWARNING: written 0 points but expected 0 points\\r\\nWARNING: written 28962793 points but expected 0 points\\r\\nWARNING: written 10437907 points but expected 0 points\\r\\nWARNING: written 29803710 points but expected 0 points\\r\\nWARNING: written 4925034 points but expected 0 points\\r\\nWARNING: written 3049925 points but expected 0 points\\r\\nWARNING: written 13166480 points but expected 0 points\\r\\nWARNING: written 31030845 points but expected 0 points\\r\\nWARNING: written 16012453 points but expected 0 points\\r\\nWARNING: written 10054938 points but expected 0 points\\r\\nWARNING: written 32503472 points but expected 0 points\\r\\nWARNING: written 21516795 points but expected 0 points\\r\\nWARNING: written 5823 points but expected 0 points\\r\\nWARNING: written 4286107 points but expected 0 points\\r\\nWARNING: written 112628 points but expected 0 points\\r\\nremoving /storage/lidar/Swinomish_Lidar_2016/interim/retiled\\\\1152000_1152000.laz\\r\\nWARNING: written 0 points but expected 0 points\\r\\nWARNING: written 14350698 points but expected 0 points\\r\\nWARNING: written 20398 points but expected 0 points\\r\\nWARNING: written 14406776 points but expected 0 points\\r\\nWARNING: written 2485070 points but expected 0 points\\r\\nWARNING: written 39553423 points but expected 0 points\\r\\nremoving /storage/lidar/Swinomish_Lidar_2016/interim/retiled\\\\1144000_1156000.laz\\r\\nWARNING: written 2 points but expected 0 points\\r\\nremoving /storage/lidar/Swinomish_Lidar_2016/interim/retiled\\\\1148000_1156000.laz\\r\\nWARNING: written 0 points but expected 0 points\\r\\nremoving /storage/lidar/Swinomish_Lidar_2016/interim/retiled\\\\1144000_1160000.laz\\r\\nWARNING: written 0 points but expected 0 points\\r\\nremoving /storage/lidar/Swinomish_Lidar_2016/interim/retiled\\\\1148000_1160000.laz\\r\\nWARNING: written 0 points but expected 0 points\\r\\nWARNING: written 1797702 points but expected 0 points\\r\\nremoving /storage/lidar/Swinomish_Lidar_2016/interim/retiled\\\\1152000_1156000.laz\\r\\nWARNING: written 0 points but expected 0 points\\r\\nremoving /storage/lidar/Swinomish_Lidar_2016/interim/retiled\\\\1152000_1160000.laz\\r\\nWARNING: written 0 points but expected 0 points\\r\\nWARNING: written 3404552 points but expected 0 points\\r\\nremoving /storage/lidar/Swinomish_Lidar_2016/interim/retiled\\\\1144000_1164000.laz\\r\\nWARNING: written 0 points but expected 0 points\\r\\nremoving /storage/lidar/Swinomish_Lidar_2016/interim/retiled\\\\1148000_1164000.laz\\r\\nWARNING: written 0 points but expected 0 points\\r\\nremoving /storage/lidar/Swinomish_Lidar_2016/interim/retiled\\\\1152000_1164000.laz\\r\\nWARNING: written 0 points but expected 0 points\\r\\nWARNING: written 35894663 points but expected 0 points\\r\\nWARNING: written 33741787 points but expected 0 points\\r\\nWARNING: written 17393330 points but expected 0 points\\r\\nWARNING: written 26768601 points but expected 0 points\\r\\nWARNING: written 22047330 points but expected 0 points\\r\\nWARNING: written 2949732 points but expected 0 points\\r\\nWARNING: written 39633678 points but expected 0 points\\r\\nWARNING: written 37316644 points but expected 0 points\\r\\nWARNING: written 16976832 points but expected 0 points\\r\\nWARNING: written 4507737 points but expected 0 points\\r\\nWARNING: written 34038138 points but expected 0 points\\r\\nWARNING: written 40391471 points but expected 0 points\\r\\nWARNING: written 4909612 points but expected 0 points\\r\\nWARNING: written 2977597 points but expected 0 points\\r\\nWARNING: written 29093376 points but expected 0 points\\r\\nWARNING: written 14619366 points but expected 0 points\\r\\nWARNING: written 42499621 points but expected 0 points\\r\\nWARNING: written 23267920 points but expected 0 points\\r\\nWARNING: written 24779346 points but expected 0 points\\r\\nWARNING: written 5055052 points but expected 0 points\\r\\nWARNING: written 4260277 points but expected 0 points\\r\\nWARNING: written 18215911 points but expected 0 points\\r\\nWARNING: written 41858671 points but expected 0 points\\r\\nWARNING: written 5764692 points but expected 0 points\\r\\nWARNING: written 17707812 points but expected 0 points\\r\\nWARNING: written 21088506 points but expected 0 points\\r\\nWARNING: written 40007828 points but expected 0 points\\r\\nWARNING: written 10544288 points but expected 0 points\\r\\nWARNING: written 44185329 points but expected 0 points\\r\\nWARNING: written 11543392 points but expected 0 points\\r\\nWARNING: written 23342470 points but expected 0 points\\r\\nWARNING: written 18569037 points but expected 0 points\\r\\nWARNING: written 26050030 points but expected 0 points\\r\\nWARNING: written 31693692 points but expected 0 points\\r\\nWARNING: written 44013347 points but expected 0 points\\r\\n')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "las.lastile(i=os.path.join(raw, '*.laz'),\n",
    "            tile_size=4000, # in units of lidar data\n",
    "            buffer=100, # assumes units are in feet... if using meters, suggest changing to ~25\n",
    "            flag_as_withheld=True,\n",
    "            olaz=True,\n",
    "            odir=os.path.join(interim, 'retiled'),\n",
    "            cores=len(c.ncores()));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to confirm that the point cloud files you've retiled haven't been corrupted (i.e., they still match valid LAS format specifications), you can use the following function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e14775cbd425475599fe410390fff616",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "@dask.delayed\n",
    "def validate(infile): # the function we'll map to a list of inputs\n",
    "    return las.lasvalidate(i=infile)\n",
    "\n",
    "infiles = glob.glob(os.path.join(interim, 'retiled', '*.laz'))\n",
    "val_results = c.compute([validate(file) for file in infiles])\n",
    "progress(val_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check to see if any of the files failed validation\n",
    "corrupted = [val_results[i].result().args[3] for i in range(len(val_results)) if 'fail' in val_results[i].result().stderr.decode()]\n",
    "corrupted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Classify points in the lidar point cloud\n",
    "First we'll remove points that are isolated as likely noise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6006fb2ec92843519cdd85063623489a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "@dask.delayed\n",
    "def denoise(tile_id): # the function we'll map to a list of inputs\n",
    "    basename = tile_id + '.laz'\n",
    "    infile = os.path.join(interim, 'retiled', basename)\n",
    "    return las.lasnoise(i=infile,\n",
    "                        remove_noise=True,\n",
    "                        odir=os.path.join(interim, 'denoised'),\n",
    "                        olaz=True) \n",
    "\n",
    "tiles = [os.path.basename(file).split('.')[0] for file in glob.glob(os.path.join(interim, 'retiled', '*.laz'))]\n",
    "denoise_results = c.compute([denoise(x) for x in tiles])\n",
    "progress(denoise_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, calculate the height aboveground for each point for use in classifying them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c107330f2d55418095a8f42b2aeaa00c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "@dask.delayed\n",
    "def hag(tile_id):\n",
    "    basename = tile_id + '.laz'\n",
    "    infile = os.path.join(interim, 'denoised', basename)\n",
    "    return las.lasheight(i=infile,\n",
    "                         odir=os.path.join(interim, 'lasheight'),\n",
    "                         olaz=True)\n",
    "\n",
    "tiles = [os.path.basename(file).split('.')[0] for file in glob.glob(os.path.join(interim, 'denoised', '*.laz'))]\n",
    "hag_results = c.compute([hag(x) for x in tiles])\n",
    "progress(hag_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we'll classify points as building or high vegetation that meet certain criteria for 'planarity' or 'ruggedness'. \n",
    "\n",
    "**THERE ARE ARGUMENTS IN THE FOLLOWING COMMAND THAT DEPEND UPON THE UNITS OF THE DATA.**\n",
    "\n",
    "If your data are in meters, you should change these parameters, or consider reprojecting the data to a projection that is in feet when you copy the source data into our working directory using `las2las` command at the top of this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c1d68cfe8aab4104846c47ddee255340",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "@dask.delayed\n",
    "def classify(tile_id):\n",
    "    basename = tile_id + '.laz'\n",
    "    infile = os.path.join(interim, 'lasheight', basename)\n",
    "    return las.lasclassify(i=infile,\n",
    "                           odir=os.path.join(interim, 'classified'),\n",
    "                           olaz=True,\n",
    "                           step=5, # if your data are in meters, the LAStools default is 2.0\n",
    "                           planar=0.5, # if your data are in meters, the LAStools default is 0.1\n",
    "                           rugged=1) # if your data are in meters, the LAStools default is 0.4\n",
    "\n",
    "tiles = [os.path.basename(file).split('.')[0] for file in glob.glob(os.path.join(interim, 'lasheight', '*.laz'))]\n",
    "classify_results = c.compute([classify(x) for x in tiles])\n",
    "progress(classify_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll now remove the points in the buffered area of each tile and put the clean tiles in the processed folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3fde69e695934309bce0a924409d12b7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "@dask.delayed\n",
    "def dropwithheld(tile_id):\n",
    "    basename = tile_id + '.laz'\n",
    "    infile = os.path.join(interim, 'classified', basename)\n",
    "    return las.las2las(i=infile,\n",
    "                       odir=os.path.join(processed, 'points'),\n",
    "                       olaz=True,\n",
    "                       drop_withheld=True, # remove points in tile buffers that were flagged as withhled with lastile\n",
    "                       set_user_data=0) # remove height aboveground calculated using lasheight\n",
    "\n",
    "tiles = [os.path.basename(file).split('.')[0] for file in glob.glob(os.path.join(interim, 'classified', '*.laz'))]\n",
    "drop_results = c.compute([dropwithheld(x) for x in tiles])\n",
    "progress(drop_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll produce a shapefile showing the layout of the tiles as a single shapefile. This is a single process that takes a few seconds to run, so no need to distribute it using `dask`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Produced a shapefile overview of clean tile boundaries.\n",
      "CPU times: user 924 ms, sys: 148 ms, total: 1.07 s\n",
      "Wall time: 4.72 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "infiles = os.path.join(processed, 'points', '*.laz')\n",
    "odir = os.path.join(processed, 'vectors')\n",
    "\n",
    "las.lasboundary(i=infiles,\n",
    "                odir=odir,\n",
    "                o='tiles.shp',\n",
    "                oshp=True,\n",
    "                use_bb=True, # use bounding box of tiles\n",
    "                overview=True,\n",
    "                labels=True,\n",
    "                cores=32) # use parallel processing\n",
    "\n",
    "print('Produced a shapefile overview of clean tile boundaries.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Generate a bare earth Digital Elevation Model\n",
    "Generate tiles of the bare earth model. This assumes that there are already ground-classified points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b8c99cbaf761404198d71908f5c3a0e5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "@dask.delayed\n",
    "def make_dem(tile_id):\n",
    "    basename = tile_id + '.laz'\n",
    "    infile = os.path.join(interim, 'classified', basename)\n",
    "    return las.las2dem(i=infile,\n",
    "                       odir=os.path.join(processed, 'rasters', 'DEM_tiles'),\n",
    "                       otif=True, # create tiles as GeoTiff rasters\n",
    "                       keep_class=2, # keep ground-classified returns only\n",
    "                       step=1, # resolution of output raster, in units of lidar data\n",
    "                       thin_with_grid=1, # use a 1 x 1 resolution for creating the TIN for the DEM\n",
    "                       extra_pass=True, # uses two passes over data to execute DEM creation more efficiently\n",
    "                       use_tile_bb=True) # remove buffers from tiles\n",
    "\n",
    "tiles = [os.path.basename(file).split('.')[0] for file in glob.glob(os.path.join(interim, 'classified', '*.laz'))]\n",
    "dem_results = c.compute([make_dem(x) for x in tiles])\n",
    "progress(dem_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "174c9d8814d6456698514d05709e783d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "@dask.delayed\n",
    "def add_crs_dem(tile_id):\n",
    "    basename = tile_id + '.tif'\n",
    "    infile = os.path.join(processed, 'rasters', 'DEM_tiles', basename)\n",
    "    return subprocess.run(['rio', 'edit-info', '--crs', 'EPSG:2286', infile],\n",
    "                          stderr=subprocess.PIPE, stdout=subprocess.PIPE)\n",
    "\n",
    "tiles = [os.path.basename(file).split('.')[0] for file in glob.glob(os.path.join(processed, 'rasters', 'DEM_tiles', '*.tif'))]\n",
    "crs_results = c.compute([add_crs_dem(x) for x in tiles])\n",
    "progress(crs_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removed 88 files with extension .tfw.\n",
      "Removed 88 files with extension .kml.\n",
      "CPU times: user 20 ms, sys: 4 ms, total: 24 ms\n",
      "Wall time: 20.1 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# get rid of the .tfw and .kml files that LAStools generates\n",
    "clean_dir(os.path.join(processed, 'rasters', 'DEM_tiles'), ['.tfw', '.kml'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Merge the bare earth tiles into a single GeoTiff."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done producing merged DEM GeoTiff.\n",
      "CPU times: user 40 s, sys: 4.52 s, total: 44.5 s\n",
      "Wall time: 3min 15s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "infiles = os.path.join(processed, 'rasters', 'DEM_tiles', '*.tif')\n",
    "outfile = os.path.join(processed, 'rasters', 'dem.tif')\n",
    "\n",
    "proc_merge = subprocess.run(['rio', 'merge', *glob.glob(infiles), outfile, '--co', 'compress=LZW'],\n",
    "                            stderr=subprocess.PIPE, stdout=subprocess.PIPE)\n",
    "\n",
    "print('Done producing merged DEM GeoTiff.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To create a hillshade layer, we'll first, generate hillshade tiles from the bare earth model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "70baa0e6332c41e5be3b2040e7d102dc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "@dask.delayed\n",
    "def make_hillshade(tile_id):\n",
    "    basename = tile_id + '.laz'\n",
    "    infile = os.path.join(interim, 'classified', basename)\n",
    "    return las.las2dem(i=infile,\n",
    "                       odir=os.path.join(processed, 'rasters', 'hillshade_tiles'),\n",
    "                       otif=True, # create tiles as GeoTiffs\n",
    "                       hillshade=True,\n",
    "                       keep_class=2, # keep ground-classified returns only\n",
    "                       step=1, # resolution of output raster, in units of lidar data\n",
    "                       thin_with_grid=1, # use a 0.5 x 0.5 resolution for creating the TIN for the DEM\n",
    "                       extra_pass=True, # uses two passes over data to execute DEM creation more efficiently\n",
    "                       use_tile_bb=True) # remove buffers from tiles\n",
    "\n",
    "tiles = [os.path.basename(file).split('.')[0] for file in glob.glob(os.path.join(interim, 'classified', '*.laz'))]\n",
    "hill_results = c.compute([make_hillshade(x) for x in tiles])\n",
    "progress(hill_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0cc16e554f6f4cdeb80c65c5afec03d8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "@dask.delayed\n",
    "def add_crs_hill(tile_id):\n",
    "    basename = tile_id + '.tif'\n",
    "    infile = os.path.join(processed, 'rasters', 'hillshade_tiles', basename)\n",
    "    return subprocess.run(['rio', 'edit-info', '--crs', 'EPSG:2286', infile],\n",
    "                          stderr=subprocess.PIPE, stdout=subprocess.PIPE)\n",
    "\n",
    "tiles = [os.path.basename(file).split('.')[0] for file in glob.glob(os.path.join(processed, 'rasters', 'hillshade_tiles', '*.tif'))]\n",
    "crs_results = c.compute([add_crs_hill(x) for x in tiles])\n",
    "progress(crs_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removed 88 files with extension .tfw.\n",
      "Removed 88 files with extension .kml.\n",
      "CPU times: user 8 ms, sys: 4 ms, total: 12 ms\n",
      "Wall time: 9.55 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# get rid of the .tfw and .kml files that LAStools generates\n",
    "clean_dir(os.path.join(processed, 'rasters', 'hillshade_tiles'), ['.tfw', '.kml'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now merge the hillshade tiles into a single raster formatted as GeoTiff."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done producing merged Hillshade GeoTiff.\n",
      "CPU times: user 12.3 s, sys: 1.56 s, total: 13.9 s\n",
      "Wall time: 59.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "infiles = os.path.join(processed, 'rasters', 'hillshade_tiles', '*.tif')\n",
    "outfile = os.path.join(processed, 'rasters', 'hillshade.tif')\n",
    "\n",
    "proc_merge = subprocess.run(['rio', 'merge', *glob.glob(infiles), outfile, '--co', 'compress=LZW'],\n",
    "                            stderr=subprocess.PIPE, stdout=subprocess.PIPE)\n",
    "\n",
    "print('Done producing merged Hillshade GeoTiff.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Identify building footprints\n",
    "First start by building shapefiles showing building boundaries in each buffered tile."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "273ddec4154c44e78ca68866aeb5372d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "@dask.delayed\n",
    "def bldg_tiles(tile_id):\n",
    "    basename = tile_id + '.laz'\n",
    "    infile = os.path.join(interim, 'classified', basename)\n",
    "    return las.lasboundary(i=infile,\n",
    "                           odir=os.path.join(interim, 'building_tiles'),\n",
    "                           keep_class=6, # use only building-classified points\n",
    "                           disjoint=True, # compute separate polygons for each building\n",
    "                           concavity=3) # map concave boundary if edge length >= 3ft\n",
    "\n",
    "tiles = [os.path.basename(file).split('.')[0] for file in glob.glob(os.path.join(interim, 'classified', '*.laz'))]\n",
    "bldg_results = c.compute([bldg_tiles(x) for x in tiles])\n",
    "progress(bldg_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate shapefiles showing the bounding box of each (unbuffered) tile that we'll use to remove buildings that fall in the buffered area."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1e73c18ed78d4dbc9fdd931d52ec2fb3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "@dask.delayed\n",
    "def bbox_shp(tile_id):\n",
    "    basename = tile_id + '.laz'\n",
    "    infile = os.path.join(processed, 'points', basename)\n",
    "    return las.lasboundary(i=infile,\n",
    "                           odir=os.path.join(interim, 'tile_boundaries'),\n",
    "                           oshp=True,\n",
    "                           use_tile_bb=True)\n",
    "\n",
    "tiles = [os.path.basename(file).split('.')[0] for file in glob.glob(os.path.join(processed, 'points', '*.laz'))]\n",
    "bbox_results = c.compute([bbox_shp(x) for x in tiles])\n",
    "progress(bbox_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each shapefile containing polygons of the building boundaries, we'll use the `clean_buffer_polys` function from `pyFIRS.utils` to remove polygons from a tile if their centroid falls in the buffered area of the tile."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "51916ef1fb4843919a3946d070fdf509",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "@dask.delayed\n",
    "def clean_bldgs(tile_id):\n",
    "    basename = tile_id + '.shp'\n",
    "    infile = os.path.join(interim, 'building_tiles', basename)\n",
    "    tile_shp = os.path.join(interim, 'tile_boundaries', basename)\n",
    "    return clean_buffer_polys(infile,\n",
    "                              tile_shp,\n",
    "                              odir=os.path.join(processed, 'vectors', 'building_tiles'),\n",
    "                              simp_tol=3,\n",
    "                              simp_topol=True)\n",
    "\n",
    "tiles = [os.path.basename(file).split('.')[0] for file in glob.glob(os.path.join(interim, 'building_tiles', '*.shp'))]\n",
    "clean_results = c.compute([clean_bldgs(x) for x in tiles])\n",
    "progress(clean_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Merge the cleaned tiles of building footprints together into a single shapefile. We'll use `geopandas` to concatenate all the polygons together into a single geodataframe and then write out to a new shapefile."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done merging tiles of building footprints into a single shapefile.\n",
      "CPU times: user 1.22 s, sys: 160 ms, total: 1.38 s\n",
      "Wall time: 1.35 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "building_tiles = glob.glob(os.path.join(processed, 'vectors', 'building_tiles', '*.shp'))\n",
    "# create a list of geodataframes containing the tiles of building footprints\n",
    "gdflist = [gpd.read_file(tile) for tile in building_tiles]\n",
    "# merge them all together\n",
    "merged = gpd.GeoDataFrame(pd.concat(gdflist, ignore_index=True))\n",
    "# using pandas' concat caused us to lose projection information, so let's add that back in\n",
    "merged.crs = gdflist[0].crs\n",
    "# and write the merged data to a new shapefile\n",
    "merged.to_file(os.path.join(processed,'vectors','buildings.shp'))\n",
    "\n",
    "print('Done merging tiles of building footprints into a single shapefile.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Create a Canopy Height Model\n",
    "We're going to switch use a FUSION command line tool to generate a Canopy Height Models (CHMs). \n",
    "\n",
    "### Using FUSION's `canopymodel` to generate CHMs\n",
    "`FUSION` wants to have ground models formatted as .dtm files, for CHM development and for estimating other canopy metrics. Let's generate these ground models first using a 1-meter x-y resolution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a7638f23d7b4428b0196f3ba5bdc290",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "@dask.delayed\n",
    "def groundDTMs(tile_id):\n",
    "    basename = tile_id + '.laz'\n",
    "    infile = os.path.join(interim, 'classified', basename)\n",
    "    odir = os.path.join(interim, 'dtm_ground_tiles')\n",
    "    outname = os.path.basename(infile).split('.')[0] + '.dtm'\n",
    "    outfile = os.path.join(odir, outname)\n",
    "    return fus.gridsurfacecreate(surfacefile=outfile,\n",
    "                           cellsize=3.28084,\n",
    "                           xyunits='F',\n",
    "                           zunits='F',\n",
    "                           coordsys=2, # in State Plane\n",
    "                           zone=0, # not in UTM\n",
    "                           horizdatum=2, # NAD83\n",
    "                           vertdatum=2, # NAVD88\n",
    "                           datafile=infile,\n",
    "                           las_class=2, # keep only ground-classified points\n",
    "                           odir=odir) # will make sure output directory is created if doesn't already exist\n",
    "\n",
    "tiles = [os.path.basename(file).split('.')[0] for file in glob.glob(os.path.join(interim, 'classified', '*.laz'))]\n",
    "dtm_results = c.compute([groundDTMs(x) for x in tiles])\n",
    "progress(dtm_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7a56a9ac85e84e83986dc0152a65722b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "@dask.delayed\n",
    "def canopymodel(tile_id):\n",
    "    basename = tile_id + '.laz'\n",
    "    infile = os.path.join(interim, 'classified', basename)\n",
    "    odir = os.path.join(interim, 'chm_tiles')\n",
    "    outname = os.path.basename(infile).split('.')[0] + '.dtm'\n",
    "    outfile = os.path.join(odir, outname)\n",
    "    return fus.canopymodel(surfacefile=outfile,\n",
    "                           cellsize=1,\n",
    "                           xyunits='F',\n",
    "                           zunits='F',\n",
    "                           coordsys=2, # in State Plane\n",
    "                           zone=0, # not in UTM\n",
    "                           horizdatum=2, # NAD83\n",
    "                           vertdatum=2, # NAVD88\n",
    "                           datafiles=infile,\n",
    "                           ground=os.path.join(interim, 'dtm_ground_tiles', outname),\n",
    "                           median=3, # median smoothing in 3x3 kernel\n",
    "                           las_class=(1,2,5), # keep only ground, unclassified, and high veg points\n",
    "                           asc=True, # also output in ascii format\n",
    "                           odir=odir) # will make sure output directory is created if doesn't already exist\n",
    "\n",
    "tiles = [os.path.basename(file).split('.')[0] for file in glob.glob(os.path.join(interim, 'classified', '*.laz'))]\n",
    "canopy_results = c.compute([canopymodel(x) for x in tiles])\n",
    "progress(canopy_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert the ascii files that `canopymodel` generated into GeoTiffs, specifying their projection. Then cleanup the files `canopymodel` generated that we don't care about."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad75b8057b6f4e99aa4048055bdb7de7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "@dask.delayed\n",
    "def asc2tif(tile_id):\n",
    "    basename = tile_id + '.asc'\n",
    "    infile = os.path.join(interim, 'chm_tiles', basename)\n",
    "    return convert_project(infile, '.tif', 'EPSG:2286')\n",
    "\n",
    "tiles = [os.path.basename(file).split('.')[0] for file in glob.glob(os.path.join(interim, 'chm_tiles', '*.asc'))]\n",
    "proj_results = c.compute([asc2tif(x) for x in tiles])\n",
    "progress(proj_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removed 85 files with extension .asc.\n",
      "Removed 85 files with extension .dtm.\n"
     ]
    }
   ],
   "source": [
    "clean_dir(os.path.join(interim, 'chm_tiles'), ['.asc', '.dtm'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clip the canopy height model tiles to remove overlapping areas that were from tile buffering to avoid edge effects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cabda4c295184892b604187d0758e4b4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "@dask.delayed\n",
    "def clip(tile_id):\n",
    "    basename = tile_id + '.tif'\n",
    "    infile = os.path.join(interim, 'chm_tiles', basename)\n",
    "    in_shp = os.path.join(interim, 'tile_boundaries', tile_id + '.shp')\n",
    "    odir = os.path.join(processed, 'rasters', 'chm_tiles')\n",
    "    return clip_tile_from_shp(infile, in_shp, odir)\n",
    "\n",
    "tiles = [os.path.basename(file).split('.')[0] for file in glob.glob(os.path.join(interim, 'chm_tiles', '*.tif'))]\n",
    "clip_results = c.compute([clip(x) for x in tiles])\n",
    "progress(clip_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Merge the trimmed canopy height model tiles into a single raster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done producing merged Canopy Height Model GeoTiff.\n",
      "CPU times: user 31.2 s, sys: 3.74 s, total: 35 s\n",
      "Wall time: 2min 3s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "infiles = os.path.join(processed, 'rasters', 'chm_tiles', '*.tif')\n",
    "outfile = os.path.join(processed, 'rasters', 'chm.tif')\n",
    "\n",
    "proc_merge = subprocess.run(['rio', 'merge', *glob.glob(infiles), outfile, '--co', 'compress=LZW'],\n",
    "                            stderr=subprocess.PIPE, stdout=subprocess.PIPE)\n",
    "\n",
    "print('Done producing merged Canopy Height Model GeoTiff.')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pyFIRS]",
   "language": "python",
   "name": "conda-env-pyFIRS-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
