{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import dask\n",
    "from dask.distributed import Client, progress, LocalCluster\n",
    "from pyFIRS.wrappers import lastools\n",
    "from pyFIRS.wrappers import fusion\n",
    "from pyFIRS.utils import validation_summary, move_invalid_tiles, fname"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data handling directories\n",
    "WORKDIR = os.path.abspath('/storage/lidar/umatilla_2014/')\n",
    "# TARGET_EPSG = 6339  # utm 10N, NAD83_2011\n",
    "TARGET_EPSG = 6340  # utm 11N, NAD83_2011"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SRC = os.path.join(WORKDIR, 'src')\n",
    "src_tiles = glob.glob(os.path.join(SRC, '*.laz'))\n",
    "# src_tiles = glob.glob(os.path.join(SRC, '*.las'))\n",
    "\n",
    "# where we're going to put processed source tiles\n",
    "RAW = os.path.join(WORKDIR, 'raw')\n",
    "\n",
    "print('Found {:,d} tiles in source directory:\\n'\n",
    "      ' {}'.format(len(src_tiles), SRC))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Enough already, let's get to work with some lidar data\n",
    "We'll define where we can find the binary executables for LAStools and FUSION command line tools."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "las = lastools.useLAStools('/storage/lidar/LAStools/bin')\n",
    "fus = fusion.useFUSION('/storage/lidar/FUSION/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# take a peak at info from a lidar source tile\n",
    "info_proc = las.lasinfo(i=src_tiles[0],\n",
    "                        echo=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting up parallel computing using `dask.distributed`\n",
    "`LAStools` offers native multi-core processing as an optional argument (`cores`) supplied to its command-line tools. `FUSION` command line tools do not. To enable parallel processing of `FUSION` commands, we'll use `dask.distributed` to schedule the processing of tiles in asynchronous parallel batches. This approach also offers us the ability to track progress using a progress bar.\n",
    "\n",
    "You'll first need to launch a parallel computing cluster. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster=LocalCluster(scheduler_port=7001, diagnostics_port=7002)\n",
    "c = Client(cluster)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point, you should also be able to view an interactive dashboard on port 7002. If you're executing this on a remote server, you'll need to set up port forward so you can view the dashboard on your local machine's browser. Once you've done that, or if you're processing on your own machine, you can view the dashboard at [http://localhost:7002/status](http://localhost:7002/status)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_cores = len(c.ncores()) # identify how many workers we have\n",
    "\n",
    "# push our working directories and wrapper classes to the workers on the cluster as well\n",
    "c.scatter([WORKDIR, SRC, RAW, \n",
    "           las, fus, \n",
    "           TARGET_EPSG, \n",
    "           num_cores], \n",
    "          broadcast=True);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get the raw data into our working directory\n",
    "First, move the tiles over to our working directory.\n",
    "\n",
    "When we define functions using the `dask.delayed` decorator, the function will have 'lazy' instead of 'eager' execution. We can map the function to a list of inputs and it will not execute for any of them until we ask for results to be computed. When we use the `compute()` method for the client managing the scheduler that sends jobs to the workers, it then starts running the jobs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dask.delayed\n",
    "def import_tile(tile_id):\n",
    "    INFILE = os.path.join(SRC, tile_id + '.laz')\n",
    "#     INFILE = os.path.join(SRC, tile_id + '.las')\n",
    "    OUTFILE = os.path.join(RAW, tile_id + '.laz')\n",
    "    \n",
    "\n",
    "    if os.path.exists(OUTFILE):\n",
    "        pass\n",
    "    else:\n",
    "        proc_import =  las.las2las(i=INFILE,\n",
    "                                   drop_withheld=True,\n",
    "                                   drop_class=(7,18),  # classified as noise\n",
    "#                                    epsg=2927,  # specify the source lidar projection\n",
    "#                                    nad83_2011=True,  # original data in nad83_2011 datum\n",
    "#                                    nad83_harn=True,  # original data in nad83_harn datum\n",
    "                                   target_epsg=TARGET_EPSG, # reproject\n",
    "                                   dont_remove_empty_files=True,\n",
    "                                   odir=RAW,\n",
    "                                   olaz=True)\n",
    "    return tile_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, validate that the data match LAS specifications and have not been corrupted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dask.delayed\n",
    "def validate(tile_id):\n",
    "    INFILE = os.path.join(RAW, tile_id + '.laz')\n",
    "    OUTFILE = os.path.join(RAW, tile_id + '.xml')\n",
    "    \n",
    "    if os.path.exists(OUTFILE):\n",
    "        pass\n",
    "    else:\n",
    "        proc_validate = las.lasvalidate(i=INFILE,\n",
    "                                        o=OUTFILE)\n",
    "    return tile_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, create spatial indexes for the input files to allow fast spatial queries (which are used, for example, when retiling and adding buffers)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dask.delayed\n",
    "def make_index(tile_id):\n",
    "    INFILE = os.path.join(RAW, tile_id + '.laz')\n",
    "    OUTFILE = os.path.join(RAW, tile_id + '.lax')\n",
    "\n",
    "    if os.path.exists(OUTFILE): \n",
    "        pass\n",
    "    else:\n",
    "        proc_index = las.lasindex(i=INFILE)\n",
    "\n",
    "    return tile_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hand-build the computational graph\n",
    "Define the recipe for computations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tile_ids = [fname(tile) for tile in src_tiles]\n",
    "\n",
    "get_data = {}\n",
    "for tile in tile_ids:\n",
    "    get_data['import-{}'.format(tile)]=(\n",
    "        import_tile, \n",
    "        tile)\n",
    "    get_data['validate-{}'.format(tile)]=(\n",
    "        validate, \n",
    "        'import-{}'.format(tile))\n",
    "    get_data['index-{}'.format(tile)]=(\n",
    "        make_index, \n",
    "        'validate-{}'.format(tile))\n",
    "    \n",
    "# this empty function will be added to recipe for computations\n",
    "# it will be defined to depend upon all previous steps being completed\n",
    "@dask.delayed\n",
    "def done_importing(*args, **kwargs):\n",
    "    return\n",
    "\n",
    "get_data['done_importing']=(\n",
    "    done_importing, \n",
    "    ['index-{}'.format(tile) for tile in tile_ids])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_data_graph = c.get(get_data, 'done_importing')  # build the computational graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_data_results = c.persist(get_data_graph)  # start executing it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "progress(get_data_results)  # progress bars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# c.cancel(get_data_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "validation_summary(xml_dir=RAW, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# move_invalid_tiles(xml_dir=RAW, dest_dir=os.path.join(RAW, 'invalid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# c.close()\n",
    "# cluster.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pyFIRS] *",
   "language": "python",
   "name": "conda-env-pyFIRS-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
