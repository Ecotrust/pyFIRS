{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import dask\n",
    "from dask.distributed import Client, progress, LocalCluster\n",
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "\n",
    "from pyFIRS.wrappers import lastools\n",
    "from pyFIRS.wrappers import fusion\n",
    "from pyFIRS.utils import validation_summary, move_invalid_tiles, fname, PipelineError, inspect_failures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data handling directories\n",
    "WORKDIR = os.path.abspath('F:/willamette-valley_2009/')\n",
    "TARGET_EPSG = 6339  # utm 10N, NAD83_2011\n",
    "# TARGET_EPSG = 6340  # utm 11N, NAD83_2011"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SRC = os.path.join(WORKDIR, 'src')\n",
    "src_tiles = glob.glob(os.path.join(SRC, '*.laz'))\n",
    "# src_tiles = glob.glob(os.path.join(SRC, '*.las'))\n",
    "\n",
    "# where we're going to put processed source tiles\n",
    "RAW = os.path.join(WORKDIR, 'raw')\n",
    "\n",
    "print('Found {:,d} tiles in source directory:\\n'\n",
    "      ' {}'.format(len(src_tiles), SRC))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Enough already, let's get to work with some lidar data\n",
    "We'll define where we can find the binary executables for LAStools and FUSION command line tools."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "las = lastools.useLAStools('C:/Program Files/LAStools/bin')\n",
    "fus = fusion.useFUSION('C:/Program Files/FUSION/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# take a peak at info from a lidar source tile\n",
    "info_proc = las.lasinfo(i=src_tiles[0],\n",
    "                        echo=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting up parallel computing using `dask.distributed`\n",
    "`LAStools` offers native multi-core processing as an optional argument (`cores`) supplied to its command-line tools. `FUSION` command line tools do not. To enable parallel processing of `FUSION` commands, we'll use `dask.distributed` to schedule the processing of tiles in asynchronous parallel batches. This approach also offers us the ability to track progress using a progress bar.\n",
    "\n",
    "You'll first need to launch a parallel computing cluster. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster=LocalCluster()#(scheduler_port=7001, dashboard_address=7002)\n",
    "c = Client(cluster)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point, you should also be able to view an interactive dashboard on port 7002. If you're executing this on a remote server, you'll need to set up port forward so you can view the dashboard on your local machine's browser. Once you've done that, or if you're processing on your own machine, you can view the dashboard at [http://localhost:7002/status](http://localhost:7002/status)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# push our working directories and wrapper classes to the workers on the cluster as well\n",
    "c.scatter([WORKDIR, SRC, RAW, \n",
    "           las, fus, \n",
    "           TARGET_EPSG], \n",
    "          broadcast=True);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_error(tile_id, process, error_msg):\n",
    "    logfile = os.path.join(RAW, 'failed', tile_id + '.txt')\n",
    "    os.makedirs(os.path.dirname(logfile), exist_ok=True)\n",
    "\n",
    "    with open(logfile, '+w') as f:\n",
    "        f.write('{} | {}: {}'.format(tile_id, process, error_msg))\n",
    "\n",
    "    return\n",
    "\n",
    "\n",
    "def has_error(tile_id):\n",
    "    errors = glob.glob(os.path.join(RAW, 'failed', '*.txt'))\n",
    "    tiles_with_errors = [fname(error) for error in errors]\n",
    "    if tile_id in tiles_with_errors:\n",
    "        return True\n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get the raw data into our working directory\n",
    "First, move the tiles over to our working directory.\n",
    "\n",
    "When we define functions using the `dask.delayed` decorator, the function will have 'lazy' instead of 'eager' execution. We can map the function to a list of inputs and it will not execute for any of them until we ask for results to be computed. When we use the `compute()` method for the client managing the scheduler that sends jobs to the workers, it then starts running the jobs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dask.delayed\n",
    "def import_tile(tile_id):\n",
    "    INFILE = os.path.join(SRC, tile_id + '.laz')\n",
    "#     INFILE = os.path.join(SRC, tile_id + '.las')\n",
    "    OUTFILE = os.path.join(RAW, tile_id + '.laz')\n",
    "    \n",
    "\n",
    "    if os.path.exists(OUTFILE):\n",
    "        pass\n",
    "    else:\n",
    "        try:\n",
    "            proc_import =  las.las2las(i=INFILE,\n",
    "                                       drop_withheld=True,\n",
    "                                       drop_class=(7,18),  # classified as noise\n",
    "    #                                    epsg=32149,  # specify the source lidar projection, washington state plane south\n",
    "    #                                    epsg=2927,  # specify the source lidar projection, washington state plane south\n",
    "                                       longlat=True,  # original data is in geographic coordinates\n",
    "    #                                    elevation_surveyfeet=True,\n",
    "    #                                    survey_feet=True,\n",
    "    #                                    nad83_2011=True,  # original data in nad83_2011 datum\n",
    "                                       nad83_harn=True,  # original data in nad83_harn datum\n",
    "                                       target_epsg=TARGET_EPSG, # reproject\n",
    "                                       dont_remove_empty_files=True,\n",
    "                                       cpu64=True,\n",
    "                                       odir=RAW,\n",
    "                                       olaz=True)\n",
    "        except PipelineError as e:\n",
    "            log_error(tile_id, 'import_tile', e.message)\n",
    "    return tile_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, validate that the data match LAS specifications and have not been corrupted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dask.delayed\n",
    "def validate(tile_id):\n",
    "    INFILE = os.path.join(RAW, tile_id + '.laz')\n",
    "    OUTFILE = os.path.join(RAW, tile_id + '.xml')\n",
    "    \n",
    "    if os.path.exists(OUTFILE):\n",
    "        pass\n",
    "    else:\n",
    "        try:\n",
    "            proc_validate = las.lasvalidate(i=INFILE,\n",
    "                                            o=OUTFILE)\n",
    "        except PipelineError as e:\n",
    "            log_error(tile_id, 'validate', e.message)\n",
    "            \n",
    "    return tile_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, create spatial indexes for the input files to allow fast spatial queries (which are used, for example, when retiling and adding buffers)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dask.delayed\n",
    "def make_index(tile_id):\n",
    "    INFILE = os.path.join(RAW, tile_id + '.laz')\n",
    "    OUTFILE = os.path.join(RAW, tile_id + '.lax')\n",
    "\n",
    "    if os.path.exists(OUTFILE): \n",
    "        pass\n",
    "    else:\n",
    "        try:\n",
    "            proc_index = las.lasindex(i=INFILE,\n",
    "                                      cpu64=True)\n",
    "        \n",
    "        except PipelineError as e:\n",
    "            log_error(tile_id, 'make_index', e.message)\n",
    "\n",
    "    return tile_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dask.delayed\n",
    "def make_boundary(tile_id):\n",
    "    INFILE = os.path.join(RAW, tile_id + '.laz')\n",
    "    OUTFILE = os.path.join(RAW, tile_id + '.shp')\n",
    "    \n",
    "    if os.path.exists(OUTFILE): \n",
    "        pass\n",
    "    else:\n",
    "        try:\n",
    "            proc_bnd = las.lasboundary(i=INFILE,\n",
    "                                       o=OUTFILE,\n",
    "                                       disjoint=True,\n",
    "                                       labels=True,\n",
    "                                       use_lax=True,\n",
    "                                       cpu64=True)\n",
    "        \n",
    "        except PipelineError as e:\n",
    "            log_error(tile_id, 'make_index', e.message)\n",
    "\n",
    "    return tile_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hand-build the computational graph\n",
    "Define the recipe for computations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tile_ids = [fname(tile) for tile in src_tiles]\n",
    "\n",
    "get_data = {}\n",
    "for tile in tile_ids:\n",
    "    get_data['import-{}'.format(tile)]=(\n",
    "        import_tile, \n",
    "        tile)\n",
    "    get_data['validate-{}'.format(tile)]=(\n",
    "        validate, \n",
    "        'import-{}'.format(tile))\n",
    "    get_data['index-{}'.format(tile)]=(\n",
    "        make_index, \n",
    "        'validate-{}'.format(tile))\n",
    "    get_data['boundary-{}'.format(tile)]=(\n",
    "        make_boundary, \n",
    "        'index-{}'.format(tile))\n",
    "    \n",
    "# this empty function will be added to recipe for computations\n",
    "# it will be defined to depend upon all previous steps being completed\n",
    "@dask.delayed\n",
    "def done_importing(*args, **kwargs):\n",
    "    return\n",
    "\n",
    "get_data['done_importing']=(\n",
    "    done_importing, \n",
    "    ['boundary-{}'.format(tile) for tile in tile_ids])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_data_graph = c.get(get_data, 'done_importing')  # build the computational graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_data_results = c.persist(get_data_graph)  # start executing it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "progress(get_data_results)  # progress bars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# c.cancel(get_data_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inspect_failures(os.path.join(RAW, 'failed'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_summary(xml_dir=RAW, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# move_invalid_tiles(xml_dir=RAW, dest_dir=os.path.join(RAW, 'invalid'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merge all the individual tile boundaries into a tile index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "boundaries = glob.glob(os.path.join(RAW, '*.shp'))\n",
    "# set up a lazy read_file function so we can read in files in parallel\n",
    "gdfs = [dask.delayed(gpd.read_file)(shp) for shp in boundaries]\n",
    "# and a lazy concatenation\n",
    "gather_tiles = dask.delayed(pd.concat)(gdfs, axis=0, ignore_index=True)\n",
    "# now execute the read and concatenate with the cluster\n",
    "tileindex = gather_tiles.compute()\n",
    "tileindex.crs = \"EPSG:{}\".format(TARGET_EPSG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tileindex.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tileindex.to_file(os.path.join(RAW, 'raw_tileindex.shp'), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# c.close()\n",
    "# cluster.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pyFIRS] *",
   "language": "python",
   "name": "conda-env-pyFIRS-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
