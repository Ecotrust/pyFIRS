{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import dask\n",
    "from dask.distributed import Client, progress, LocalCluster\n",
    "from pyFIRS.wrappers import lastools\n",
    "from pyFIRS.wrappers import fusion\n",
    "from pyFIRS.utils import validation_summary, move_invalid_tiles, fname"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data handling directories\n",
    "WORKDIR = os.path.abspath('/storage/lidar/portland-metro_2014/')\n",
    "TARGET_EPSG = 6339  # utm 10N, NAD83_2011\n",
    "# TARGET_EPSG = 6340  # utm 11N, NAD83_2011"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2,602 tiles in source directory:\n",
      " /storage/lidar/portland-metro_2014/src\n"
     ]
    }
   ],
   "source": [
    "SRC = os.path.join(WORKDIR, 'src')\n",
    "src_tiles = glob.glob(os.path.join(SRC, '*.laz'))\n",
    "# src_tiles = glob.glob(os.path.join(SRC, '*.las'))\n",
    "\n",
    "# where we're going to put processed source tiles\n",
    "RAW = os.path.join(WORKDIR, 'raw')\n",
    "\n",
    "print('Found {:,d} tiles in source directory:\\n'\n",
    "      ' {}'.format(len(src_tiles), SRC))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Enough already, let's get to work with some lidar data\n",
    "We'll define where we can find the binary executables for LAStools and FUSION command line tools."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "las = lastools.useLAStools('/storage/lidar/LAStools/bin')\n",
    "fus = fusion.useFUSION('/storage/lidar/FUSION/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "lasinfo (190321) report for '/storage/lidar/portland-metro_2014/src/45122D1414.laz'\n",
      "reporting all LAS header entries:\n",
      "  file signature:             'LASF'\n",
      "  file source ID:             0\n",
      "  global_encoding:            0\n",
      "  project ID GUID data 1-4:   00000000-0000-0000-0000-000000000000\n",
      "  version major.minor:        1.2\n",
      "  system identifier:          'NOAA OCM'\n",
      "  generating software:        'datum_shift (9515 2017-07-12)'\n",
      "  file creation day/year:     126/2015\n",
      "  header size:                227\n",
      "  offset to point data:       525\n",
      "  number var. length records: 3\n",
      "  point data format:          3\n",
      "  point data record length:   34\n",
      "  number of point records:    52316423\n",
      "  number of points by return: 21652327 17089749 9789058 3785289 0\n",
      "  scale factor x y z:         0.0000001 0.0000001 0.01\n",
      "  offset x y z:               -122 45 0\n",
      "  min x y z:                  -122.0249857 45.3999944 693.78\n",
      "  max x y z:                  -122.0124856 45.4124946 957.74\n",
      "variable length header record 1 of 3:\n",
      "  reserved             16716\n",
      "  user ID              'SF_Spec'\n",
      "  record ID            7\n",
      "  length after header  48\n",
      "  description          'Oregon Lambert (ft) - Ft Int'l'\n",
      "variable length header record 2 of 3:\n",
      "  reserved             16716\n",
      "  user ID              'SF_Spec'\n",
      "  record ID            7\n",
      "  length after header  40\n",
      "  description          'LAS Georeferencing'\n",
      "variable length header record 3 of 3:\n",
      "  reserved             0\n",
      "  user ID              'LASF_Projection'\n",
      "  record ID            34735\n",
      "  length after header  48\n",
      "  description          'LAS Georeferencing'\n",
      "    GeoKeyDirectoryTag version 1.1.0 number of keys 5\n",
      "      key 1024 tiff_tag_location 0 count 1 value_offset 2 - GTModelTypeGeoKey: ModelTypeGeographic\n",
      "      key 2048 tiff_tag_location 0 count 1 value_offset 4759 - GeographicTypeGeoKey: look-up for 4759 not implemented\n",
      "      key 2054 tiff_tag_location 0 count 1 value_offset 9102 - GeogAngularUnitsGeoKey: Angular_Degree\n",
      "      key 4096 tiff_tag_location 0 count 1 value_offset 5703 - VerticalCSTypeGeoKey: NAVD88 height (Reserved EPSG)\n",
      "      key 4099 tiff_tag_location 0 count 1 value_offset 9001 - VerticalUnitsGeoKey: Linear_Meter\n",
      "LASzip compression (version 3.0r0 c2 50000): POINT10 2 GPSTIME11 2 RGB12 2\n",
      "reporting minimum and maximum for all LAS point record entries ...\n",
      "  X             -249857    -124856\n",
      "  Y             3999944    4124946\n",
      "  Z               69378      95774\n",
      "  intensity           1        837\n",
      "  return_number       1          4\n",
      "  number_of_returns   1          4\n",
      "  edge_of_flight_line 0          1\n",
      "  scan_direction_flag 0          1\n",
      "  classification      1          2\n",
      "  scan_angle_rank   -15         15\n",
      "  user_data           0          0\n",
      "  point_source_ID  5041       5086\n",
      "  gps_time 58348.844969 581153.583222\n",
      "  Color R 1280 59392\n",
      "        G 2048 58624\n",
      "        B 0 58368\n",
      "number of first returns:        21652327\n",
      "number of intermediate returns: 13576712\n",
      "number of last returns:         21648754\n",
      "number of single returns:       4561370\n",
      "overview over number of returns of given pulse: 4561370 14599358 18011006 15144689 0 0 0\n",
      "histogram of classification of points:\n",
      "        51819853  unclassified (1)\n",
      "          496570  ground (2)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# take a peak at info from a lidar source tile\n",
    "info_proc = las.lasinfo(i=src_tiles[0],\n",
    "                        echo=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting up parallel computing using `dask.distributed`\n",
    "`LAStools` offers native multi-core processing as an optional argument (`cores`) supplied to its command-line tools. `FUSION` command line tools do not. To enable parallel processing of `FUSION` commands, we'll use `dask.distributed` to schedule the processing of tiles in asynchronous parallel batches. This approach also offers us the ability to track progress using a progress bar.\n",
    "\n",
    "You'll first need to launch a parallel computing cluster. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/storage/anaconda3/envs/pyFIRS/lib/python3.6/site-packages/distributed/bokeh/core.py:74: UserWarning: \n",
      "Port 8787 is already in use. \n",
      "Perhaps you already have a cluster running?\n",
      "Hosting the diagnostics dashboard on a random port instead.\n",
      "  warnings.warn(\"\\n\" + msg)\n"
     ]
    }
   ],
   "source": [
    "cluster=LocalCluster()#scheduler_port=7001, diagnostics_port=7002)\n",
    "c = Client(cluster)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point, you should also be able to view an interactive dashboard on port 7002. If you're executing this on a remote server, you'll need to set up port forward so you can view the dashboard on your local machine's browser. Once you've done that, or if you're processing on your own machine, you can view the dashboard at [http://localhost:7002/status](http://localhost:7002/status)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_cores = len(c.ncores()) # identify how many workers we have\n",
    "\n",
    "# push our working directories and wrapper classes to the workers on the cluster as well\n",
    "c.scatter([WORKDIR, SRC, RAW, \n",
    "           las, fus, \n",
    "           TARGET_EPSG, \n",
    "           num_cores], \n",
    "          broadcast=True);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get the raw data into our working directory\n",
    "First, move the tiles over to our working directory.\n",
    "\n",
    "When we define functions using the `dask.delayed` decorator, the function will have 'lazy' instead of 'eager' execution. We can map the function to a list of inputs and it will not execute for any of them until we ask for results to be computed. When we use the `compute()` method for the client managing the scheduler that sends jobs to the workers, it then starts running the jobs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dask.delayed\n",
    "def import_tile(tile_id):\n",
    "    INFILE = os.path.join(SRC, tile_id + '.laz')\n",
    "#     INFILE = os.path.join(SRC, tile_id + '.las')\n",
    "    OUTFILE = os.path.join(RAW, tile_id + '.laz')\n",
    "    \n",
    "\n",
    "    if os.path.exists(OUTFILE):\n",
    "        pass\n",
    "    else:\n",
    "        proc_import =  las.las2las(i=INFILE,\n",
    "                                   drop_withheld=True,\n",
    "                                   drop_class=(7,18),  # classified as noise\n",
    "#                                    epsg=32149,  # specify the source lidar projection, washington state plane south\n",
    "#                                    epsg=2927,  # specify the source lidar projection, washington state plane south\n",
    "                                   longlat=True,  # original data is in geographic coordinates\n",
    "#                                    elevation_surveyfeet=True,\n",
    "#                                    survey_feet=True,\n",
    "#                                    nad83_2011=True,  # original data in nad83_2011 datum\n",
    "                                   nad83_harn=True,  # original data in nad83_harn datum\n",
    "                                   target_epsg=TARGET_EPSG, # reproject\n",
    "                                   dont_remove_empty_files=True,\n",
    "                                   odir=RAW,\n",
    "                                   olaz=True)\n",
    "    return tile_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, validate that the data match LAS specifications and have not been corrupted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dask.delayed\n",
    "def validate(tile_id):\n",
    "    INFILE = os.path.join(RAW, tile_id + '.laz')\n",
    "    OUTFILE = os.path.join(RAW, tile_id + '.xml')\n",
    "    \n",
    "    if os.path.exists(OUTFILE):\n",
    "        pass\n",
    "    else:\n",
    "        proc_validate = las.lasvalidate(i=INFILE,\n",
    "                                        o=OUTFILE)\n",
    "    return tile_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, create spatial indexes for the input files to allow fast spatial queries (which are used, for example, when retiling and adding buffers)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dask.delayed\n",
    "def make_index(tile_id):\n",
    "    INFILE = os.path.join(RAW, tile_id + '.laz')\n",
    "    OUTFILE = os.path.join(RAW, tile_id + '.lax')\n",
    "\n",
    "    if os.path.exists(OUTFILE): \n",
    "        pass\n",
    "    else:\n",
    "        proc_index = las.lasindex(i=INFILE)\n",
    "\n",
    "    return tile_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hand-build the computational graph\n",
    "Define the recipe for computations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "tile_ids = [fname(tile) for tile in src_tiles]\n",
    "\n",
    "get_data = {}\n",
    "for tile in tile_ids:\n",
    "    get_data['import-{}'.format(tile)]=(\n",
    "        import_tile, \n",
    "        tile)\n",
    "    get_data['validate-{}'.format(tile)]=(\n",
    "        validate, \n",
    "        'import-{}'.format(tile))\n",
    "    get_data['index-{}'.format(tile)]=(\n",
    "        make_index, \n",
    "        'validate-{}'.format(tile))\n",
    "    \n",
    "# this empty function will be added to recipe for computations\n",
    "# it will be defined to depend upon all previous steps being completed\n",
    "@dask.delayed\n",
    "def done_importing(*args, **kwargs):\n",
    "    return\n",
    "\n",
    "get_data['done_importing']=(\n",
    "    done_importing, \n",
    "    ['index-{}'.format(tile) for tile in tile_ids])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_data_graph = c.get(get_data, 'done_importing')  # build the computational graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_data_results = c.persist(get_data_graph)  # start executing it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4fd55122b2854920a2429b08b4bd05ec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "progress(get_data_results)  # progress bars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# c.cancel(get_data_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LASvalidate Summary\n",
      "====================\n",
      "Passed: 2,602\n",
      "Failed: 0\n",
      "Warnings: 0\n",
      "ParseErrors: 0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "validation_summary(xml_dir=RAW, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# move_invalid_tiles(xml_dir=RAW, dest_dir=os.path.join(RAW, 'invalid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# c.close()\n",
    "# cluster.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pyFIRS]",
   "language": "python",
   "name": "conda-env-pyFIRS-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
