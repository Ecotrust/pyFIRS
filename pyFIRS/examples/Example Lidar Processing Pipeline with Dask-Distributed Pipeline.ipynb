{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/storage/anaconda3/envs/pyFIRS/lib/python3.6/site-packages/tqdm/autonotebook/__init__.py:14: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  \" (e.g. in jupyter console)\", TqdmExperimentalWarning)\n"
     ]
    }
   ],
   "source": [
    "import os, shutil, glob, platform, subprocess\n",
    "import geopandas as gpd, pandas as pd\n",
    "import rasterio\n",
    "from matplotlib import pyplot as plt\n",
    "from pyFIRS.wrappers import lastools, fusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyFIRS.utils import clean_dir, clean_buffer_polys, clip_tile_from_shp, convert_project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask.distributed import Client, progress, LocalCluster\n",
    "from dask import delayed\n",
    "import dask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting up parallel computing using `dask.distributed`\n",
    "`LAStools` offers native multi-core processing as an optional argument (`cores`) supplied to its command-line tools. `FUSION` command line tools do not. To enable parallel processing of `FUSION` commands, we'll use `dask.distributed` to schedule the processing of tiles in asynchronous parallel batches. This approach also offers us the ability to track progress using a progress bar.\n",
    "\n",
    "You'll first need to launch a parallel computing cluster. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster=LocalCluster(scheduler_port=7001, diagnostics_port=7002)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = Client(cluster)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point, you should also be able to view an interactive dashboard on port 7002. If you're executing this on a remote server, you'll need to set up port forward so you can view the dashboard on your local machine's browser. Once you've done that, or if you're processing on your own machine, you can view the dashboard at [http://localhost:7002/status](http://localhost:7002/status)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Enough already, let's get to work with some lidar data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll define where we can find the binary executables for LAStools and FUSION command line tools."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "las = lastools.useLAStools('/storage/lidar/LAStools/bin')\n",
    "fus = fusion.useFUSION('/storage/lidar/FUSION/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll create working directories for raw (imported with modest clean-up from source files), interim, and processed data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# where the raw lidar data is currently stored\n",
    "src_tiles = '/storage/lidar/OregonCIG_Lidar/olc_metro_2014/POINTS/HYDRO_FLATTENED_POINTS/*.laz'\n",
    "workdir = os.path.abspath('/storage/lidar/olc_metro_2014')\n",
    "\n",
    "target_epsg = 2992 # oregon lambert (ft) coordinate reference system\n",
    "num_cores = len(c.ncores())\n",
    "\n",
    "# define data handling directories\n",
    "raw, interim, processed = os.path.join(workdir,'raw'), os.path.join(workdir,'interim'), os.path.join(workdir,'processed')\n",
    "\n",
    "# push our working directories and wrapper classes to the workers on the cluster as well\n",
    "c.scatter([raw, interim, processed, las, fus, target_epsg, num_cores], broadcast=True);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wine_prefix = ['/storage/wine/.wine-{}'.format(x) for x in range(len(c.ncores()))]\n",
    "# c.scatter(wine_prefix, broadcast=False);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Get the raw data into our working directory\n",
    "First, move the tiles over to our working directory.\n",
    "\n",
    "When we define functions using the `dask.delayed` decorator, the function will have 'lazy' instead of 'eager' execution. We can map the function to a list of inputs and it will not execute for any of them until we ask for results to be computed. When we use the `compute()` method for the client managing the scheduler that sends jobs to the workers, it then starts running the jobs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c4ef90cf83924e9dbb99d428821b4ad5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "@dask.delayed\n",
    "def import_tiles(infile): # the function we'll map to a list of inputs\n",
    "    return las.las2las(i=infile,\n",
    "                       odir=raw,\n",
    "                       drop_withheld=True, # drop any points flagged as withheld by vendor\n",
    "                       drop_class=(7,18), # drop any points classified as noise by vendor\n",
    "                       clip_to_bounding_box=True, # make sure corrupted files don't change bbox\n",
    "                       set_version=1.4, # upgrade to latest LAS file version\n",
    "                       epsg=target_epsg, # specify the CRS in case it isn't automatically recognized\n",
    "                       set_ogc_wkt=True, # make sure CRS is recorded in VLR of las file\n",
    "                       olaz=True)\n",
    "\n",
    "infiles = glob.glob(src_tiles) # input files we'll map the function to\n",
    "imp_results = c.compute([import_tiles(file) for file in infiles]) # begin computation\n",
    "progress(imp_results) # monitor progress"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, create spatial indexes for the input files to allow fast spatial queries (which are used, for example, when retiling and adding buffers)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "26f72302088b4cc98c63da9041c3b90a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "@dask.delayed\n",
    "def make_index(infile): # the function we'll map to a list of inputs\n",
    "    return las.lasindex(i=infile) \n",
    "\n",
    "infiles = glob.glob(os.path.join(raw, '*.laz'))\n",
    "index_results = c.compute([make_index(file) for file in infiles])\n",
    "progress(index_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Retile the data to add buffers for avoiding edge effects during processing.\n",
    "\n",
    "In practice, executing the `lastile` command on individual tiles in parallel is likely to corrupt your output files. I suspect this is because the dynamic re-tiling of input files means that many output tiles are likely to require inputs from multiple input files, and that parallel processing outside of LAStools may result in collisions writing data from multiple inputs to these output tiles. So, for this case, we'll let `lastile` handle the parallelism under the hood. We won't have a progress bar, but this shouldn't take more than 5-10 minutes per ~100 tiles (with vendor tile size ~1000x1000m with 4-8 pts/m2).\n",
    "\n",
    "**THERE ARE ARGUMENTS IN THE FOLLOWING COMMAND THAT DEPEND UPON THE UNITS OF THE DATA.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "tile_proc = las.lastile(i=os.path.join(raw, '*.laz'),\n",
    "                   tile_size=4000, # in units of lidar data\n",
    "                   buffer=100, # assumes units are in feet... if using meters, suggest changing to ~25\n",
    "                   flag_as_withheld=True, # flag buffer points as \"withheld\", enables handling with other LAStools\n",
    "                   extra_pass=True, # if outputting to LAZ format, can help avoid memory limits\n",
    "                   full_bb=True,\n",
    "                   olaz=True,\n",
    "                   odir=os.path.join(interim, 'retiled'),\n",
    "                   cores=num_cores);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the original tiles delivered by the vendor included overlapping edges, our retiling may result in duplicated points in the new tiles from overlapping edges of vendor-provided input tiles. In the next step, we will ensure that only one point with unique (X,Y,Z) coordinates are retained in the point cloud data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dask.delayed\n",
    "def remove_dupes(infile): # the function we'll map to a list of inputs\n",
    "    return las.lasduplicate(i=infile,\n",
    "                            unique_xyz=True,\n",
    "                            olaz=True,\n",
    "                            odir=os.path.join(interim, 'deduped')) \n",
    "\n",
    "infiles = glob.glob(os.path.join(interim, 'retiled', '*.laz')\n",
    "dedupe_results = c.compute([remove_dupes(file) for file in infiles])\n",
    "progress(dedupe_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to confirm that the point cloud files you've retiled haven't been corrupted (i.e., they still match valid LAS format specifications), you can use the following function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dask.delayed\n",
    "def validate(infile): # the function we'll map to a list of inputs\n",
    "    return las.lasvalidate(i=infile)\n",
    "\n",
    "infiles = glob.glob(os.path.join(interim, 'deduped', '*.laz'))\n",
    "val_results = c.compute([validate(file) for file in infiles])\n",
    "progress(val_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check to see if any of the files failed validation\n",
    "corrupted = [val_results[i].result().args[3] for i in range(len(val_results)) if 'fail' in val_results[i].result().stderr.decode()]\n",
    "corrupted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Classify points in the lidar point cloud\n",
    "First we'll remove points that are isolated as likely noise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dask.delayed\n",
    "def denoise(tile_id): # the function we'll map to a list of inputs\n",
    "    basename = tile_id + '.laz'\n",
    "    infile = os.path.join(interim, 'deduped', basename)\n",
    "    odir = os.path.join(interim, 'denoised')\n",
    "    \n",
    "    if not os.path.exists(os.path.join(odir, basename)):\n",
    "        # file doesn't already exist\n",
    "        proc= las.lasnoise(i=infile,\n",
    "                           remove_noise=True,\n",
    "                           odir=odir,\n",
    "                           olaz=True)\n",
    "    else:\n",
    "        pass\n",
    "    \n",
    "    return tile_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, calculate the height aboveground for each point for use in classifying them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dask.delayed\n",
    "def hag(tile_id):\n",
    "    basename = tile_id + '.laz'\n",
    "    infile = os.path.join(interim, 'denoised', basename)\n",
    "    odir = os.path.join(interim, 'lasheight')\n",
    "    \n",
    "    if not os.path.exists(os.path.join(odir, basename)):\n",
    "        # file doesn't already exist\n",
    "        proc = las.lasheight(i=infile,\n",
    "                             odir=odir,\n",
    "                             olaz=True)\n",
    "    else:\n",
    "        pass\n",
    "    \n",
    "    return tile_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we'll classify points as building or high vegetation that meet certain criteria for 'planarity' or 'ruggedness'. \n",
    "\n",
    "**THERE ARE ARGUMENTS IN THE FOLLOWING COMMAND THAT DEPEND UPON THE UNITS OF THE DATA.**\n",
    "\n",
    "If your data are in meters, you should change these parameters, or consider reprojecting the data to a projection that is in feet when you copy the source data into our working directory using `las2las` command at the top of this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dask.delayed\n",
    "def classify(tile_id):\n",
    "    basename = tile_id + '.laz'\n",
    "    infile = os.path.join(interim, 'lasheight', basename)\n",
    "    odir = os.path.join(interim, 'classified')\n",
    "    \n",
    "    if not os.path.exists(os.path.join(odir, basename)):\n",
    "        # file doesn't already exist\n",
    "        proc = las.lasclassify(i=infile,\n",
    "                               odir=odir,\n",
    "                               olaz=True,\n",
    "                               step=5, # if your data are in meters, the LAStools default is 2.0\n",
    "                               planar=0.5, # if your data are in meters, the LAStools default is 0.1\n",
    "                               rugged=1) # if your data are in meters, the LAStools default is 0.4\n",
    "    else:\n",
    "        pass\n",
    "    \n",
    "    return tile_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll now remove the points in the buffered area of each tile and put the clean tiles in the processed folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dask.delayed\n",
    "def dropwithheld(tile_id):\n",
    "    basename = tile_id + '.laz'\n",
    "    infile = os.path.join(interim, 'classified', basename)\n",
    "    odir = os.path.join(processed, 'points')\n",
    "    \n",
    "    if not os.path.exists(os.path.join(odir, basename)):\n",
    "        # file doesn't already exist\n",
    "#         proc = las.las2las(i=infile,\n",
    "#                            odir=odir,\n",
    "#                            olaz=True,\n",
    "#                            drop_withheld=True, # remove points in tile buffers that were flagged as withhled with lastile\n",
    "#                            set_user_data=0) # remove height aboveground calculated using lasheight\n",
    "        proc = las.lastile(i=infile,\n",
    "                           odir=odir,\n",
    "                           olaz=True,\n",
    "                           extra_pass=True,\n",
    "                           full_bb=True, # keep bbox to full extent of tile, even if points don't use full extent\n",
    "                           remove_buffer=True, # remove points in tile buffers\n",
    "                           drop_withheld=True, # remove points flagged as withhled\n",
    "                           set_user_data=0) # remove height aboveground calculated using lasheight\n",
    "    else:\n",
    "        pass\n",
    "    \n",
    "    return tile_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll produce a shapefile showing the layout of the tiles as a single shapefile. This is a single process that takes a few seconds to run, so no need to distribute it using `dask`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dask.delayed\n",
    "def tiles_overview(*args, **kwargs):\n",
    "    infiles = os.path.join(processed, 'points', '*.laz')\n",
    "    odir = os.path.join(processed, 'vectors')\n",
    "    \n",
    "    if not os.path.exists(os.path.join(odir, 'tiles.shp')):\n",
    "        # file doesn't already exist\n",
    "        proc = las.lasboundary(i=infiles,\n",
    "                               odir=odir,\n",
    "                               o='tiles.shp',\n",
    "                               oshp=True,\n",
    "                               use_bb=True, # use bounding box of tiles\n",
    "                               overview=True,\n",
    "                               labels=True,\n",
    "                               cores=num_cores) # use parallel processing\n",
    "    else:\n",
    "        pass\n",
    "    \n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Generate a bare earth Digital Elevation Model\n",
    "Generate tiles of the bare earth model. This assumes that there are already ground-classified points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dask.delayed\n",
    "def make_dem(tile_id):\n",
    "    basename = tile_id + '.laz'\n",
    "    infile = os.path.join(interim, 'classified', basename)\n",
    "    odir = os.path.join(processed, 'rasters', 'DEM_tiles')\n",
    "    \n",
    "    if not os.path.exists(os.path.join(odir, basename)):\n",
    "        # file doesn't already exist\n",
    "        proc = las.las2dem(i=infile,\n",
    "                           odir=odir,\n",
    "                           otif=True, # create tiles as GeoTiff rasters\n",
    "                           keep_class=2, # keep ground-classified returns only\n",
    "                           step=1, # resolution of output raster, in units of lidar data\n",
    "                           thin_with_grid=1, # use a 1 x 1 resolution for creating the TIN for the DEM\n",
    "                           extra_pass=True, # uses two passes over data to execute DEM creation more efficiently\n",
    "                           use_tile_bb=True) # remove buffers from tiles\n",
    "    else:\n",
    "        pass\n",
    "    \n",
    "    return tile_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dask.delayed\n",
    "def add_crs_dem(tile_id):\n",
    "    basename = tile_id + '.tif'\n",
    "    infile = os.path.join(processed, 'rasters', 'DEM_tiles', basename)\n",
    "    \n",
    "    proc = subprocess.run(['rio', 'edit-info', '--crs', 'EPSG:{}'.format(target_epsg), infile],\n",
    "                   stderr=subprocess.PIPE, stdout=subprocess.PIPE)\n",
    "    \n",
    "    return tile_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Merge the bare earth tiles into a single GeoTiff."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dask.delayed\n",
    "def merge_dem(*args, **kwargs):\n",
    "    infiles = os.path.join(processed, 'rasters', 'DEM_tiles', '*.tif')\n",
    "    outfile = os.path.join(processed, 'rasters', 'dem.tif')\n",
    "    \n",
    "    if not os.path.exists(outfile):\n",
    "        # file doesn't already exist\n",
    "        return subprocess.run(['rio', 'merge', *glob.glob(infiles), outfile, '--co', 'compress=LZW'],\n",
    "                              stderr=subprocess.PIPE, stdout=subprocess.PIPE)\n",
    "    else:\n",
    "        return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To create a hillshade layer, we'll first, generate hillshade tiles from the bare earth model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dask.delayed\n",
    "def make_hillshade(tile_id):\n",
    "    basename = tile_id + '.laz'\n",
    "    infile = os.path.join(interim, 'classified', basename)\n",
    "    odir = os.path.join(processed, 'rasters', 'hillshade_tiles')\n",
    "    \n",
    "    if not os.path.exists(os.path.join(odir, basename)):\n",
    "        # file doesn't already exist\n",
    "        proc = las.las2dem(i=infile,\n",
    "                           odir=odir,\n",
    "                           otif=True, # create tiles as GeoTiffs\n",
    "                           hillshade=True,\n",
    "                           keep_class=2, # keep ground-classified returns only\n",
    "                           step=1, # resolution of output raster, in units of lidar data\n",
    "                           thin_with_grid=1, # use a 0.5 x 0.5 resolution for creating the TIN for the DEM\n",
    "                           extra_pass=True, # uses two passes over data to execute DEM creation more efficiently\n",
    "                           use_tile_bb=True) # remove buffers from tiles\n",
    "    else:\n",
    "        pass\n",
    "    \n",
    "    return tile_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dask.delayed\n",
    "def add_crs_hill(tile_id):\n",
    "    basename = tile_id + '.tif'\n",
    "    infile = os.path.join(processed, 'rasters', 'hillshade_tiles', basename)\n",
    "    \n",
    "    proc = subprocess.run(['rio', 'edit-info', '--crs', 'EPSG:{}'.format(target_epsg), infile],\n",
    "                   stderr=subprocess.PIPE, stdout=subprocess.PIPE)\n",
    "    \n",
    "    return tile_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now merge the hillshade tiles into a single raster formatted as GeoTiff."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dask.delayed\n",
    "def merge_hillshade(*args, **kwargs):\n",
    "    infiles = os.path.join(processed, 'rasters', 'hillshade_tiles', '*.tif')\n",
    "    outfile = os.path.join(processed, 'rasters', 'hillshade.tif')\n",
    "\n",
    "    if not os.path.exists(outfile):\n",
    "        # file doesn't already exist\n",
    "        return subprocess.run(['rio', 'merge', *glob.glob(infiles), outfile, '--co', 'compress=LZW'],\n",
    "                              stderr=subprocess.PIPE, stdout=subprocess.PIPE)\n",
    "    else:\n",
    "        pass\n",
    "    \n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Identify building footprints\n",
    "First start by building shapefiles showing building boundaries in each buffered tile."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dask.delayed\n",
    "def bldg_tiles(tile_id):\n",
    "    basename = tile_id + '.laz'\n",
    "    infile = os.path.join(interim, 'classified', basename)\n",
    "    odir = os.path.join(interim, 'building_tiles')\n",
    "    \n",
    "    if not os.path.exists(os.path.join(odir, basename)):\n",
    "        # file doesn't already exist\n",
    "        proc = las.lasboundary(i=infile,\n",
    "                               odir=odir,\n",
    "                               keep_class=6, # use only building-classified points\n",
    "                               disjoint=True, # compute separate polygons for each building\n",
    "                               concavity=3) # map concave boundary if edge length >= 3ft\n",
    "    else:\n",
    "        pass\n",
    "    \n",
    "    return tile_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate shapefiles showing the bounding box of each (unbuffered) tile that we'll use to remove buildings that fall in the buffered area."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dask.delayed\n",
    "def bbox_shp(tile_id):\n",
    "    basename = tile_id + '.laz'\n",
    "    infile = os.path.join(processed, 'points', basename)\n",
    "    odir = os.path.join(interim, 'tile_boundaries')\n",
    "    \n",
    "    if not os.path.exists(os.path.join(odir, basename)):\n",
    "        # file doesn't already exist\n",
    "        proc = las.lasboundary(i=infile,\n",
    "                               odir=odir,\n",
    "                               oshp=True,\n",
    "                               use_tile_bb=True)\n",
    "    else:\n",
    "        pass\n",
    "    \n",
    "    return tile_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each shapefile containing polygons of the building boundaries, we'll use the `clean_buffer_polys` function from `pyFIRS.utils` to remove polygons from a tile if their centroid falls in the buffered area of the tile."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dask.delayed\n",
    "def clean_bldgs(tile_id, *args):\n",
    "    if type(tile_id) == list:\n",
    "        tile_id = tile_id[0]\n",
    "    basename = tile_id + '.shp'\n",
    "    infile = os.path.join(interim, 'building_tiles', basename)\n",
    "    tile_shp = os.path.join(interim, 'tile_boundaries', basename)\n",
    "    odir = os.path.join(processed, 'vectors', 'building_tiles')\n",
    "    \n",
    "    if not os.path.exists(os.path.join(odir, basename)):\n",
    "        # file doesn't already exist\n",
    "        clean_buffer_polys(infile,\n",
    "                           tile_shp,\n",
    "                           odir=odir,\n",
    "                           simp_tol=3,\n",
    "                           simp_topol=True)\n",
    "    else:\n",
    "        pass\n",
    "    \n",
    "    return tile_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Merge the cleaned tiles of building footprints together into a single shapefile. We'll use `geopandas` to concatenate all the polygons together into a single geodataframe and then write out to a new shapefile."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dask.delayed\n",
    "def merge_bldgs(*args, **kwargs):\n",
    "    \n",
    "    if not os.path.exists(os.path.join(processed,'vectors','buildings.shp')):\n",
    "        # output file doesn't already exist\n",
    "        \n",
    "        building_tiles = glob.glob(os.path.join(processed, 'vectors', 'building_tiles', '*.shp'))\n",
    "        # create a list of geodataframes containing the tiles of building footprints\n",
    "        gdflist = [gpd.read_file(tile) for tile in building_tiles]\n",
    "        # merge them all together\n",
    "        merged = gpd.GeoDataFrame(pd.concat(gdflist, ignore_index=True))\n",
    "        # using pandas' concat caused us to lose projection information, so let's add that back in\n",
    "        merged.crs = gdflist[0].crs\n",
    "        # and write the merged data to a new shapefile\n",
    "        merged.to_file(os.path.join(processed,'vectors','buildings.shp'))\n",
    "    \n",
    "    else:\n",
    "        pass\n",
    "    \n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Create a Canopy Height Model\n",
    "We're going to switch use a FUSION command line tool to generate a Canopy Height Models (CHMs). \n",
    "\n",
    "### Using FUSION's `canopymodel` to generate CHMs\n",
    "`FUSION` wants to have ground models formatted as .dtm files, for CHM development and for estimating other canopy metrics. Let's generate these ground models first using a 1-meter x-y resolution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dask.delayed\n",
    "def groundDTMs(tile_id):\n",
    "    basename = tile_id + '.laz'\n",
    "    infile = os.path.join(interim, 'classified', basename)\n",
    "    odir = os.path.join(interim, 'dtm_ground_tiles')\n",
    "    outname = os.path.basename(infile).split('.')[0] + '.dtm'\n",
    "    outfile = os.path.join(odir, outname)\n",
    "    \n",
    "    if not os.path.exists(os.path.join(odir, basename)):\n",
    "        # file doesn't already exist\n",
    "        proc = fus.gridsurfacecreate(surfacefile=outfile,\n",
    "                                     cellsize=3.28084,\n",
    "                                     xyunits='F',\n",
    "                                     zunits='F',\n",
    "                                     coordsys=2, # in State Plane\n",
    "                                     zone=0, # not in UTM\n",
    "                                     horizdatum=2, # NAD83\n",
    "                                     vertdatum=2, # NAVD88\n",
    "                                     datafile=infile,\n",
    "                                     las_class=2, # keep only ground-classified points\n",
    "                                     odir=odir) # will make sure output directory is created if doesn't already exist\n",
    "    else:\n",
    "        pass\n",
    "    \n",
    "    return tile_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dask.delayed\n",
    "def canopymodel(tile_id):\n",
    "    basename = tile_id + '.laz'\n",
    "    infile = os.path.join(interim, 'classified', basename)\n",
    "    odir = os.path.join(interim, 'chm_tiles')\n",
    "    outname = os.path.basename(infile).split('.')[0] + '.dtm'\n",
    "    outfile = os.path.join(odir, outname)\n",
    "    \n",
    "    if not os.path.exists(os.path.join(odir, basename)):\n",
    "        # file doesn't already exist\n",
    "        proc = fus.canopymodel(surfacefile=outfile,\n",
    "                               cellsize=1,\n",
    "                               xyunits='F',\n",
    "                               zunits='F',\n",
    "                               coordsys=2, # in State Plane\n",
    "                               zone=0, # not in UTM\n",
    "                               horizdatum=2, # NAD83\n",
    "                               vertdatum=2, # NAVD88\n",
    "                               datafiles=infile,\n",
    "                               ground=os.path.join(interim, 'dtm_ground_tiles', outname),\n",
    "                               median=3, # median smoothing in 3x3 kernel\n",
    "                               las_class=(1,2,5), # keep only ground, unclassified, and high veg points\n",
    "                               asc=True, # also output in ascii format\n",
    "                               odir=odir) # will make sure output directory is created if doesn't already exist\n",
    "    else:\n",
    "        pass\n",
    "    \n",
    "    return tile_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert the ascii files that `canopymodel` generated into GeoTiffs, specifying their projection. Then cleanup the files `canopymodel` generated that we don't care about."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dask.delayed\n",
    "def asc2tif(tile_id):\n",
    "    basename = tile_id + '.asc'\n",
    "    infile = os.path.join(interim, 'chm_tiles', basename)\n",
    "    \n",
    "    if not os.path.exists(os.path.join(interim, 'chm_tiles', tile_id + '.tif')):\n",
    "        # output file doesn't already exist\n",
    "        convert_project(infile, '.tif', 'EPSG:2286')\n",
    "    else:\n",
    "        pass\n",
    "    \n",
    "    return tile_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clip the canopy height model tiles to remove overlapping areas that were from tile buffering to avoid edge effects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dask.delayed\n",
    "def clip(tile_id, *args):\n",
    "    if type(tile_id) == list:\n",
    "        tile_id = tile_id[0]\n",
    "    basename = tile_id + '.tif'\n",
    "    infile = os.path.join(interim, 'chm_tiles', basename)\n",
    "    in_shp = os.path.join(interim, 'tile_boundaries', tile_id + '.shp')\n",
    "    odir = os.path.join(processed, 'rasters', 'chm_tiles')\n",
    "    \n",
    "    if not os.path.exists(os.path.join(odir, basename)):\n",
    "        # output file doesn't already exist\n",
    "        clip_tile_from_shp(infile, in_shp, odir)\n",
    "    else:\n",
    "        pass\n",
    "    \n",
    "    return tile_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Merge the trimmed canopy height model tiles into a single raster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dask.delayed\n",
    "def merge_chm(*args, **kwargs):\n",
    "    infiles = os.path.join(processed, 'rasters', 'chm_tiles', '*.tif')\n",
    "    outfile = os.path.join(processed, 'rasters', 'chm.tif')\n",
    "    \n",
    "    if not os.path.exists(outfile):\n",
    "        # output file doesn't already exist\n",
    "        proc = subprocess.run(['rio', 'merge', *glob.glob(infiles), outfile, '--co', 'compress=LZW'],\n",
    "                              stderr=subprocess.PIPE, stdout=subprocess.PIPE)\n",
    "    else:\n",
    "        pass\n",
    "    \n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dask.delayed\n",
    "def done(*args, **kwargs):\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tile_ids = [os.path.basename(file).split('.')[0] for file in glob.glob(os.path.join(interim, 'deduped', '*.laz'))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dsk = {}\n",
    "for tile in tile_ids:\n",
    "    dsk['denoise-{}'.format(tile)]=(denoise, tile)\n",
    "    dsk['normalize-{}'.format(tile)]=(hag, 'denoise-{}'.format(tile))\n",
    "    dsk['classify-{}'.format(tile)]=(classify, 'normalize-{}'.format(tile))\n",
    "    dsk['drop-{}'.format(tile)]=(dropwithheld, 'classify-{}'.format(tile))\n",
    "    dsk['bbox-{}'.format(tile)]=(bbox_shp, 'drop-{}'.format(tile))\n",
    "    dsk['dem-{}'.format(tile)]=(make_dem, 'classify-{}'.format(tile))\n",
    "    dsk['prj_dem-{}'.format(tile)]=(add_crs_dem, 'dem-{}'.format(tile))\n",
    "    dsk['hill-{}'.format(tile)]=(make_hillshade, 'classify-{}'.format(tile))\n",
    "    dsk['prj_hill-{}'.format(tile)]=(add_crs_hill, 'hill-{}'.format(tile))\n",
    "    dsk['bldgs_buff-{}'.format(tile)]=(bldg_tiles, 'classify-{}'.format(tile))\n",
    "    dsk['bldgs_clean-{}'.format(tile)]=(clean_bldgs, ['bldgs_buff-{}'.format(tile), 'bbox-{}'.format(tile)])\n",
    "    dsk['ground_dtm-{}'.format(tile)]=(groundDTMs, 'classify-{}'.format(tile))\n",
    "    dsk['canopy-{}'.format(tile)]=(canopymodel, 'ground_dtm-{}'.format(tile))\n",
    "    dsk['canopy_tif-{}'.format(tile)] = (asc2tif, 'canopy-{}'.format(tile))\n",
    "    dsk['canopy_clip-{}'.format(tile)]=(clip, ['canopy_tif-{}'.format(tile), 'bbox-{}'.format(tile)])\n",
    "\n",
    "dsk['tiles_over'] = (tiles_overview, ['drop-{}'.format(tile) for tile in tile_ids])\n",
    "dsk['merge_bldgs'] = (merge_bldgs, ['bldgs_clean-{}'.format(tile) for tile in tile_ids])\n",
    "dsk['merge_hill'] = (merge_hillshade, ['prj_hill-{}'.format(tile) for tile in tile_ids])\n",
    "dsk['merge_dem'] = (merge_dem, ['prj_dem-{}'.format(tile) for tile in tile_ids])\n",
    "dsk['merge_chm'] = (merge_chm, ['canopy_clip-{}'.format(tile) for tile in tile_ids])\n",
    "dsk['done']=(done, ['tiles_over', 'merge_bldgs', 'merge_hill', 'merge_dem', 'merge_chm'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_big = c.get(dsk, 'done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_big.visualize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = c.persist(res_big)\n",
    "progress(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# get rid of the .tfw and .kml files that LAStools generates\n",
    "clean_dir(os.path.join(processed, 'rasters', 'DEM_tiles'), ['.tfw', '.kml'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# get rid of the .tfw and .kml files that LAStools generates\n",
    "clean_dir(os.path.join(processed, 'rasters', 'hillshade_tiles'), ['.tfw', '.kml'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# get rid of the .asc and .dtm files that FUSION generates\n",
    "clean_dir(os.path.join(interim, 'chm_tiles'), ['.asc', '.dtm'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# c.cancel(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c.close()\n",
    "cluster.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pyFIRS]",
   "language": "python",
   "name": "conda-env-pyFIRS-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
