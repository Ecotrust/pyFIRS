{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, shutil, glob, platform, subprocess\n",
    "import geopandas as gpd, pandas as pd\n",
    "import rasterio\n",
    "from matplotlib import pyplot as plt\n",
    "from pyFIRS.wrappers import lastools, fusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyFIRS.utils import clean_dir, clean_buffer_polys, clip_tile_from_shp, convert_project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask.distributed import Client, progress, LocalCluster\n",
    "from dask import delayed\n",
    "import dask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting up parallel computing using `dask.distributed`\n",
    "`LAStools` offers native multi-core processing as an optional argument (`cores`) supplied to its command-line tools. `FUSION` command line tools do not. To enable parallel processing of `FUSION` commands, we'll use `dask.distributed` to schedule the processing of tiles in asynchronous parallel batches. This approach also offers us the ability to track progress using a progress bar.\n",
    "\n",
    "You'll first need to launch a parallel computing cluster. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster=LocalCluster(scheduler_port=7001, diagnostics_port=7002)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = Client(cluster)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point, you should also be able to view an interactive dashboard on port 7002. If you're executing this on a remote server, you'll need to set up port forward so you can view the dashboard on your local machine's browser. Once you've done that, or if you're processing on your own machine, you can view the dashboard at [http://localhost:7002/status](http://localhost:7002/status)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Enough already, let's get to work with some lidar data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll define where we can find the binary executables for LAStools and FUSION command line tools."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "las = lastools.useLAStools('/storage/lidar/LAStools/bin')\n",
    "fus = fusion.useFUSION('/storage/lidar/FUSION/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll create working directories for raw (imported with modest clean-up from source files), interim, and processed data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# where the raw lidar data is currently stored\n",
    "src_tiles = '/storage/lidar/Swinomish_Lidar_2016/source/*.laz'\n",
    "workdir = os.path.abspath('/storage/lidar/Swinomish_Lidar_2016')\n",
    "\n",
    "# define data handling directories\n",
    "raw, interim, processed = os.path.join(workdir,'raw'), os.path.join(workdir,'interim'), os.path.join(workdir,'processed')\n",
    "\n",
    "# push our working directories and wrapper classes to the workers on the cluster as well\n",
    "c.scatter([raw, interim, processed, las, fus], broadcast=True);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wine_prefix = ['/storage/wine/.wine-{}'.format(x) for x in range(len(c.ncores()))]\n",
    "# c.scatter(wine_prefix, broadcast=False);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Get the raw data into our working directory\n",
    "First, move the tiles over to our working directory.\n",
    "\n",
    "When we define functions using the `dask.delayed` decorator, the function will have 'lazy' instead of 'eager' execution. That means we can map the function to a list of inputs and it will not execute for any of them until we ask for results to be computed. When we use the `compute()` method for the client managing the scheduler that sends jobs to the workers, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dask.delayed\n",
    "def import_tiles(infile): # the function we'll map to a list of inputs\n",
    "    return las.las2las(i=infile,\n",
    "                       odir=raw,\n",
    "                       drop_withheld=True, # drop any points flagged as withheld by vendor\n",
    "                       drop_class=(7,18), # drop any points classified as noise by vendor\n",
    "                       olaz=True)\n",
    "\n",
    "infiles = glob.glob(src_tiles) # input files we'll map the function to\n",
    "imp_results = c.compute([import_tiles(file) for file in infiles]) # begin computation\n",
    "progress(imp_results) # monitor progress"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, create spatial indexes for the input files to allow fast spatial queries (which we'll use for adding buffers)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dask.delayed\n",
    "def make_index(infile): # the function we'll map to a list of inputs\n",
    "    return las.lasindex(i=infile) \n",
    "\n",
    "infiles = glob.glob(os.path.join(raw, '*.laz'))\n",
    "index_results = c.compute([make_index(file) for file in infiles])\n",
    "progress(index_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Retile the data to add buffers for avoiding edge effects during processing.\n",
    "\n",
    "In practice, executing the `lastile` command on individual tiles in parallel is likely to corrupt your output files. I suspect this is because the dynamic re-tiling of input files means that many output tiles are likely to require inputs from multiple input files, and that parallel processing outside of LAStools may result in collisions writing data from multiple inputs to these output tiles. So, for this case, we'll let `lastile` handle the parallelism under the hood. We won't have a progress bar, but this shouldn't take more than 5-10 minutes.\n",
    "\n",
    "**THERE ARE ARGUMENTS IN THE FOLLOWING COMMAND THAT DEPEND UPON THE UNITS OF THE DATA.**\n",
    "\n",
    "The workflow demonstrated here is working in units of US feet on a dataset in Washington State Plane (South). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "las.lastile(i=os.path.join(raw, '*.laz'),\n",
    "            tile_size=4000, # in units of lidar data\n",
    "            buffer=100, # assumes units are in feet... if using meters, suggest changing to ~25\n",
    "            flag_as_withheld=True,\n",
    "            olaz=True,\n",
    "            odir=os.path.join(interim, 'retiled'),\n",
    "            cores=len(c.ncores()));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to confirm that the point cloud files you've retiled haven't been corrupted (i.e., they still match valid LAS format specifications), you can use the following function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dask.delayed\n",
    "def validate(infile): # the function we'll map to a list of inputs\n",
    "    return las.lasvalidate(i=infile)\n",
    "\n",
    "infiles = glob.glob(os.path.join(interim, 'retiled', '*.laz'))\n",
    "val_results = c.compute([validate(file) for file in infiles])\n",
    "progress(val_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check to see if any of the files failed validation\n",
    "corrupted = [val_results[i].result().args[3] for i in range(len(val_results)) if 'fail' in val_results[i].result().stderr.decode()]\n",
    "corrupted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Classify points in the lidar point cloud\n",
    "First we'll remove points that are isolated as likely noise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dask.delayed\n",
    "def denoise(tile_id): # the function we'll map to a list of inputs\n",
    "    basename = tile_id + '.laz'\n",
    "    infile = os.path.join(interim, 'retiled', basename)\n",
    "    \n",
    "    proc= las.lasnoise(i=infile,\n",
    "                 remove_noise=True,\n",
    "                 odir=os.path.join(interim, 'denoised'),\n",
    "                 olaz=True) \n",
    "    \n",
    "    return tile_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, calculate the height aboveground for each point for use in classifying them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dask.delayed\n",
    "def hag(tile_id):\n",
    "    basename = tile_id + '.laz'\n",
    "    infile = os.path.join(interim, 'denoised', basename)\n",
    "    \n",
    "    proc = las.lasheight(i=infile,\n",
    "                  odir=os.path.join(interim, 'lasheight'),\n",
    "                  olaz=True)\n",
    "    \n",
    "    return tile_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we'll classify points as building or high vegetation that meet certain criteria for 'planarity' or 'ruggedness'. \n",
    "\n",
    "**THERE ARE ARGUMENTS IN THE FOLLOWING COMMAND THAT DEPEND UPON THE UNITS OF THE DATA.**\n",
    "\n",
    "If your data are in meters, you should change these parameters, or consider reprojecting the data to a projection that is in feet when you copy the source data into our working directory using `las2las` command at the top of this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dask.delayed\n",
    "def classify(tile_id):\n",
    "    basename = tile_id + '.laz'\n",
    "    infile = os.path.join(interim, 'lasheight', basename)\n",
    "    \n",
    "    proc = las.lasclassify(i=infile,\n",
    "                    odir=os.path.join(interim, 'classified'),\n",
    "                    olaz=True,\n",
    "                    step=5, # if your data are in meters, the LAStools default is 2.0\n",
    "                    planar=0.5, # if your data are in meters, the LAStools default is 0.1\n",
    "                    rugged=1) # if your data are in meters, the LAStools default is 0.4\n",
    "    \n",
    "    return tile_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll now remove the points in the buffered area of each tile and put the clean tiles in the processed folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dask.delayed\n",
    "def dropwithheld(tile_id):\n",
    "    basename = tile_id + '.laz'\n",
    "    infile = os.path.join(interim, 'classified', basename)\n",
    "    \n",
    "    proc = las.las2las(i=infile,\n",
    "                odir=os.path.join(processed, 'points'),\n",
    "                olaz=True,\n",
    "                drop_withheld=True, # remove points in tile buffers that were flagged as withhled with lastile\n",
    "                set_user_data=0) # remove height aboveground calculated using lasheight\n",
    "    \n",
    "    return tile_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll produce a shapefile showing the layout of the tiles as a single shapefile. This is a single process that takes a few seconds to run, so no need to distribute it using `dask`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dask.delayed\n",
    "def tiles_overview(*args, **kwargs):\n",
    "    infiles = os.path.join(processed, 'points', '*.laz')\n",
    "    odir = os.path.join(processed, 'vectors')\n",
    "    \n",
    "    proc = las.lasboundary(i=infiles,\n",
    "                odir=odir,\n",
    "                o='tiles.shp',\n",
    "                oshp=True,\n",
    "                use_bb=True, # use bounding box of tiles\n",
    "                overview=True,\n",
    "                labels=True,\n",
    "                cores=32) # use parallel processing\n",
    "    \n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Generate a bare earth Digital Elevation Model\n",
    "Generate tiles of the bare earth model. This assumes that there are already ground-classified points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dask.delayed\n",
    "def make_dem(tile_id):\n",
    "    basename = tile_id + '.laz'\n",
    "    infile = os.path.join(interim, 'classified', basename)\n",
    "    \n",
    "    proc = las.las2dem(i=infile,\n",
    "                odir=os.path.join(processed, 'rasters', 'DEM_tiles'),\n",
    "                otif=True, # create tiles as GeoTiff rasters\n",
    "                keep_class=2, # keep ground-classified returns only\n",
    "                step=1, # resolution of output raster, in units of lidar data\n",
    "                thin_with_grid=1, # use a 1 x 1 resolution for creating the TIN for the DEM\n",
    "                extra_pass=True, # uses two passes over data to execute DEM creation more efficiently\n",
    "                use_tile_bb=True) # remove buffers from tiles\n",
    "    \n",
    "    return tile_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dask.delayed\n",
    "def add_crs_dem(tile_id):\n",
    "    basename = tile_id + '.tif'\n",
    "    infile = os.path.join(processed, 'rasters', 'DEM_tiles', basename)\n",
    "    \n",
    "    proc = subprocess.run(['rio', 'edit-info', '--crs', 'EPSG:2286', infile],\n",
    "                   stderr=subprocess.PIPE, stdout=subprocess.PIPE)\n",
    "    \n",
    "    return tile_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removed 88 files with extension .tfw.\n",
      "Removed 88 files with extension .kml.\n",
      "CPU times: user 0 ns, sys: 8 ms, total: 8 ms\n",
      "Wall time: 7.74 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# get rid of the .tfw and .kml files that LAStools generates\n",
    "clean_dir(os.path.join(processed, 'rasters', 'DEM_tiles'), ['.tfw', '.kml'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Merge the bare earth tiles into a single GeoTiff."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dask.delayed\n",
    "def merge_dem(*args, **kwargs):\n",
    "    infiles = os.path.join(processed, 'rasters', 'DEM_tiles', '*.tif')\n",
    "    outfile = os.path.join(processed, 'rasters', 'dem.tif')\n",
    "    return subprocess.run(['rio', 'merge', *glob.glob(infiles), outfile, '--co', 'compress=LZW'],\n",
    "                          stderr=subprocess.PIPE, stdout=subprocess.PIPE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To create a hillshade layer, we'll first, generate hillshade tiles from the bare earth model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dask.delayed\n",
    "def make_hillshade(tile_id):\n",
    "    basename = tile_id + '.laz'\n",
    "    infile = os.path.join(interim, 'classified', basename)\n",
    "    \n",
    "    proc = las.las2dem(i=infile,\n",
    "                odir=os.path.join(processed, 'rasters', 'hillshade_tiles'),\n",
    "                otif=True, # create tiles as GeoTiffs\n",
    "                hillshade=True,\n",
    "                keep_class=2, # keep ground-classified returns only\n",
    "                step=1, # resolution of output raster, in units of lidar data\n",
    "                thin_with_grid=1, # use a 0.5 x 0.5 resolution for creating the TIN for the DEM\n",
    "                extra_pass=True, # uses two passes over data to execute DEM creation more efficiently\n",
    "                use_tile_bb=True) # remove buffers from tiles\n",
    "    \n",
    "    return tile_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dask.delayed\n",
    "def add_crs_hill(tile_id):\n",
    "    basename = tile_id + '.tif'\n",
    "    infile = os.path.join(processed, 'rasters', 'hillshade_tiles', basename)\n",
    "    \n",
    "    proc = subprocess.run(['rio', 'edit-info', '--crs', 'EPSG:2286', infile],\n",
    "                   stderr=subprocess.PIPE, stdout=subprocess.PIPE)\n",
    "    \n",
    "    return tile_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removed 88 files with extension .tfw.\n",
      "Removed 88 files with extension .kml.\n",
      "CPU times: user 0 ns, sys: 8 ms, total: 8 ms\n",
      "Wall time: 7.47 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# get rid of the .tfw and .kml files that LAStools generates\n",
    "clean_dir(os.path.join(processed, 'rasters', 'hillshade_tiles'), ['.tfw', '.kml'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now merge the hillshade tiles into a single raster formatted as GeoTiff."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dask.delayed\n",
    "def merge_hillshade(*args, **kwargs):\n",
    "    infiles = os.path.join(processed, 'rasters', 'hillshade_tiles', '*.tif')\n",
    "    outfile = os.path.join(processed, 'rasters', 'hillshade.tif')\n",
    "\n",
    "    subprocess.run(['rio', 'merge', *glob.glob(infiles), outfile, '--co', 'compress=LZW'],\n",
    "                          stderr=subprocess.PIPE, stdout=subprocess.PIPE)\n",
    "    \n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Identify building footprints\n",
    "First start by building shapefiles showing building boundaries in each buffered tile."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dask.delayed\n",
    "def bldg_tiles(tile_id):\n",
    "    basename = tile_id + '.laz'\n",
    "    infile = os.path.join(interim, 'classified', basename)\n",
    "    \n",
    "    proc = las.lasboundary(i=infile,\n",
    "                    odir=os.path.join(interim, 'building_tiles'),\n",
    "                    keep_class=6, # use only building-classified points\n",
    "                    disjoint=True, # compute separate polygons for each building\n",
    "                    concavity=3) # map concave boundary if edge length >= 3ft\n",
    "    \n",
    "    return tile_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate shapefiles showing the bounding box of each (unbuffered) tile that we'll use to remove buildings that fall in the buffered area."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dask.delayed\n",
    "def bbox_shp(tile_id):\n",
    "    basename = tile_id + '.laz'\n",
    "    infile = os.path.join(processed, 'points', basename)\n",
    "    \n",
    "    proc = las.lasboundary(i=infile,\n",
    "                    odir=os.path.join(interim, 'tile_boundaries'),\n",
    "                    oshp=True,\n",
    "                    use_tile_bb=True)\n",
    "    \n",
    "    return tile_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each shapefile containing polygons of the building boundaries, we'll use the `clean_buffer_polys` function from `pyFIRS.utils` to remove polygons from a tile if their centroid falls in the buffered area of the tile."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dask.delayed\n",
    "def clean_bldgs(tile_id, *args):\n",
    "    if type(tile_id) == list:\n",
    "        tile_id = tile_id[0]\n",
    "    basename = tile_id + '.shp'\n",
    "    infile = os.path.join(interim, 'building_tiles', basename)\n",
    "    tile_shp = os.path.join(interim, 'tile_boundaries', basename)\n",
    "    \n",
    "    clean_buffer_polys(infile,\n",
    "                       tile_shp,\n",
    "                       odir=os.path.join(processed, 'vectors', 'building_tiles'),\n",
    "                       simp_tol=3,\n",
    "                       simp_topol=True)\n",
    "    \n",
    "    return tile_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Merge the cleaned tiles of building footprints together into a single shapefile. We'll use `geopandas` to concatenate all the polygons together into a single geodataframe and then write out to a new shapefile."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dask.delayed\n",
    "def merge_bldgs(*args, **kwargs):\n",
    "    building_tiles = glob.glob(os.path.join(processed, 'vectors', 'building_tiles', '*.shp'))\n",
    "    # create a list of geodataframes containing the tiles of building footprints\n",
    "    gdflist = [gpd.read_file(tile) for tile in building_tiles]\n",
    "    # merge them all together\n",
    "    merged = gpd.GeoDataFrame(pd.concat(gdflist, ignore_index=True))\n",
    "    # using pandas' concat caused us to lose projection information, so let's add that back in\n",
    "    merged.crs = gdflist[0].crs\n",
    "    # and write the merged data to a new shapefile\n",
    "    merged.to_file(os.path.join(processed,'vectors','buildings.shp'))\n",
    "    \n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Create a Canopy Height Model\n",
    "We're going to switch use a FUSION command line tool to generate a Canopy Height Models (CHMs). \n",
    "\n",
    "### Using FUSION's `canopymodel` to generate CHMs\n",
    "`FUSION` wants to have ground models formatted as .dtm files, for CHM development and for estimating other canopy metrics. Let's generate these ground models first using a 1-meter x-y resolution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dask.delayed\n",
    "def groundDTMs(tile_id):\n",
    "    basename = tile_id + '.laz'\n",
    "    infile = os.path.join(interim, 'classified', basename)\n",
    "    odir = os.path.join(interim, 'dtm_ground_tiles')\n",
    "    outname = os.path.basename(infile).split('.')[0] + '.dtm'\n",
    "    outfile = os.path.join(odir, outname)\n",
    "    \n",
    "    proc = fus.gridsurfacecreate(surfacefile=outfile,\n",
    "                          cellsize=3.28084,\n",
    "                          xyunits='F',\n",
    "                          zunits='F',\n",
    "                          coordsys=2, # in State Plane\n",
    "                          zone=0, # not in UTM\n",
    "                          horizdatum=2, # NAD83\n",
    "                          vertdatum=2, # NAVD88\n",
    "                          datafile=infile,\n",
    "                          las_class=2, # keep only ground-classified points\n",
    "                          odir=odir) # will make sure output directory is created if doesn't already exist\n",
    "    \n",
    "    return tile_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dask.delayed\n",
    "def canopymodel(tile_id):\n",
    "    basename = tile_id + '.laz'\n",
    "    infile = os.path.join(interim, 'classified', basename)\n",
    "    odir = os.path.join(interim, 'chm_tiles')\n",
    "    outname = os.path.basename(infile).split('.')[0] + '.dtm'\n",
    "    outfile = os.path.join(odir, outname)\n",
    "    \n",
    "    proc = fus.canopymodel(surfacefile=outfile,\n",
    "                    cellsize=1,\n",
    "                    xyunits='F',\n",
    "                    zunits='F',\n",
    "                    coordsys=2, # in State Plane\n",
    "                    zone=0, # not in UTM\n",
    "                    horizdatum=2, # NAD83\n",
    "                    vertdatum=2, # NAVD88\n",
    "                    datafiles=infile,\n",
    "                    ground=os.path.join(interim, 'dtm_ground_tiles', outname),\n",
    "                    median=3, # median smoothing in 3x3 kernel\n",
    "                    las_class=(1,2,5), # keep only ground, unclassified, and high veg points\n",
    "                    asc=True, # also output in ascii format\n",
    "                    odir=odir) # will make sure output directory is created if doesn't already exist\n",
    "    \n",
    "    return tile_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert the ascii files that `canopymodel` generated into GeoTiffs, specifying their projection. Then cleanup the files `canopymodel` generated that we don't care about."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dask.delayed\n",
    "def asc2tif(tile_id):\n",
    "    basename = tile_id + '.asc'\n",
    "    infile = os.path.join(interim, 'chm_tiles', basename)\n",
    "    \n",
    "    convert_project(infile, '.tif', 'EPSG:2286')\n",
    "    \n",
    "    return tile_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removed 85 files with extension .asc.\n",
      "Removed 85 files with extension .dtm.\n"
     ]
    }
   ],
   "source": [
    "clean_dir(os.path.join(interim, 'chm_tiles'), ['.asc', '.dtm'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clip the canopy height model tiles to remove overlapping areas that were from tile buffering to avoid edge effects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dask.delayed\n",
    "def clip(tile_id, *args):\n",
    "    if type(tile_id) == list:\n",
    "        tile_id = tile_id[0]\n",
    "    basename = tile_id + '.tif'\n",
    "    infile = os.path.join(interim, 'chm_tiles', basename)\n",
    "    in_shp = os.path.join(interim, 'tile_boundaries', tile_id + '.shp')\n",
    "    odir = os.path.join(processed, 'rasters', 'chm_tiles')\n",
    "    \n",
    "    clip_tile_from_shp(infile, in_shp, odir)\n",
    "    \n",
    "    return tile_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Merge the trimmed canopy height model tiles into a single raster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dask.delayed\n",
    "def merge_chm(*args, **kwargs):\n",
    "    infiles = os.path.join(processed, 'rasters', 'chm_tiles', '*.tif')\n",
    "    outfile = os.path.join(processed, 'rasters', 'chm.tif')\n",
    "\n",
    "    proc = subprocess.run(['rio', 'merge', *glob.glob(infiles), outfile, '--co', 'compress=LZW'],\n",
    "                          stderr=subprocess.PIPE, stdout=subprocess.PIPE)\n",
    "    \n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dask.delayed\n",
    "def done(*args, **kwargs):\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "tile_ids = [os.path.basename(file).split('.')[0] for file in glob.glob(os.path.join(interim, 'retiled', '*.laz'))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "dsk = {}\n",
    "for tile in tile_ids:\n",
    "    dsk['denoise-{}'.format(tile)]=(denoise, tile)\n",
    "    dsk['normalize-{}'.format(tile)]=(hag, 'denoise-{}'.format(tile))\n",
    "    dsk['classify-{}'.format(tile)]=(classify, 'normalize-{}'.format(tile))\n",
    "    dsk['drop-{}'.format(tile)]=(dropwithheld, 'classify-{}'.format(tile))\n",
    "    dsk['bbox-{}'.format(tile)]=(bbox_shp, 'drop-{}'.format(tile))\n",
    "    dsk['dem-{}'.format(tile)]=(make_dem, 'classify-{}'.format(tile))\n",
    "    dsk['prj_dem-{}'.format(tile)]=(add_crs_dem, 'dem-{}'.format(tile))\n",
    "    dsk['hill-{}'.format(tile)]=(make_hillshade, 'classify-{}'.format(tile))\n",
    "    dsk['prj_hill-{}'.format(tile)]=(add_crs_hill, 'hill-{}'.format(tile))\n",
    "    dsk['bldgs_buff-{}'.format(tile)]=(bldg_tiles, 'classify-{}'.format(tile))\n",
    "    dsk['bldgs_clean-{}'.format(tile)]=(clean_bldgs, ['bldgs_buff-{}'.format(tile), 'bbox-{}'.format(tile)])\n",
    "    dsk['ground_dtm-{}'.format(tile)]=(groundDTMs, 'classify-{}'.format(tile))\n",
    "    dsk['canopy-{}'.format(tile)]=(canopymodel, 'ground_dtm-{}'.format(tile))\n",
    "    dsk['canopy_tif-{}'.format(tile)] = (asc2tif, 'canopy-{}'.format(tile))\n",
    "    dsk['canopy_clip-{}'.format(tile)]=(clip, ['canopy_tif-{}'.format(tile), 'bbox-{}'.format(tile)])\n",
    "\n",
    "dsk['tiles_over'] = (tiles_overview, ['drop-{}'.format(tile) for tile in tile_ids])\n",
    "dsk['merge_bldgs'] = (merge_bldgs, ['bldgs_clean-{}'.format(tile) for tile in tile_ids])\n",
    "dsk['merge_hill'] = (merge_hillshade, ['prj_hill-{}'.format(tile) for tile in tile_ids])\n",
    "dsk['merge_dem'] = (merge_dem, ['prj_dem-{}'.format(tile) for tile in tile_ids])\n",
    "dsk['merge_chm'] = (merge_chm, ['canopy_clip-{}'.format(tile) for tile in tile_ids])\n",
    "dsk['done']=(done, ['tiles_over', 'merge_bldgs', 'merge_hill', 'merge_dem', 'merge_chm'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_big = c.get(dsk, 'done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_big.visualize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "35254791718b46089a3b7644cd3adb9e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <bound method GraphPlot.__del__ of <distributed.bokeh.scheduler.GraphPlot object at 0x7ffb941dca20>>\n",
      "Traceback (most recent call last):\n",
      "  File \"/storage/anaconda3/envs/pyFIRS/lib/python3.6/site-packages/distributed/bokeh/scheduler.py\", line 741, in __del__\n",
      "    self.scheduler.remove_plugin(self.layout)\n",
      "  File \"/storage/anaconda3/envs/pyFIRS/lib/python3.6/site-packages/distributed/scheduler.py\", line 2172, in remove_plugin\n",
      "    self.plugins.remove(plugin)\n",
      "ValueError: list.remove(x): x not in list\n",
      "Exception ignored in: <bound method GraphPlot.__del__ of <distributed.bokeh.scheduler.GraphPlot object at 0x7ffaf470e7f0>>\n",
      "Traceback (most recent call last):\n",
      "  File \"/storage/anaconda3/envs/pyFIRS/lib/python3.6/site-packages/distributed/bokeh/scheduler.py\", line 741, in __del__\n",
      "    self.scheduler.remove_plugin(self.layout)\n",
      "  File \"/storage/anaconda3/envs/pyFIRS/lib/python3.6/site-packages/distributed/scheduler.py\", line 2172, in remove_plugin\n",
      "    self.plugins.remove(plugin)\n",
      "ValueError: list.remove(x): x not in list\n"
     ]
    }
   ],
   "source": [
    "res = c.persist(res_big)\n",
    "progress(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#c.cancel(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "distributed.comm.tcp - WARNING - Closing dangling stream in <TCP local=tcp://127.0.0.1:42593 remote=tcp://127.0.0.1:7001>\n",
      "distributed.comm.tcp - WARNING - Closing dangling stream in <TCP local=tcp://127.0.0.1:42681 remote=tcp://127.0.0.1:7001>\n"
     ]
    }
   ],
   "source": [
    "c.close()\n",
    "cluster.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pyFIRS]",
   "language": "python",
   "name": "conda-env-pyFIRS-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
