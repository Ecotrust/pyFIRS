{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import subprocess\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import dask\n",
    "from dask.distributed import Client, progress, LocalCluster\n",
    "from pyFIRS.wrappers import lastools\n",
    "from pyFIRS.utils import fname"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Launch a parallel computing cluster. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster=LocalCluster(scheduler_port=7001, diagnostics_port=7002)\n",
    "c = Client(cluster)\n",
    "num_cores = len(c.ncores()) # identify how many workers we have"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point, you should also be able to view an interactive dashboard on port 7002. If you're executing this on a remote server, you'll need to set up port forward so you can view the dashboard on your local machine's browser. Once you've done that, or if you're processing on your own machine, you can view the dashboard at [http://localhost:7002/status](http://localhost:7002/status)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "las = lastools.useLAStools('/storage/lidar/LAStools/bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# where the imported lidar data is currently stored\n",
    "WORKDIR = os.path.abspath('/storage/lidar/klamath_2010/')\n",
    "# define data handling directories\n",
    "INTERIM = os.path.join(WORKDIR, 'interim')\n",
    "PROCESSED = os.path.join(WORKDIR,'processed')\n",
    "\n",
    "# the coordinate reference system we'll be working with\n",
    "TARGET_EPSG = 6339 # utm 10N, NAD83_2011"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# push our working directories and wrapper classes to the workers on the cluster as well\n",
    "c.scatter([INTERIM, PROCESSED, las, TARGET_EPSG, num_cores], broadcast=True);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tiles_to_merge = [fname(tile) for tile in \n",
    "                  glob.glob(os.path.join(PROCESSED, 'points', '*.laz'))\n",
    "                 ]\n",
    "\n",
    "print('Found {:,d} tiles to merge derivative products from.'.format(len(tiles_to_merge)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Merge tiled derivative outputs together\n",
    "Merge all the tiled GeoTiffs and Shapefiles into single overview files."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll produce a shapefile showing the layout of the non-buffered tiles as a single shapefile. This is a single process that takes a few seconds to run, so no need to distribute it using `dask`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dask.delayed\n",
    "def tile_boundaries(*args, **kwargs):\n",
    "    odir = os.path.join(PROCESSED, 'vectors')\n",
    "    \n",
    "    if os.path.exists(os.path.join(PROCESSED, 'vectors', 'tiles.shp')):\n",
    "        pass\n",
    "    else:\n",
    "        proc = las.lasboundary(i=os.path.join(PROCESSED, 'points', '*.laz'),\n",
    "                               use_bb=True, # use bounding box of tiles\n",
    "                               overview=True,\n",
    "                               labels=True,\n",
    "                               cores=num_cores, # use parallel processing\n",
    "                               oshp=True,\n",
    "                               o=os.path.join(PROCESSED, 'vectors', 'tiles.shp'))\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dask.delayed\n",
    "def make_footprint(*args, **kwargs):\n",
    "    if os.path.exists(os.path.join(PROCESSED, 'vectors', 'footprint.shp')):\n",
    "        pass\n",
    "    else:\n",
    "        gdf = gpd.read_file(os.path.join(PROCESSED, 'vectors', 'tiles.shp'))\n",
    "        gdf['mil_points'] = gdf['num_points'] / 1000000.\n",
    "        buffered = gdf.drop(['file_name', 'point_size', 'point_type', 'num_points'], axis=1)\n",
    "        buffered.geometry = gdf.buffer(0.01) # buffer by 1cm\n",
    "        \n",
    "        try:\n",
    "            union = gpd.GeoDataFrame(geometry=list(buffered.unary_union), crs=buffered.crs)\n",
    "        except TypeError: # line above will fail if there is only one polygon for the footprint\n",
    "            union = gpd.GeoDataFrame(geometry=[buffered.unary_union], crs=buffered.crs)\n",
    "    \n",
    "        union['footprint_id'] = union.index + 1\n",
    "        \n",
    "        buffered = gpd.tools.sjoin(buffered, union, how='left').drop('index_right', axis=1)\n",
    "        \n",
    "        aggfuncs={'mil_points':'sum', \n",
    "          'version':'first', \n",
    "          'min_x':'min',\n",
    "          'min_y':'min',\n",
    "          'min_z':'min',\n",
    "          'max_x':'max',\n",
    "          'max_y':'max',\n",
    "          'max_z':'max'}\n",
    "        \n",
    "        dissolved = buffered.dissolve(by='footprint_id', aggfunc=aggfuncs)\n",
    "        OUTFILE = os.path.join(PROCESSED, 'vectors', 'footprint.shp')\n",
    "        dissolved.to_file(OUTFILE)\n",
    "        \n",
    "        return 'footprint'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_chunks(infiles, outfile):\n",
    "    \"\"\"Merges a list of rasters, one chunk at a time.\n",
    "    \n",
    "    Parameters\n",
    "    -----------\n",
    "    infiles : list\n",
    "        list containing paths to input files\n",
    "    outfile : string, path to file\n",
    "        the output file that will be created by merging all the input files\n",
    "    \"\"\"\n",
    "    if os.path.exists(outfile):\n",
    "        pass\n",
    "    else:\n",
    "        outname = os.path.basename(outfile).split('.')[0]\n",
    "        OUTDIR = os.path.join(PROCESSED, 'rasters','chunks')\n",
    "        os.makedirs(OUTDIR, exist_ok=True)\n",
    "        \n",
    "        # break the list of input files into chunks of 500\n",
    "        for i in range(0,len(infiles),500):\n",
    "            chunk_infiles = infiles[i:i+500]\n",
    "            chunk_outfile = os.path.join(OUTDIR, 'chunk{}_{}.tif'.format(i, outname))\n",
    "            proc = subprocess.run(['rio', 'merge', *chunk_infiles, chunk_outfile, '--co', 'compress=LZW',\n",
    "                              '--co', 'tiled=true', '--co', 'blockxsize=256', '--co', 'blockysize=256',\n",
    "                              '--co', 'bigtiff=YES'],\n",
    "                              stderr=subprocess.PIPE, stdout=subprocess.PIPE)\n",
    "        \n",
    "        chunked_outfiles = glob.glob(os.path.join(OUTDIR, 'chunk*{}*.tif'.format(outname)))\n",
    "    \n",
    "        proc = subprocess.run(['rio', 'merge', *chunked_outfiles, outfile, '--co', 'compress=LZW',\n",
    "                              '--co', 'tiled=true', '--co', 'blockxsize=256', '--co', 'blockysize=256',\n",
    "                              '--co', 'bigtiff=YES'],\n",
    "                              stderr=subprocess.PIPE, stdout=subprocess.PIPE)\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Merge the bare earth tiles into a single GeoTiff."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dask.delayed\n",
    "def merge_dem(*args, **kwargs):\n",
    "    infiles = glob.glob(os.path.join(PROCESSED, 'rasters', 'DEM_tiles', '*.tif'))\n",
    "    OUTFILE = os.path.join(PROCESSED, 'rasters', 'dem.tif')\n",
    "    \n",
    "    if os.path.exists(OUTFILE):\n",
    "        pass\n",
    "    \n",
    "    elif len(infiles) < 500:\n",
    "            return subprocess.run(['rio', 'merge', *infiles, OUTFILE, '--co', 'compress=LZW',\n",
    "                                  '--co', 'tiled=true', '--co', 'blockxsize=256', '--co', 'blockysize=256',\n",
    "                                  '--co', 'bigtiff=YES'],\n",
    "                                  stderr=subprocess.PIPE, stdout=subprocess.PIPE)\n",
    "    else:\n",
    "        merge_chunks(infiles, OUTFILE)\n",
    "    \n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now merge the hillshade tiles into a single raster formatted as GeoTiff."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dask.delayed\n",
    "def merge_hillshade(*args, **kwargs):\n",
    "    infiles = glob.glob(os.path.join(PROCESSED, 'rasters', 'hillshade_tiles', '*.tif'))\n",
    "    OUTFILE = os.path.join(PROCESSED, 'rasters', 'hillshade.tif')\n",
    "\n",
    "    if os.path.exists(OUTFILE):\n",
    "        pass\n",
    "    \n",
    "    elif len(infiles) < 500:\n",
    "            return subprocess.run(['rio', 'merge', *infiles, OUTFILE, '--co', 'compress=LZW',\n",
    "                              '--co', 'tiled=true', '--co', 'blockxsize=256', '--co', 'blockysize=256',\n",
    "                              '--co', 'bigtiff=YES'],\n",
    "                              stderr=subprocess.PIPE, stdout=subprocess.PIPE)\n",
    "    else:\n",
    "        merge_chunks(infiles, OUTFILE)\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Merge the trimmed canopy height model tiles into a single raster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dask.delayed\n",
    "def merge_chm(*args, **kwargs):\n",
    "    infiles = glob.glob(os.path.join(PROCESSED, 'rasters', 'chm_tiles', '*.tif'))\n",
    "    OUTFILE = os.path.join(PROCESSED, 'rasters', 'chm.tif')\n",
    "    \n",
    "    if os.path.exists(OUTFILE):\n",
    "        pass\n",
    "    \n",
    "    elif len(infiles) < 500:\n",
    "            proc = subprocess.run(['rio', 'merge', *infiles, OUTFILE, '--co', 'compress=LZW',\n",
    "                                  '--co', 'tiled=true', '--co', 'blockxsize=256', '--co', 'blockysize=256',\n",
    "                                  '--co', 'bigtiff=YES'],\n",
    "                                  stderr=subprocess.PIPE, stdout=subprocess.PIPE)\n",
    "    else:\n",
    "        merge_chunks(infiles, OUTFILE)\n",
    "    \n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Merge the cleaned tiles of building footprints together into a single shapefile. We'll use `geopandas` to concatenate all the polygons together into a single geodataframe and then write out to a new shapefile."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dask.delayed\n",
    "def merge_bldgs(*args, **kwargs):\n",
    "    \n",
    "    if os.path.exists(os.path.join(PROCESSED, 'vectors', 'buildings.shp')):\n",
    "        pass\n",
    "    else:\n",
    "        building_tiles = glob.glob(os.path.join(PROCESSED, 'vectors', 'building_tiles', '*.shp'))\n",
    "        # create a list of geodataframes containing the tiles of building footprints\n",
    "        gdflist = [gpd.read_file(tile) for tile in building_tiles]\n",
    "        # merge them all together\n",
    "        merged = gpd.GeoDataFrame(pd.concat(gdflist, ignore_index=True))\n",
    "        # using pandas' concat caused us to lose projection information, so let's add that back in\n",
    "        merged.crs = gdflist[0].crs\n",
    "        # and write the merged data to a new shapefile\n",
    "        merged.to_file(os.path.join(PROCESSED, 'vectors', 'buildings.shp'))\n",
    "\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_grid_tiles_paths = glob.glob(os.path.join(INTERIM, 'gridmetrics', 'rasters', '*_strat0_intensity-median.tif'))\n",
    "all_grid_tiles = [fname(tile).split('_strat0_intensity-median')[0] for \n",
    "                  tile in all_grid_tiles_paths]\n",
    "example_tile = os.path.basename(all_grid_tiles_paths[0]).split('_strat0_intensity-median.tif')[0]\n",
    "grid_rasters = [os.path.basename(file).split(example_tile)[-1][1:-4] for file in \n",
    "                glob.glob(os.path.join(INTERIM, 'gridmetrics', 'rasters', example_tile + '*.tif'))\n",
    "               ]\n",
    "                \n",
    "print('{:d} different types of rasters from gridmetrics to process for each tile:\\r\\n'.format(len(grid_rasters)))\n",
    "for i, raster in enumerate(grid_rasters):\n",
    "    print('{}.  {}'.format(i+1, raster))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_gridsurf_tiles_paths = glob.glob(os.path.join(INTERIM, 'gridsurface', '*_potential_volume.tif'))\n",
    "all_gridsurf_tiles = [fname(tile).split('_strat0_intensity-median')[0] for \n",
    "                  tile in all_gridsurf_tiles_paths]\n",
    "example_tile = os.path.basename(all_gridsurf_tiles_paths[0]).split('_potential_volume.tif')[0]\n",
    "gridsurf_rasters = [os.path.basename(file).split(example_tile)[-1][1:-4] for file in \n",
    "                glob.glob(os.path.join(INTERIM, 'gridsurface', example_tile + '*.tif'))\n",
    "               ]\n",
    "                \n",
    "print('{:d} different types of rasters from gridsurface to process for each tile:\\r\\n'.format(len(gridsurf_rasters)))\n",
    "for i, raster in enumerate(gridsurf_rasters):\n",
    "    print('{}.  {}'.format(i+1, raster))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @dask.delayed\n",
    "def merge_gridmetric(metric):\n",
    "    infiles = glob.glob(os.path.join(INTERIM, 'gridmetrics', 'rasters', '*{}*.tif'.format(metric)))\n",
    "    OUTFILE = os.path.join(PROCESSED, 'rasters', '{}.tif'.format(metric))\n",
    "    \n",
    "    if os.path.exists(OUTFILE):\n",
    "        pass\n",
    "    \n",
    "    elif len(infiles) < 500:\n",
    "        proc = subprocess.run(['rio', 'merge', *infiles, OUTFILE, '--co', 'compress=LZW',\n",
    "                              '--co', 'tiled=true', #'--co', 'blockxsize=256', '--co', 'blockysize=256',\n",
    "                              '--co', 'bigtiff=YES'],\n",
    "                              stderr=subprocess.PIPE, stdout=subprocess.PIPE)\n",
    "    else:\n",
    "        merge_chunks(infiles, OUTFILE)\n",
    "    \n",
    "#     print(metric)\n",
    "    return metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @dask.delayed\n",
    "def merge_gridsurface(metric):\n",
    "    infiles = glob.glob(os.path.join(INTERIM, 'rasters', 'gridsurface', '*{}*.tif'.format(metric)))\n",
    "    OUTFILE = os.path.join(PROCESSED, 'rasters', '{}.tif'.format(metric))\n",
    "    \n",
    "    if os.path.exists(OUTFILE):\n",
    "        pass\n",
    "    \n",
    "    elif len(infiles) < 500:\n",
    "        proc = subprocess.run(['rio', 'merge', *infiles, OUTFILE, '--co', 'compress=LZW',\n",
    "                              '--co', 'tiled=true', #'--co', 'blockxsize=256', '--co', 'blockysize=256',\n",
    "                              '--co', 'bigtiff=YES'],\n",
    "                              stderr=subprocess.PIPE, stdout=subprocess.PIPE)\n",
    "    else:\n",
    "        merge_chunks(infiles, OUTFILE)\n",
    "    \n",
    "#     print(metric)\n",
    "    return metric"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A single state that will depend upon the completion of the merged rasters and vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dask.delayed\n",
    "def merge_done(*args, **kwargs):\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# building the computation receipe\n",
    "merge_dsk = {}\n",
    "merge_dsk['tile_boundaries'] = (tile_boundaries,)\n",
    "merge_dsk['footprint'] = (make_footprint, 'tile_boundaries')\n",
    "merge_dsk['merge_bldgs'] = (merge_bldgs,)\n",
    "merge_dsk['merge_hill'] = (merge_hillshade,)\n",
    "merge_dsk['merge_dem'] = (merge_dem,)\n",
    "merge_dsk['merge_chm'] = (merge_chm,)\n",
    "# for raster in grid_rasters:\n",
    "#     merge_dsk['merge_gridmetric-{}'.format(raster)] = (merge_gridmetric, raster)\n",
    "    \n",
    "merge_dsk['merge_done']=(merge_done, ['tile_boundaries', 'merge_bldgs', 'footprint']) #+\n",
    "#                                      ['merge_hill', 'merge_dem', 'merge_chm'] + \n",
    "#                                      ['merge_gridmetric-{}'.format(raster) for raster in grid_rasters])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merge_graph = c.get(merge_dsk, 'merge_done') # build the computation graph\n",
    "merge_graph.visualize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merge_results = c.compute(merge_graph) # this might take a while..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "progress(merge_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge_results.result()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# c.cancel(merge_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for raster in grid_rasters:\n",
    "    merge_gridmetric(raster)\n",
    "    print(raster)\n",
    "print('Done.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for raster in gridsurf_rasters:\n",
    "    merge_gridsurface(raster)\n",
    "    print(raster)\n",
    "print('Done.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# c.close()\n",
    "# cluster.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pyFIRS]",
   "language": "python",
   "name": "conda-env-pyFIRS-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
