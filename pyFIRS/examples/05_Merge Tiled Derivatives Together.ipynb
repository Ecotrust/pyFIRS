{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import glob\n",
    "import subprocess\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import dask\n",
    "from dask.distributed import Client, progress, LocalCluster\n",
    "from pyFIRS.wrappers import lastools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Launch a parallel computing cluster. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster=LocalCluster(scheduler_port=7001, diagnostics_port=7002)\n",
    "c = Client(cluster)\n",
    "num_cores = len(c.ncores()) # identify how many workers we have"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point, you should also be able to view an interactive dashboard on port 7002. If you're executing this on a remote server, you'll need to set up port forward so you can view the dashboard on your local machine's browser. Once you've done that, or if you're processing on your own machine, you can view the dashboard at [http://localhost:7002/status](http://localhost:7002/status)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "las = lastools.useLAStools('/storage/lidar/LAStools/bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# where the imported lidar data is currently stored\n",
    "workdir = os.path.abspath('/storage/lidar/odf_northwest_2015/wilkerson/')\n",
    "# define data handling directories\n",
    "processed = os.path.join(workdir,'processed')\n",
    "\n",
    "# the coordinate reference system we'll be working with\n",
    "target_epsg = 26910 # utm 10 N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# push our working directories and wrapper classes to the workers on the cluster as well\n",
    "c.scatter([processed, las, target_epsg, num_cores], broadcast=True);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Merge tiled derivative outputs together\n",
    "Merge all the tiled GeoTiffs and Shapefiles into single overview files."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll produce a shapefile showing the layout of the non-buffered tiles as a single shapefile. This is a single process that takes a few seconds to run, so no need to distribute it using `dask`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dask.delayed\n",
    "def tiles_overview(*args, **kwargs):\n",
    "    odir = os.path.join(processed, 'vectors')\n",
    "    \n",
    "    if os.path.exists(os.path.join(processed, 'vectors', 'tiles.shp')):\n",
    "        pass\n",
    "    else:\n",
    "        proc = las.lasboundary(i=os.path.join(processed, 'points', '*.laz'),\n",
    "                               use_bb=True, # use bounding box of tiles\n",
    "                               overview=True,\n",
    "                               labels=True,\n",
    "                               cores=num_cores, # use parallel processing\n",
    "                               oshp=True,\n",
    "                               o=os.path.join(processed, 'vectors', 'tiles.shp'))\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Merge the bare earth tiles into a single GeoTiff."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dask.delayed\n",
    "def merge_dem(*args, **kwargs):\n",
    "    infiles = os.path.join(processed, 'rasters', 'DEM_tiles', '*.tif')\n",
    "    outfile = os.path.join(processed, 'rasters', 'dem.tif')\n",
    "    \n",
    "    if os.path.exists(outfile):\n",
    "        return\n",
    "    else:\n",
    "        return subprocess.run(['rio', 'merge', *glob.glob(infiles), outfile, '--co', 'compress=LZW',\n",
    "                              '--co', 'tiled=true', '--co', 'blockxsize=256', '--co', 'blockysize=256'],\n",
    "                              stderr=subprocess.PIPE, stdout=subprocess.PIPE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now merge the hillshade tiles into a single raster formatted as GeoTiff."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dask.delayed\n",
    "def merge_hillshade(*args, **kwargs):\n",
    "    infiles = os.path.join(processed, 'rasters', 'hillshade_tiles', '*.tif')\n",
    "    outfile = os.path.join(processed, 'rasters', 'hillshade.tif')\n",
    "\n",
    "    if os.path.exists(outfile):\n",
    "        return\n",
    "    else:\n",
    "        return subprocess.run(['rio', 'merge', *glob.glob(infiles), outfile, '--co', 'compress=LZW',\n",
    "                              '--co', 'tiled=true', '--co', 'blockxsize=256', '--co', 'blockysize=256'],\n",
    "                              stderr=subprocess.PIPE, stdout=subprocess.PIPE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Merge the trimmed canopy height model tiles into a single raster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dask.delayed\n",
    "def merge_chm(*args, **kwargs):\n",
    "    infiles = os.path.join(processed, 'rasters', 'chm_tiles', '*.tif')\n",
    "    outfile = os.path.join(processed, 'rasters', 'chm.tif')\n",
    "    \n",
    "    if os.path.exists(outfile):\n",
    "        pass\n",
    "    else:\n",
    "        proc = subprocess.run(['rio', 'merge', *glob.glob(infiles), outfile, '--co', 'compress=LZW',\n",
    "                              '--co', 'tiled=true', '--co', 'blockxsize=256', '--co', 'blockysize=256'],\n",
    "                              stderr=subprocess.PIPE, stdout=subprocess.PIPE)\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Merge the cleaned tiles of building footprints together into a single shapefile. We'll use `geopandas` to concatenate all the polygons together into a single geodataframe and then write out to a new shapefile."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dask.delayed\n",
    "def merge_bldgs(*args, **kwargs):\n",
    "    \n",
    "    if os.path.exists(os.path.join(processed,'vectors','buildings.shp')):\n",
    "        pass\n",
    "    else:\n",
    "        building_tiles = glob.glob(os.path.join(processed, 'vectors', 'building_tiles', '*.shp'))\n",
    "        # create a list of geodataframes containing the tiles of building footprints\n",
    "        gdflist = [gpd.read_file(tile) for tile in building_tiles]\n",
    "        # merge them all together\n",
    "        merged = gpd.GeoDataFrame(pd.concat(gdflist, ignore_index=True))\n",
    "        # using pandas' concat caused us to lose projection information, so let's add that back in\n",
    "        merged.crs = gdflist[0].crs\n",
    "        # and write the merged data to a new shapefile\n",
    "        merged.to_file(os.path.join(processed,'vectors','buildings.shp'))\n",
    "\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A single state that will depend upon the completion of the merged rasters and vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dask.delayed\n",
    "def merge_done(*args, **kwargs):\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# building the computation receipe\n",
    "merge_dsk = {}\n",
    "merge_dsk['tiles_over'] = (tiles_overview, ['tiles_done'])\n",
    "merge_dsk['merge_bldgs'] = (merge_bldgs, ['tiles_done'])\n",
    "merge_dsk['merge_hill'] = (merge_hillshade, ['tiles_done'])\n",
    "merge_dsk['merge_dem'] = (merge_dem, ['tiles_done'])\n",
    "merge_dsk['merge_chm'] = (merge_chm, ['tiles_done'])\n",
    "merge_dsk['merge_done']=(merge_done, ['tiles_over', 'merge_bldgs', 'merge_hill', 'merge_dem', 'merge_chm'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merge_graph = c.get(merge_dsk, 'merge_done') # build the computation graph\n",
    "merge_results = c.persist(merge_graph) # this might take a while...\n",
    "progress(merge_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# c.cancel(merge_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c.close()\n",
    "cluster.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pyFIRS]",
   "language": "python",
   "name": "conda-env-pyFIRS-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
