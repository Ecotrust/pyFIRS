{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import subprocess\n",
    "\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import dask\n",
    "from dask.distributed import Client, progress, LocalCluster\n",
    "\n",
    "from pyFIRS.wrappers import lastools\n",
    "from pyFIRS.utils import fname"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# where the imported lidar data is currently stored\n",
    "WORKDIR = os.path.abspath('/storage/lidar/siskiyou_2017/')\n",
    "\n",
    "# the coordinate reference system we'll be working with\n",
    "TARGET_EPSG = 6339  # utm 10N, NAD83_2011\n",
    "# TARGET_EPSG = 6340  # utm 11N, NAD83_2011"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Launch a parallel computing cluster. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster = LocalCluster(scheduler_port=7001, dashboard_address=7002)\n",
    "c = Client(cluster)\n",
    "num_cores = len(c.ncores())  # identify how many workers we have"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point, you should also be able to view an interactive dashboard on port 7002. If you're executing this on a remote server, you'll need to set up port forward so you can view the dashboard on your local machine's browser. Once you've done that, or if you're processing on your own machine, you can view the dashboard at [http://localhost:7002/status](http://localhost:7002/status)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "las = lastools.useLAStools('/storage/lidar/LAStools/bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# define data handling directories\n",
    "INTERIM = os.path.join(WORKDIR, 'interim')\n",
    "PROCESSED = os.path.join(WORKDIR,'processed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# push stuff to the workers on the cluster\n",
    "c.scatter([INTERIM, PROCESSED, las, TARGET_EPSG, num_cores], broadcast=True);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tiles_to_merge = [fname(tile) for tile in\n",
    "                  glob.glob(os.path.join(PROCESSED, 'points', '*.laz'))]\n",
    "\n",
    "print('Found {:,d} tiles to merge derivative products from.'.format(\n",
    "    len(tiles_to_merge)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Merge tiled derivative outputs together\n",
    "Merge all the tiled GeoTiffs and Shapefiles into single overview files."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll produce a shapefile showing the layout of the non-buffered tiles as a single shapefile. This is a single process that takes a few seconds to run, so no need to distribute it using `dask`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dask.delayed\n",
    "def tile_boundaries(*args, **kwargs):\n",
    "    infiles_path = os.path.join(PROCESSED, 'points', '*.laz')\n",
    "    OUTFILE = os.path.join(PROCESSED, 'vectors', 'tiles.shp')\n",
    "\n",
    "    if os.path.exists(OUTFILE):\n",
    "        pass\n",
    "    else:\n",
    "        proc = las.lasboundary(i=infiles_path,\n",
    "                               use_bb=True,  # use bounding box of tiles\n",
    "                               overview=True,\n",
    "                               labels=True,\n",
    "                               cores=num_cores,  # use parallel processing\n",
    "                               oshp=True,\n",
    "                               o=OUTFILE)\n",
    "    return 'tile_boundaries'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dask.delayed\n",
    "def make_footprint(*args, **kwargs):\n",
    "    if os.path.exists(os.path.join(PROCESSED, 'vectors', 'footprint.shp')):\n",
    "        pass\n",
    "    else:\n",
    "        gdf = gpd.read_file(os.path.join(PROCESSED, 'vectors', 'tiles.shp'))\n",
    "        gdf['mil_points'] = gdf['num_points'] / 1000000.\n",
    "        buffered = gdf.drop(['file_name',\n",
    "                             'point_size',\n",
    "                             'point_type',\n",
    "                             'num_points'],\n",
    "                            axis=1)\n",
    "\n",
    "        buffered.geometry = gdf.buffer(0.01)  # buffer by 1cm\n",
    "        union = gpd.GeoDataFrame(geometry=[buffered.unary_union],\n",
    "                                 crs=buffered.crs)\n",
    "        union['footprint_id'] = union.index + 1\n",
    "\n",
    "        buffered = gpd.tools.sjoin(buffered,\n",
    "                                   union,\n",
    "                                   how='left').drop('index_right', axis=1)\n",
    "\n",
    "        aggfuncs = {'mil_points': 'sum',\n",
    "                    'version': 'first',\n",
    "                    'min_x': 'min',\n",
    "                    'min_y': 'min',\n",
    "                    'min_z': 'min',\n",
    "                    'max_x': 'max',\n",
    "                    'max_y': 'max',\n",
    "                    'max_z': 'max'}\n",
    "\n",
    "        dissolved = buffered.dissolve(by='footprint_id', aggfunc=aggfuncs)\n",
    "        OUTFILE = os.path.join(PROCESSED, 'vectors', 'footprint.shp')\n",
    "        dissolved.to_file(OUTFILE)\n",
    "\n",
    "        return 'footprint'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Merge the bare earth tiles into a single GeoTiff."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @dask.delayed\n",
    "def make_vrt(infiles, vrtfile):\n",
    "    \"\"\"Mosaics files into a single GeoTiff\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    infiles : list\n",
    "      list of paths to input files to mosaic\n",
    "    vrtfile : str, path to file\n",
    "      path to VRT file that will be created\n",
    "    \n",
    "    Returns\n",
    "    --------\n",
    "    proc : CompletedProcess\n",
    "      the result of executing gdalbuildvrt using subprocess\n",
    "    \"\"\"\n",
    "    proc = subprocess.run(['gdalbuildvrt',\n",
    "                           vrtfile,\n",
    "                           *infiles],\n",
    "                          stderr=subprocess.PIPE,\n",
    "                          stdout=subprocess.PIPE)\n",
    "    return proc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dask.delayed\n",
    "def merge_dem(*args, **kwargs):\n",
    "    infiles = glob.glob(\n",
    "        os.path.join(PROCESSED, 'rasters', 'dem_tiles', '*.tif'))\n",
    "    VRTFILE = os.path.join(PROCESSED, 'rasters', 'dem.vrt')\n",
    "\n",
    "    if os.path.exists(VRTFILE):\n",
    "        pass\n",
    "\n",
    "    else:\n",
    "        make_vrt(infiles, VRTFILE)\n",
    "\n",
    "    return 'dem'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dask.delayed\n",
    "def merge_intensity(*args, **kwargs):\n",
    "    infiles = glob.glob(\n",
    "        os.path.join(PROCESSED, 'rasters', 'intensity_tiles', '*.tif'))\n",
    "    VRTFILE = os.path.join(PROCESSED, 'rasters', 'intensity.vrt')\n",
    "    \n",
    "    if os.path.exists(VRTFILE):\n",
    "        pass\n",
    "\n",
    "    else:\n",
    "        make_vrt(infiles, VRTFILE)\n",
    "\n",
    "    return 'intensity'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now merge the hillshade tiles into a single raster formatted as GeoTiff."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dask.delayed\n",
    "def merge_hillshade(*args, **kwargs):\n",
    "    infiles = glob.glob(\n",
    "        os.path.join(PROCESSED, 'rasters', 'hillshade_tiles', '*.tif'))\n",
    "    VRTFILE = os.path.join(PROCESSED, 'rasters', 'hillshade.vrt')\n",
    "\n",
    "    if os.path.exists(VRTFILE):\n",
    "        pass\n",
    "\n",
    "    else:\n",
    "        make_vrt(infiles, VRTFILE)\n",
    "\n",
    "    return 'hillshade'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Merge the trimmed canopy height model tiles into a single raster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dask.delayed\n",
    "def merge_chm(*args, **kwargs):\n",
    "    infiles = glob.glob(\n",
    "        os.path.join(PROCESSED, 'rasters', 'chm_tiles', '*.tif'))\n",
    "    VRTFILE = os.path.join(PROCESSED, 'rasters', 'chm.vrt')\n",
    "\n",
    "    if os.path.exists(VRTFILE):\n",
    "        pass\n",
    "\n",
    "    else:\n",
    "        make_vrt(infiles, VRTFILE)\n",
    "\n",
    "    return 'chm'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Merge the cleaned tiles of building footprints together into a single shapefile. We'll use `geopandas` to concatenate all the polygons together into a single geodataframe and then write out to a new shapefile."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dask.delayed\n",
    "def merge_bldgs(*args, **kwargs):\n",
    "    infiles = glob.glob(\n",
    "        os.path.join(PROCESSED, 'vectors', 'building_tiles', '*.shp'))\n",
    "    OUTFILE = os.path.join(PROCESSED, 'vectors', 'buildings.shp')\n",
    "\n",
    "    if os.path.exists(OUTFILE):\n",
    "        pass\n",
    "    else:\n",
    "        # list of geodataframes with tiles of building footprints\n",
    "        gdflist = [gpd.read_file(tile) for tile in infiles]\n",
    "        # merge them all together\n",
    "        merged = pd.concat(gdflist, ignore_index=True)\n",
    "        # add projection information back in\n",
    "        merged.crs = gdflist[0].crs\n",
    "        # and write the merged data to a new shapefile\n",
    "        merged.to_file(OUTFILE)\n",
    "\n",
    "    return 'bldgs'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dask.delayed\n",
    "def merge_highpoints(*args, **kwargs):\n",
    "    infiles = glob.glob(\n",
    "        os.path.join(INTERIM, 'chm_tiles', 'treesegs', '*HighPoints.shp'))\n",
    "    OUTFILE = os.path.join(PROCESSED, 'vectors', 'high_points.shp')\n",
    "\n",
    "    if os.path.exists(OUTFILE):\n",
    "        pass\n",
    "    else:\n",
    "        # list of geodataframes with tiles of building footprints\n",
    "        gdflist = [gpd.read_file(tile) for tile in infiles]\n",
    "        # merge them all together\n",
    "        merged = pd.concat(gdflist, ignore_index=True)\n",
    "        # add projection information back in\n",
    "        merged.crs = gdflist[0].crs\n",
    "        # and write the merged data to a new shapefile\n",
    "        merged.to_file(OUTFILE)\n",
    "\n",
    "    return 'highpoints'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dask.delayed\n",
    "def merge_crowns(*args, **kwargs):\n",
    "    infiles = glob.glob(\n",
    "        os.path.join(INTERIM, 'chm_tiles', 'treesegs', '*Polygons.shp'))\n",
    "    OUTFILE = os.path.join(PROCESSED, 'vectors', 'tree_crowns.shp')\n",
    "\n",
    "    if os.path.exists(OUTFILE):\n",
    "        pass\n",
    "    else:\n",
    "        # list of geodataframes with tiles of building footprints\n",
    "        gdflist = [gpd.read_file(tile) for tile in infiles]\n",
    "        # merge them all together\n",
    "        merged = gpd.GeoDataFrame(pd.concat(gdflist, ignore_index=True))\n",
    "        # add projection information back in\n",
    "        merged.crs = gdflist[0].crs\n",
    "        # and write the merged data to a new shapefile\n",
    "        merged.to_file(OUTFILE)\n",
    "\n",
    "    return 'crowns'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_grid_tiles_paths = glob.glob(\n",
    "    os.path.join(PROCESSED, 'rasters',\n",
    "                 'gridmetrics_tiles', '*_strat0_intensity-median.tif'))\n",
    "\n",
    "all_grid_tiles = [fname(tile).split('_strat0_intensity-median')[0] for\n",
    "                  tile in all_grid_tiles_paths]\n",
    "\n",
    "example_tile = os.path.basename(\n",
    "    all_grid_tiles_paths[0]).split('_strat0_intensity-median.tif')[0]\n",
    "\n",
    "grid_rasters = [os.path.basename(file).split(example_tile)[-1][1:-4] for\n",
    "                file in glob.glob(\n",
    "                    os.path.join(PROCESSED, 'rasters',\n",
    "                                 'gridmetrics_tiles', example_tile + '*.tif'))]\n",
    "\n",
    "print('{:d} different types of rasters from gridmetrics '\n",
    "      'to process for each tile:\\r\\n'.format(len(grid_rasters)))\n",
    "\n",
    "for i, raster in enumerate(grid_rasters):\n",
    "    print('{}.  {}'.format(i+1, raster))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_gridsurf_tiles_paths = glob.glob(\n",
    "    os.path.join(PROCESSED, 'rasters',\n",
    "                 'gridsurface_tiles', '*_potential_volume.tif'))\n",
    "\n",
    "all_gridsurf_tiles = [fname(tile).split('_strat0_intensity-median')[0] for\n",
    "                      tile in all_gridsurf_tiles_paths]\n",
    "\n",
    "example_tile = os.path.basename(\n",
    "    all_gridsurf_tiles_paths[0]).split('_potential_volume.tif')[0]\n",
    "\n",
    "gridsurf_rasters = [os.path.basename(file).split(example_tile)[-1][1:-4] for\n",
    "                    file in glob.glob(\n",
    "                        os.path.join(PROCESSED, 'rasters',\n",
    "                                     'gridsurface_tiles',\n",
    "                                     example_tile + '*.tif'))]\n",
    "\n",
    "# we don't want these redundant rasters\n",
    "gridsurf_rasters = [x for x in gridsurf_rasters if x not\n",
    "                    in ['mean_height', 'max_height']]\n",
    "\n",
    "print('{:d} different types of rasters from gridsurface '\n",
    "      'to process for each tile:\\r\\n'.format(len(gridsurf_rasters)))\n",
    "for i, raster in enumerate(gridsurf_rasters):\n",
    "    print('{}.  {}'.format(i+1, raster))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dask.delayed\n",
    "def merge_gridmetric(metric):\n",
    "    infiles = glob.glob(\n",
    "        os.path.join(PROCESSED, 'rasters',\n",
    "                     'gridmetrics_tiles', '*{}.tif'.format(metric)))\n",
    "    VRTFILE = os.path.join(PROCESSED, 'rasters', '{}.vrt'.format(metric))\n",
    "\n",
    "    if os.path.exists(VRTFILE):\n",
    "        pass\n",
    "\n",
    "    else:\n",
    "        make_vrt(infiles, VRTFILE)\n",
    "\n",
    "    return metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dask.delayed\n",
    "def merge_gridsurface(metric):\n",
    "    infiles = glob.glob(\n",
    "        os.path.join(PROCESSED, 'rasters',\n",
    "                     'gridsurface_tiles', '*{}.tif'.format(metric)))\n",
    "    VRTFILE = os.path.join(PROCESSED, 'rasters',\n",
    "                           'gridsurface_tiles', '{}.vrt'.format(metric))\n",
    "\n",
    "    if os.path.exists(VRTFILE):\n",
    "        pass\n",
    "\n",
    "    else:\n",
    "        make_vrt(infiles, VRTFILE)\n",
    "\n",
    "    return metric"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A single state that will depend upon the completion of the merged rasters and vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dask.delayed\n",
    "def merge_done(*args, **kwargs):\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# building the computation receipe\n",
    "merge_dsk = {}\n",
    "merge_dsk['tile_boundaries'] = (tile_boundaries,)\n",
    "merge_dsk['footprint'] = (make_footprint, 'tile_boundaries')\n",
    "merge_dsk['merge_bldgs'] = (merge_bldgs,)\n",
    "merge_dsk['merge_hill'] = (merge_hillshade,)\n",
    "merge_dsk['merge_dem'] = (merge_dem,)\n",
    "merge_dsk['merge_intensity'] = (merge_intensity,)\n",
    "merge_dsk['merge_chm'] = (merge_chm,)\n",
    "for raster in grid_rasters:\n",
    "    merge_dsk['merge_gridmetric-{}'.format(raster)] = (merge_gridmetric,\n",
    "                                                       raster)\n",
    "for raster in gridsurf_rasters:\n",
    "    merge_dsk['merge_gridsurface-{}'.format(raster)] = (merge_gridsurface,\n",
    "                                                        raster)\n",
    "\n",
    "merge_dsk['merge_done'] = (merge_done,\n",
    "                           ['tile_boundaries', 'footprint']+ #) # +\n",
    "                           ['merge_bldgs'] + #)\n",
    "                           ['merge_hill', 'merge_dem'] +\n",
    "                           ['merge_chm', 'merge_intensity'] +\n",
    "                           ['merge_gridmetric-{}'.format(raster) for\n",
    "                            raster in grid_rasters] +\n",
    "                           ['merge_gridsurface-{}'.format(raster) for\n",
    "                            raster in gridsurf_rasters])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merge_graph = c.get(merge_dsk, 'merge_done')  # build the computation graph\n",
    "merge_graph.visualize(rankdir='LR')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merge_results = c.persist(merge_graph) # this might take a while..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "progress(merge_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge_results.result()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# c.cancel(merge_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# c.close()\n",
    "# cluster.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pyFIRS]",
   "language": "python",
   "name": "conda-env-pyFIRS-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
