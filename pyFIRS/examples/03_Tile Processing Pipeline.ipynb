{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main Lidar Processing Pipeline\n",
    "This notebook takes a directory of tiles and runs it through a series of command line tools from `FUSION` and `LAStools` to generate useful vector and raster products. It is assumed that the tiles have buffers added to each tile to avoid edge effects (e.g., using notebook `02_Retile...` in this repo).\n",
    "\n",
    "This pipeline performs denoising and deduplication, classifies points into buildings and vegetation, and generates raster products including 1m DEM, 1m hillshade, 0.5m intensity, and 0.5m canopy height model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import glob\n",
    "import subprocess\n",
    "import time\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import dask\n",
    "from dask.distributed import Client, progress, LocalCluster\n",
    "from pyFIRS.wrappers import lastools, fusion\n",
    "from pyFIRS.utils import clean_buffer_polys, clip_tile_from_shp, convert_project, PipelineError, fname, processing_summary, inspect_failures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# where the imported lidar data is currently stored\n",
    "WORKDIR = os.path.abspath('/storage/lidar/swinomish/sitc_skagit_2012/')\n",
    "\n",
    "# the coordinate reference system we'll be working with\n",
    "TARGET_EPSG = 6339  # utm 10N, nad83_2011\n",
    "# TARGET_EPSG = 6340  # utm 11N, nad83_2011"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting up parallel computing using `dask.distributed`\n",
    "`LAStools` offers native multi-core processing as an optional argument (`cores`) supplied to its command-line tools. `FUSION` command line tools do not. To enable parallel processing of `FUSION` commands, we'll use `dask.distributed` to schedule the processing of tiles in asynchronous parallel batches. This approach also offers us the ability to track progress using a progress bar.\n",
    "\n",
    "You'll first need to launch a parallel computing cluster. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster = LocalCluster(scheduler_port=7001, diagnostics_port=7002)\n",
    "c = Client(cluster)\n",
    "num_cores = len(c.ncores())  # identify how many workers we have"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point, you should also be able to view an interactive dashboard on port 7002. If you're executing this on a remote server, you'll need to set up port forward so you can view the dashboard on your local machine's browser. Once you've done that, or if you're processing on your own machine, you can view the dashboard at [http://localhost:7002/status](http://localhost:7002/status)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's get to work with some lidar data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll define where we can find the binary executables for LAStools and FUSION command line tools."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "las = lastools.useLAStools('/storage/lidar/LAStools/bin')\n",
    "fus = fusion.useFUSION('/storage/lidar/FUSION/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define data handling directories\n",
    "INTERIM = os.path.join(WORKDIR, 'interim')\n",
    "PROCESSED = os.path.join(WORKDIR, 'processed')\n",
    "LAYERS = os.path.join(INTERIM, 'layers')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_error(tile_id, process, error_msg):\n",
    "    logfile = os.path.join(INTERIM, 'failed', tile_id + '.txt')\n",
    "    os.makedirs(os.path.dirname(logfile), exist_ok=True)\n",
    "\n",
    "    with open(logfile, '+w') as f:\n",
    "        f.write('{} | {}: {}'.format(tile_id, process, error_msg))\n",
    "\n",
    "    return\n",
    "\n",
    "\n",
    "def has_error(tile_id):\n",
    "    errors = glob.glob(os.path.join(INTERIM, 'failed', '*.txt'))\n",
    "    tiles_with_errors = [fname(error) for error in errors]\n",
    "    if tile_id in tiles_with_errors:\n",
    "        return True\n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_coords_from_tileid(tile_id):\n",
    "    \"\"\"Get the coordinates of the lower left corner of the tile, assuming the tile \n",
    "    has been named in the pattern {XMIN}_{YMIN}_{RASTERLENGTH}.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    tile_id : string\n",
    "        assumed tile_id follows the naming convention of {LLX}_{LLY}_{LENGTH} where \n",
    "        LLX = x-coordinate of lower-left corner of tile (in projected units)\n",
    "        LLY = y-coordinate of lower-left corner of tile (in projected units)\n",
    "        LENGTH = length of the raster (in projected units), assumed to be a square tile shape\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    llx, lly, length : int\n",
    "        x- and y- coordinates of lower-left corner and length of raster\n",
    "    \"\"\"\n",
    "    tile_parts = tile_id.split('_')\n",
    "    if len(tile_parts) == 2:\n",
    "        llx, lly = [int(coord) for coord in tile_parts]\n",
    "        length = 1000 # assumed tile width if not explicit in tile_id\n",
    "    elif len(tile_parts) == 3:\n",
    "        llx, lly, length = [int(coord) for coord in tile_parts]\n",
    "    \n",
    "    return llx, lly, length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# push our working directories and wrapper classes to the workers on the cluster as well\n",
    "c.scatter([INTERIM, PROCESSED, LAYERS, las, fus, TARGET_EPSG, \n",
    "           num_cores, has_error, log_error], \n",
    "          broadcast=True);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Classify points in the lidar point cloud"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the original tiles delivered by the vendor included overlapping edges, our retiling may result in duplicated points in the new tiles from overlapping edges of vendor-provided input tiles. In the next step, we will ensure that only one point with unique (X,Y,Z) coordinates are retained in the point cloud data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dask.delayed\n",
    "def remove_duplicate_points(tile_id):\n",
    "    infile = os.path.join(INTERIM, 'retiled', tile_id + '.laz')\n",
    "\n",
    "    if not os.path.exists(os.path.join(PROCESSED, 'points', tile_id + '.laz')):\n",
    "        try:\n",
    "            proc = las.lasduplicate(i=infile,\n",
    "                                    unique_xyz=True,\n",
    "                                    olay=True,\n",
    "                                    olaydir=LAYERS,\n",
    "                                    odir=LAYERS)\n",
    "        except PipelineError as e:\n",
    "            log_error(tile_id, 'remove_duplicate_points', e.message)\n",
    "    else:  # output file already exists\n",
    "        pass\n",
    "\n",
    "    return tile_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dask.delayed\n",
    "def height_above_ground(tile_id):\n",
    "    infile = os.path.join(INTERIM, 'retiled', tile_id + '.laz')\n",
    "\n",
    "    if not has_error(tile_id) and \\\n",
    "    not os.path.exists(os.path.join(PROCESSED, 'points', tile_id + '.laz')):\n",
    "        try:\n",
    "            proc = las.lasheight(i=infile,\n",
    "                                 ilaydir=LAYERS,\n",
    "                                 olay=True,\n",
    "                                 classify_below=(-1.0, 7),  # noise\n",
    "                                 olaydir=LAYERS)\n",
    "        except PipelineError as e:\n",
    "            log_error(tile_id, 'height_above_ground', e)\n",
    "\n",
    "    else:  # this tile already has an error recorded in the error_log\n",
    "        pass\n",
    "\n",
    "    return tile_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll remove points that are isolated as likely noise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dask.delayed\n",
    "def denoise(tile_id):\n",
    "    infile = os.path.join(INTERIM, 'retiled', tile_id + '.laz')\n",
    "\n",
    "    if not has_error(tile_id) and \\\n",
    "    not os.path.exists(os.path.join(PROCESSED, 'points', tile_id + '.laz')):\n",
    "        try:\n",
    "            proc = las.lasnoise(i=infile,\n",
    "                                ilaydir=LAYERS,\n",
    "                                olay=True,\n",
    "                                step=2,  # cell-size for 3x3x3 search area\n",
    "                                isolated=10,  # min. neighbors in search area\n",
    "                                olaydir=LAYERS)\n",
    "        except PipelineError as e:\n",
    "            log_error(tile_id, 'denoise', e)\n",
    "\n",
    "    else:  # this tile already has an error recorded in the error_log\n",
    "        pass\n",
    "\n",
    "    return tile_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we'll classify points as building or high vegetation that meet certain criteria for 'planarity' or 'ruggedness'. \n",
    "\n",
    "**THERE ARE ARGUMENTS IN THE FOLLOWING COMMAND THAT DEPEND UPON THE UNITS OF THE DATA.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dask.delayed\n",
    "def classify(tile_id):\n",
    "    infile = os.path.join(INTERIM, 'retiled', tile_id + '.laz')\n",
    "    ODIR = os.path.join(PROCESSED, 'points')\n",
    "\n",
    "    if not has_error(tile_id) and \\\n",
    "    not os.path.exists(os.path.join(PROCESSED, 'points', tile_id + '.laz')):\n",
    "        try:\n",
    "            proc = las.lasclassify(i=infile,\n",
    "                                   ilaydir=LAYERS,\n",
    "                                   step=2.0,\n",
    "                                   planar=0.1,  # if your data are in meters\n",
    "                                   rugged=0.4,  # if your data are in meters\n",
    "                                   olaz=True,\n",
    "                                   odir=ODIR)\n",
    "        except PipelineError as e:\n",
    "            log_error(tile_id, 'classify', e.message)\n",
    "\n",
    "    else:  # this tile already has an error recorded in the error_log\n",
    "        pass\n",
    "\n",
    "    return tile_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dask.delayed\n",
    "def fusion_intensity(tile_id):\n",
    "    infile = os.path.join(PROCESSED, 'points', tile_id + '.laz')\n",
    "    ODIR = os.path.join(INTERIM, 'intensity_tiles')\n",
    "    outfile = os.path.join(ODIR, tile_id + '.bmp')\n",
    "        \n",
    "    if not has_error(tile_id):\n",
    "        try:\n",
    "            proc_fus = fus.intensityimage(0.5,\n",
    "                                          outfile,\n",
    "                                          infile,\n",
    "                                          odir=ODIR,\n",
    "                                          rasterorigin=True,\n",
    "                                          intrange=(1,254)\n",
    "                                         )\n",
    "        except PipelineError as e:\n",
    "            log_error(tile_id, 'fusion_intensity', e.message)\n",
    "    \n",
    "    else:  # this tile already has an error recorded in the error_log\n",
    "        pass\n",
    "\n",
    "    return tile_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dask.delayed\n",
    "def reformat_intensity(tile_id):\n",
    "    infile = os.path.join(INTERIM, 'intensity_tiles', tile_id + '.bmp')\n",
    "    ODIR = os.path.join(PROCESSED, 'rasters', 'intensity_tiles_fusion')\n",
    "    outfile = os.path.join(ODIR, tile_id + '.tif')\n",
    "    os.makedirs(ODIR, exist_ok=True)\n",
    "\n",
    "    tile_llx, tile_lly, tile_len = parse_coords_from_tileid(tile_id)\n",
    "    ul_x, ul_y = tile_llx, tile_lly + tile_len\n",
    "    lr_x, lr_y = tile_llx + tile_len, tile_lly\n",
    "        \n",
    "    if not has_error(tile_id):\n",
    "        try:\n",
    "            proc_gdal = subprocess.run(['gdal_translate',\n",
    "                                        '-of', 'GTiff',\n",
    "                                        '-b', '1',\n",
    "                                        '-a_nodata', '255',\n",
    "                                        '-projwin', str(ul_x), str(ul_y), str(lr_x), str(lr_y),\n",
    "                                        '-projwin_srs', 'EPSG:{}'.format(TARGET_EPSG),\n",
    "                                        '-a_srs', 'EPSG:{}'.format(TARGET_EPSG),\n",
    "                                        '-colorinterp', 'gray',\n",
    "                                        infile,\n",
    "                                        outfile], \n",
    "                                       stderr=subprocess.PIPE,\n",
    "                                       stdout=subprocess.PIPE)\n",
    "        except PipelineError as e:\n",
    "                log_error(tile_id, 'reformat_intensity', e.message)\n",
    "    \n",
    "    else:  # this tile already has an error recorded in the error_log\n",
    "        pass\n",
    "\n",
    "    return tile_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2. Generate a bare earth Digital Elevation Model\n",
    "Generate tiles of the bare earth model. This assumes that there are already ground-classified points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dask.delayed\n",
    "def make_dem(tile_id):\n",
    "    infile = os.path.join(PROCESSED, 'points', tile_id + '.laz')\n",
    "    ODIR = os.path.join(INTERIM, 'dem_tiles')\n",
    "    outfile = os.path.join(ODIR, tile_id + '.tif')\n",
    "\n",
    "    if not os.path.exists(outfile):\n",
    "        if not has_error(tile_id):\n",
    "            try:\n",
    "                proc = las.las2dem(i=infile,\n",
    "                                   odir=ODIR,\n",
    "                                   otif=True,  # output GeoTIFF\n",
    "                                   keep_class=2,  # ground returns only\n",
    "                                   step=1,\n",
    "                                   thin_with_grid=1,  # resolution for TIN\n",
    "                                   extra_pass=True)\n",
    "            except PipelineError as e:\n",
    "                log_error(tile_id, 'make_dem', e.message)\n",
    "    else:  # output file already exists\n",
    "        pass\n",
    "\n",
    "    return tile_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dask.delayed\n",
    "def reformat_dem(tile_id):\n",
    "    infile = os.path.join(INTERIM, 'dem_tiles', tile_id + '.tif')\n",
    "    ODIR = os.path.join(PROCESSED, 'rasters', 'dem_tiles')\n",
    "    outfile = os.path.join(ODIR, tile_id + '.tif')\n",
    "    os.makedirs(ODIR, exist_ok=True)\n",
    "\n",
    "    tile_llx, tile_lly, tile_len = parse_coords_from_tileid(tile_id)\n",
    "    ul_x, ul_y = tile_llx, tile_lly + tile_len\n",
    "    lr_x, lr_y = tile_llx + tile_len, tile_lly\n",
    "        \n",
    "    if not has_error(tile_id):\n",
    "        try:\n",
    "            proc_gdal = subprocess.run(['gdal_translate',\n",
    "                                        '-of', 'GTiff',\n",
    "                                        '-projwin', str(ul_x), str(ul_y), str(lr_x), str(lr_y),\n",
    "                                        '-projwin_srs', 'EPSG:{}'.format(TARGET_EPSG),\n",
    "                                        '-a_srs', 'EPSG:{}'.format(TARGET_EPSG),\n",
    "                                        infile,\n",
    "                                        outfile], \n",
    "                                       stderr=subprocess.PIPE,\n",
    "                                       stdout=subprocess.PIPE)\n",
    "        except PipelineError as e:\n",
    "                log_error(tile_id, 'reformat_chm', e.message)\n",
    "    \n",
    "    else:  # this tile already has an error recorded in the error_log\n",
    "        pass\n",
    "\n",
    "    return tile_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To create a hillshade layer, we'll first, generate hillshade tiles from the bare earth model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dask.delayed\n",
    "def make_hillshade(tile_id):\n",
    "    infile = os.path.join(PROCESSED, 'points', tile_id + '.laz')\n",
    "    ODIR = os.path.join(INTERIM, 'hillshade_tiles')\n",
    "    outfile = os.path.join(ODIR, tile_id + '.tif')\n",
    "\n",
    "    if not os.path.exists(outfile):\n",
    "        if not has_error(tile_id):\n",
    "            try:\n",
    "                proc = las.las2dem(i=infile,\n",
    "                                   odir=ODIR,\n",
    "                                   otif=True,\n",
    "                                   hillshade=True,\n",
    "                                   keep_class=2,  # ground returns only\n",
    "                                   step=1,\n",
    "                                   thin_with_grid=1,  # resolution for TIN\n",
    "                                   extra_pass=True)\n",
    "\n",
    "            except PipelineError as e:\n",
    "                log_error(tile_id, 'make_hillshade', e.message)\n",
    "    else:  # output file already exists\n",
    "        pass\n",
    "\n",
    "    return tile_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dask.delayed\n",
    "def reformat_hillshade(tile_id):\n",
    "    infile = os.path.join(INTERIM, 'hillshade_tiles', tile_id + '.tif')\n",
    "    ODIR = os.path.join(PROCESSED, 'rasters', 'hillshade_tiles')\n",
    "    outfile = os.path.join(ODIR, tile_id + '.tif')\n",
    "    os.makedirs(ODIR, exist_ok=True)\n",
    "\n",
    "    tile_llx, tile_lly, tile_len = parse_coords_from_tileid(tile_id)\n",
    "    ul_x, ul_y = tile_llx, tile_lly + tile_len\n",
    "    lr_x, lr_y = tile_llx + tile_len, tile_lly\n",
    "        \n",
    "    if not has_error(tile_id):\n",
    "        try:\n",
    "            proc_gdal = subprocess.run(['gdal_translate',\n",
    "                                        '-of', 'GTiff',\n",
    "                                        '-projwin', str(ul_x), str(ul_y), str(lr_x), str(lr_y),\n",
    "                                        '-projwin_srs', 'EPSG:{}'.format(TARGET_EPSG),\n",
    "                                        '-a_srs', 'EPSG:{}'.format(TARGET_EPSG),\n",
    "                                        infile,\n",
    "                                        outfile], \n",
    "                                       stderr=subprocess.PIPE,\n",
    "                                       stdout=subprocess.PIPE)\n",
    "        except PipelineError as e:\n",
    "                log_error(tile_id, 'reformat_chm', e.message)\n",
    "    \n",
    "    else:  # this tile already has an error recorded in the error_log\n",
    "        pass\n",
    "\n",
    "    return tile_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Identify building footprints\n",
    "First start by building shapefiles showing building boundaries in each buffered tile."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dask.delayed\n",
    "def make_building_footprint_tiles(tile_id):\n",
    "    infile = os.path.join(PROCESSED, 'points', tile_id + '.laz')\n",
    "    ODIR = os.path.join(INTERIM, 'building_tiles')\n",
    "    outfile = os.path.join(ODIR, tile_id + '.shp')\n",
    "\n",
    "    if not os.path.exists(outfile):\n",
    "        if not has_error(tile_id):\n",
    "            try:\n",
    "                proc = las.lasboundary(i=infile,\n",
    "                                       keep_class=6,  # building points\n",
    "                                       disjoint=True,  # separate polygons\n",
    "                                       concavity=1,  # min edge length\n",
    "                                       oshp=True,\n",
    "                                       odir=ODIR)\n",
    "            except PipelineError as e:\n",
    "                log_error(tile_id, 'make_building_footprint_tiles', e.message)\n",
    "    else:  # output file already exists\n",
    "        pass\n",
    "\n",
    "    return tile_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate shapefiles showing the bounding box of each (unbuffered) tile that we'll use to remove buildings that fall in the buffered area."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dask.delayed\n",
    "def unbuffered_tile_boundary(tile_id):\n",
    "    infile = os.path.join(PROCESSED, 'points', tile_id + '.laz')\n",
    "    ODIR = os.path.join(INTERIM, 'tile_boundaries')\n",
    "    outfile = os.path.join(ODIR, tile_id + '.shp')\n",
    "\n",
    "    if not os.path.exists(outfile):\n",
    "        if not has_error(tile_id):\n",
    "            try:\n",
    "                proc = las.lasboundary(i=infile,\n",
    "                                       odir=ODIR,\n",
    "                                       oshp=True,\n",
    "                                       use_tile_bb=True)\n",
    "            except PipelineError as e:\n",
    "                log_error(tile_id, 'unbuffered_tile_boundary', e.message)\n",
    "    else:  # output file already exists\n",
    "        pass\n",
    "\n",
    "    return tile_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each shapefile containing polygons of the building boundaries, we'll use the `clean_buffer_polys` function from `pyFIRS.utils` to remove polygons from a tile if their centroid falls in the buffered area of the tile."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dask.delayed\n",
    "def remove_building_buffers(tile_id, *args):\n",
    "    if type(tile_id) == list:\n",
    "        tile_id = tile_id[0]\n",
    "\n",
    "    infile = os.path.join(INTERIM, 'building_tiles', tile_id + '.shp')\n",
    "    tile_shp = os.path.join(INTERIM, 'tile_boundaries', tile_id + '.shp')\n",
    "    ODIR = os.path.join(PROCESSED, 'vectors', 'building_tiles')\n",
    "    outfile = os.path.join(ODIR, tile_id + '.shp')\n",
    "\n",
    "    if not os.path.exists(outfile):\n",
    "        if not has_error(tile_id):\n",
    "            try:\n",
    "                clean_buffer_polys(infile,\n",
    "                                   tile_shp,\n",
    "                                   odir=ODIR,\n",
    "                                   simp_tol=3,\n",
    "                                   simp_topol=True)\n",
    "            except Exception as e:\n",
    "                log_error(tile_id, 'remove_building_buffers', e.message)\n",
    "    else:  # output file already exists\n",
    "        pass\n",
    "\n",
    "    return tile_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Create a Canopy Height Model\n",
    "We're going to switch use a FUSION command line tool to generate a Canopy Height Models (CHMs). \n",
    "\n",
    "### Using FUSION's `canopymodel` to generate CHMs\n",
    "`FUSION` wants to have ground models formatted as .dtm files, for CHM development and for estimating other canopy metrics. Let's generate these ground models first using a 1-meter x-y resolution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dask.delayed\n",
    "def make_ground_dtm(tile_id):\n",
    "    infile = os.path.join(PROCESSED, 'points', tile_id + '.laz')\n",
    "    ODIR = os.path.join(INTERIM, 'dtm_ground_tiles')\n",
    "    outfile = os.path.join(ODIR, tile_id + '.dtm')\n",
    "\n",
    "    if not os.path.exists(outfile):\n",
    "        if not has_error(tile_id):\n",
    "            try:\n",
    "                proc = fus.gridsurfacecreate(surfacefile=outfile,\n",
    "                                             cellsize=1,\n",
    "                                             xyunits='M',\n",
    "                                             zunits='M',\n",
    "                                             coordsys=1,  # UTM\n",
    "                                             zone='10N',\n",
    "                                             horizdatum=2,  # NAD83\n",
    "                                             vertdatum=2,  # NAVD88\n",
    "                                             datafile=infile,\n",
    "                                             las_class=2,  # ground points\n",
    "                                             odir=ODIR)\n",
    "            except PipelineError as e:\n",
    "                log_error(tile_id, 'make_ground_dtm', e.message)\n",
    "    else:  # output file already exists\n",
    "        pass\n",
    "\n",
    "    return tile_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dask.delayed\n",
    "def make_canopy_model(tile_id):\n",
    "    infile = os.path.join(PROCESSED, 'points', tile_id + '.laz')\n",
    "    ODIR = os.path.join(INTERIM, 'chm_tiles')\n",
    "    outfile = os.path.join(ODIR, tile_id + '.dtm')\n",
    "    groundfile = os.path.join(INTERIM, 'dtm_ground_tiles', tile_id + '.dtm')\n",
    "\n",
    "    if not os.path.exists(outfile):\n",
    "        if not has_error(tile_id):\n",
    "            try:\n",
    "                proc = fus.canopymodel(surfacefile=outfile,\n",
    "                                       cellsize=0.5,\n",
    "                                       xyunits='M',\n",
    "                                       zunits='M',\n",
    "                                       coordsys=1,  # UTM\n",
    "                                       zone='10N',  # UTM\n",
    "                                       horizdatum=2,   # NAD83\n",
    "                                       vertdatum=2,  # NAVD88\n",
    "                                       datafiles=infile,\n",
    "                                       ground=groundfile,\n",
    "                                       median=3,  # kernel size\n",
    "                                       las_class=(1, 2, 5),  # grd, unclas, veg\n",
    "                                       outlier=(-1, 110),\n",
    "                                       asc=True,  # also output in ascii format\n",
    "                                       multiplier=100, # convert to centimeters from meters\n",
    "                                       odir=ODIR)\n",
    "\n",
    "            except PipelineError as e:\n",
    "                log_error(tile_id, 'make_canopy_model', e.message)\n",
    "    else:  # output file already exists\n",
    "        pass\n",
    "\n",
    "    return tile_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert the ascii files that `canopymodel` generated into GeoTiffs and trim them to the unbuffered tile boundary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dask.delayed\n",
    "def reformat_chm(tile_id):\n",
    "    infile = os.path.join(INTERIM, 'chm_tiles', tile_id + '.asc')\n",
    "    ODIR = os.path.join(PROCESSED, 'rasters', 'chm_tiles')\n",
    "    outfile = os.path.join(ODIR, tile_id + '.tif')\n",
    "    os.makedirs(ODIR, exist_ok=True)\n",
    "\n",
    "    tile_llx, tile_lly, tile_len = parse_coords_from_tileid(tile_id)\n",
    "    ul_x, ul_y = tile_llx, tile_lly + tile_len\n",
    "    lr_x, lr_y = tile_llx + tile_len, tile_lly\n",
    "        \n",
    "    if not has_error(tile_id):\n",
    "        try:\n",
    "            proc_rio = subprocess.run(['rio', 'edit-info', \n",
    "                                       '--nodata', '65535', \n",
    "                                       infile],\n",
    "                                      stderr=subprocess.PIPE,\n",
    "                                      stdout=subprocess.PIPE)\n",
    "            proc_gdal = subprocess.run(['gdal_translate',\n",
    "                                        '-of', 'GTiff',\n",
    "                                        '-ot', 'UInt16',\n",
    "                                        '-projwin', str(ul_x), str(ul_y), str(lr_x), str(lr_y),\n",
    "                                        '-projwin_srs', 'EPSG:{}'.format(TARGET_EPSG),\n",
    "                                        '-a_srs', 'EPSG:{}'.format(TARGET_EPSG),\n",
    "                                        '-a_scale', '0.01',\n",
    "                                        infile,\n",
    "                                        outfile], \n",
    "                                       stderr=subprocess.PIPE,\n",
    "                                       stdout=subprocess.PIPE)\n",
    "        except PipelineError as e:\n",
    "                log_error(tile_id, 'reformat_chm', e.message)\n",
    "    \n",
    "    else:  # this tile already has an error recorded in the error_log\n",
    "        pass\n",
    "\n",
    "    return tile_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dask.delayed\n",
    "def tile_done(tile_id, *args, **kwargs):\n",
    "    if type(tile_id) == list:\n",
    "        tile_id = tile_id[0].split('unbuffered-')[-1]\n",
    "\n",
    "    if not has_error(tile_id):\n",
    "        outfile = os.path.join(INTERIM, 'finished', tile_id + '.txt')\n",
    "        os.makedirs(os.path.dirname(outfile), exist_ok=True)\n",
    "\n",
    "        with open(outfile, '+a') as f:\n",
    "            f.write('{}'.format(tile_id))\n",
    "\n",
    "    return tile_id\n",
    "\n",
    "\n",
    "@dask.delayed\n",
    "def tiles_done(*args, **kwargs):\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hand-build the computational graph\n",
    "Define the recipe for computations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tile_ids = set(fname(tile) for tile in\n",
    "               glob.glob(os.path.join(INTERIM, 'retiled', '*.laz')))\n",
    "\n",
    "failed_tiles = set(fname(tile) for tile in\n",
    "                   glob.glob(os.path.join(INTERIM, 'failed', '*.txt')))\n",
    "\n",
    "finished_tiles = set(fname(tile) for tile in\n",
    "                     glob.glob(os.path.join(INTERIM, 'finished', '*.txt')))\n",
    "\n",
    "tiles_to_process = list(tile_ids - failed_tiles - finished_tiles)\n",
    "\n",
    "print('Found {:,d} tiles to process'.format(len(tiles_to_process)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dsk = {}\n",
    "for tile in tiles_to_process:\n",
    "    # pre-processing of lidar point clouds\n",
    "    dsk['deduped-{}'.format(tile)] = (\n",
    "        remove_duplicate_points,\n",
    "        tile)\n",
    "    dsk['normalized-{}'.format(tile)] = (\n",
    "        height_above_ground,\n",
    "        'deduped-{}'.format(tile))\n",
    "    dsk['denoised-{}'.format(tile)] = (\n",
    "        denoise,\n",
    "        'normalized-{}'.format(tile))\n",
    "    dsk['redenoised-{}'.format(tile)] = (\n",
    "        denoise,\n",
    "        'denoised-{}'.format(tile))\n",
    "    dsk['classified-{}'.format(tile)] = (\n",
    "        classify,\n",
    "        'redenoised-{}'.format(tile))\n",
    "    dsk['bbox-{}'.format(tile)] = (\n",
    "        unbuffered_tile_boundary,\n",
    "        'classified-{}'.format(tile))\n",
    "\n",
    "    # make derivative vector and raster products\n",
    "    dsk['dem-{}'.format(tile)] = (\n",
    "        make_dem,\n",
    "        'classified-{}'.format(tile))\n",
    "    dsk['fusion_intensity-{}'.format(tile)] = (\n",
    "        fusion_intensity,\n",
    "        'classified-{}'.format(tile))\n",
    "    dsk['ground_dtm-{}'.format(tile)] = (\n",
    "        make_ground_dtm,\n",
    "        'classified-{}'.format(tile))\n",
    "    dsk['canopy-{}'.format(tile)] = (\n",
    "        make_canopy_model,\n",
    "        'ground_dtm-{}'.format(tile))\n",
    "    dsk['hillshade-{}'.format(tile)] = (\n",
    "        make_hillshade,\n",
    "        'classified-{}'.format(tile))\n",
    "    dsk['buildings_buffered-{}'.format(tile)] = (\n",
    "        make_building_footprint_tiles,\n",
    "        'classified-{}'.format(tile))\n",
    "\n",
    "    # post-processing of derivative products\n",
    "    dsk['buildings_unbuffered-{}'.format(tile)] = (\n",
    "        remove_building_buffers,\n",
    "        ['buildings_buffered-{}'.format(tile), 'bbox-{}'.format(tile)])\n",
    "    dsk['reformat_intensity-{}'.format(tile)] = (\n",
    "        reformat_intensity,\n",
    "        'fusion_intensity-{}'.format(tile))\n",
    "    dsk['reformat_chm-{}'.format(tile)] = (\n",
    "        reformat_chm,\n",
    "        'canopy-{}'.format(tile))\n",
    "    dsk['reformat_dem-{}'.format(tile)] = (\n",
    "        reformat_dem,\n",
    "        'dem-{}'.format(tile))\n",
    "    dsk['reformat_hillshade-{}'.format(tile)] = (\n",
    "        reformat_hillshade,\n",
    "        'hillshade-{}'.format(tile))\n",
    "#     dsk['dem_clipped-{}'.format(tile)] = (\n",
    "#         remove_dem_buffer,\n",
    "#         ['projected_dem-{}'.format(tile), 'bbox-{}'.format(tile)])\n",
    "#     dsk['hillshade_clipped-{}'.format(tile)] = (\n",
    "#         remove_hillshade_buffer,\n",
    "#         ['projected_hillshade-{}'.format(tile), 'bbox-{}'.format(tile)])\n",
    "\n",
    "    # a catch-all when a tile is finished processing\n",
    "    dsk['tile_done-{}'.format(tile)] = (\n",
    "        tile_done,\n",
    "        ['unbuffered-{}'.format(tile),\n",
    "         'reformat_dem-{}'.format(tile),\n",
    "         'reformat_hillshade-{}'.format(tile),\n",
    "         'reformat_intensity-{}'.format(tile),\n",
    "         'buildings_unbuffered-{}'.format(tile),\n",
    "         'reformat_chm-{}'.format(tile)])\n",
    "\n",
    "dsk['tiles_done'] = (\n",
    "    tiles_done,\n",
    "    ['tile_done-{}'.format(tile) for tile in tiles_to_process])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unfinished_tiles = [tile for tile in tile_ids if tile not in finished_tiles]\n",
    "INT_DIRS = ['building_tiles', 'chm_tiles', 'dem_tiles', 'dtm_ground_tiles',\n",
    "            'hillshade_tiles', 'intensity_tiles', 'layers', 'tile_boundaries']\n",
    "\n",
    "files_removed = 0\n",
    "for tile in unfinished_tiles:\n",
    "    for d in INT_DIRS:\n",
    "        files_to_remove = glob.glob(os.path.join(INTERIM, d, tile + '*'))\n",
    "        for f in files_to_remove:\n",
    "            os.remove(f)\n",
    "            files_removed += 1\n",
    "\n",
    "print(\"Removed {:,d} files.\".format(files_removed))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unfinished_tiles = [tile for tile in tile_ids if tile not in finished_tiles]\n",
    "PROC_DIRS = ['points', 'rasters/chm_tiles', 'rasters/dem_tiles',\n",
    "             'rasters/hillshade_tiles', 'rasters/intensity_tiles_fusion',\n",
    "             'vectors/buildings_tiles']\n",
    "\n",
    "files_removed = 0\n",
    "for tile in unfinished_tiles:\n",
    "    for d in PROC_DIRS:\n",
    "        files_to_remove = glob.glob(os.path.join(PROCESSED, d, tile + '*'))\n",
    "        for f in files_to_remove:\n",
    "            os.remove(f)\n",
    "            files_removed += 1\n",
    "\n",
    "print(\"Removed {:,d} files.\".format(files_removed))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The computational pipeline, including dependencies of each step in the pipeline, can be specified using a dictionary. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "example_tile_graph = c.get(dsk, 'tile_done-{}'.format(tiles_to_process[0]))\n",
    "example_tile_graph.visualize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get Dask to determine how to get to the last state of the tile-processing pipeline, building a computational graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "tiles_graph = c.get(dsk, 'tiles_done')  # this might take a few minutes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tiles_results = c.persist(tiles_graph)  # start computing\n",
    "start_time = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "progress(tiles_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tiles_results.result()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processing_summary(tile_ids,\n",
    "                   finished_tiles,\n",
    "                   tiles_to_process,\n",
    "                   os.path.join(INTERIM, 'finished'),\n",
    "                   os.path.join(INTERIM, 'failed'),\n",
    "                   start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inspect_failures(os.path.join(INTERIM, 'failed'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# c.cancel(tiles_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# c.close()\n",
    "# cluster.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pyFIRS] *",
   "language": "python",
   "name": "conda-env-pyFIRS-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
