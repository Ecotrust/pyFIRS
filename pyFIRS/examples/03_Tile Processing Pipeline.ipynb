{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import glob\n",
    "import subprocess\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import dask\n",
    "from dask.distributed import Client, progress, LocalCluster\n",
    "from pyFIRS.wrappers import lastools, fusion\n",
    "from pyFIRS.utils import clean_dir, clean_buffer_polys, clip_tile_from_shp, convert_project, PipelineError, fname"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting up parallel computing using `dask.distributed`\n",
    "`LAStools` offers native multi-core processing as an optional argument (`cores`) supplied to its command-line tools. `FUSION` command line tools do not. To enable parallel processing of `FUSION` commands, we'll use `dask.distributed` to schedule the processing of tiles in asynchronous parallel batches. This approach also offers us the ability to track progress using a progress bar.\n",
    "\n",
    "You'll first need to launch a parallel computing cluster. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster=LocalCluster(scheduler_port=7001, diagnostics_port=7002)\n",
    "c = Client(cluster)\n",
    "num_cores = len(c.ncores()) # identify how many workers we have"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point, you should also be able to view an interactive dashboard on port 7002. If you're executing this on a remote server, you'll need to set up port forward so you can view the dashboard on your local machine's browser. Once you've done that, or if you're processing on your own machine, you can view the dashboard at [http://localhost:7002/status](http://localhost:7002/status)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's get to work with some lidar data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll define where we can find the binary executables for LAStools and FUSION command line tools."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "las = lastools.useLAStools('/storage/lidar/LAStools/bin')\n",
    "fus = fusion.useFUSION('/storage/lidar/FUSION/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll create working directories for raw (imported with modest clean-up from source files), interim, and processed data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# where the imported lidar data is currently stored\n",
    "workdir = os.path.abspath('/storage/lidar/olc_metro_2014/')\n",
    "# define data handling directories\n",
    "interim = os.path.join(workdir,'interim')\n",
    "processed = os.path.join(workdir,'processed')\n",
    "layers = os.path.join(interim, 'layers')\n",
    "\n",
    "\n",
    "# the coordinate reference system we'll be working with\n",
    "target_epsg = 26910 # utm 10 N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_error(tile_id, process, error_msg):\n",
    "    logfile = os.path.join(interim, 'failed', tile_id + '.txt')\n",
    "    os.makedirs(os.path.dirname(logfile), exist_ok=True)\n",
    "    \n",
    "    with open(logfile, '+w') as f:\n",
    "        f.write('{} | {}: {}'.format(tile_id, process, error_msg))\n",
    "    \n",
    "    return\n",
    "\n",
    "def has_error(tile_id):\n",
    "    errors = glob.glob(os.path.join(interim, 'failed', '*.txt'))\n",
    "    tiles_with_errors = [os.path.basename(error).split('.')[0] for error in errors]\n",
    "    if tile_id in tiles_with_errors:\n",
    "        return True\n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# push our working directories and wrapper classes to the workers on the cluster as well\n",
    "c.scatter([interim, processed, layers, las, fus, target_epsg, num_cores, has_error, log_error], broadcast=True);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Classify points in the lidar point cloud"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the original tiles delivered by the vendor included overlapping edges, our retiling may result in duplicated points in the new tiles from overlapping edges of vendor-provided input tiles. In the next step, we will ensure that only one point with unique (X,Y,Z) coordinates are retained in the point cloud data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dask.delayed\n",
    "def remove_duplicate_points(tile_id):\n",
    "    if not os.path.exists(os.path.join(interim, 'layers', tile_id + '.lay')):\n",
    "        try:\n",
    "            proc = las.lasduplicate(i=os.path.join(interim, 'retiled', tile_id + '.laz'),\n",
    "                                    unique_xyz=True,\n",
    "                                    olay=True,\n",
    "                                    olaydir=layers,\n",
    "                                    odir=layers)\n",
    "        except PipelineError as e:\n",
    "            log_error(tile_id, 'remove_duplicate_points', e.message)\n",
    "    else: # output file already exists\n",
    "        pass\n",
    "    \n",
    "    return tile_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dask.delayed\n",
    "def height_above_ground(tile_id):\n",
    "    if not has_error(tile_id):\n",
    "        try:\n",
    "            proc = las.lasheight(i=os.path.join(interim, 'retiled', tile_id + '.laz'),\n",
    "                                 ilaydir=layers,\n",
    "                                 olay=True,\n",
    "                                 classify_below=(-1.0, 7), # noise\n",
    "                                 olaydir=layers)\n",
    "        except PipelineError as e:\n",
    "            log_error(tile_id, 'height_above_ground', e)\n",
    "    \n",
    "    else: # this tile already has an error recorded in the error_log\n",
    "        pass\n",
    "    \n",
    "    return tile_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll remove points that are isolated as likely noise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dask.delayed\n",
    "def denoise(tile_id):    \n",
    "    if not has_error(tile_id):\n",
    "        try:\n",
    "            proc = las.lasnoise(i=os.path.join(interim, 'retiled', tile_id + '.laz'),\n",
    "                                ilaydir=layers,\n",
    "                                olay=True,\n",
    "                                step=2, # step-size for each cell in 3x3x3 search area is 2m\n",
    "                                isolated=10, # points with fewer than 10 neighbors in a 3x3x3 search area\n",
    "                                olaydir=layers)\n",
    "        except PipelineError as e:\n",
    "            log_error(tile_id, 'denoise', e)\n",
    "    \n",
    "    else: # this tile already has an error recorded in the error_log\n",
    "        pass\n",
    "    \n",
    "    return tile_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we'll classify points as building or high vegetation that meet certain criteria for 'planarity' or 'ruggedness'. \n",
    "\n",
    "**THERE ARE ARGUMENTS IN THE FOLLOWING COMMAND THAT DEPEND UPON THE UNITS OF THE DATA.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dask.delayed\n",
    "def classify(tile_id):\n",
    "    if not has_error(tile_id):\n",
    "        try:\n",
    "            proc = las.lasclassify(i=os.path.join(interim, 'retiled', tile_id + '.laz'),\n",
    "                                   ilaydir=layers,\n",
    "                                   step=2.0, # if your data are in meters\n",
    "                                   planar=0.1, # if your data are in meters\n",
    "                                   rugged=0.4, # if your data are in meters\n",
    "                                   olaz=True,\n",
    "                                   odir=os.path.join(processed, 'points'))\n",
    "        except PipelineError as e:\n",
    "            log_error(tile_id, 'classify', e.message)\n",
    "    \n",
    "    else: # this tile already has an error recorded in the error_log\n",
    "        pass\n",
    "    \n",
    "    return tile_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Generate a bare earth Digital Elevation Model\n",
    "Generate tiles of the bare earth model. This assumes that there are already ground-classified points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dask.delayed\n",
    "def make_dem(tile_id):\n",
    "    odir = os.path.join(interim, 'dem_tiles')\n",
    "    outfile = os.path.join(odir, tile_id + '.tif')\n",
    "    \n",
    "    if not os.path.exists(outfile):\n",
    "        if not has_error(tile_id):\n",
    "            try: \n",
    "                proc = las.las2dem(i=os.path.join(processed, 'points', tile_id + '.laz'),\n",
    "                                   odir=odir,\n",
    "                                   otif=True, # create tiles as GeoTiff rasters\n",
    "                                   keep_class=2, # keep ground-classified returns only\n",
    "                                   step=1, # resolution of output raster, in units of lidar data\n",
    "                                   thin_with_grid=1, # use a 1 x 1 resolution for creating the TIN for the DEM\n",
    "                                   extra_pass=True) # uses two passes over data to execute DEM creation more efficiently\n",
    "            except PipelineError as e:\n",
    "                log_error(tile_id, 'make_dem', e.message)\n",
    "    else: # output file already exists\n",
    "        pass\n",
    "        \n",
    "    return tile_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dask.delayed\n",
    "def add_dem_projection(tile_id):\n",
    "    basename = tile_id + '.tif'\n",
    "    infile = os.path.join(interim, 'dem_tiles', basename)\n",
    "    \n",
    "    if not has_error(tile_id):\n",
    "        proc = subprocess.run(['rio', 'edit-info', '--crs', 'EPSG:{}'.format(target_epsg), infile],\n",
    "                              stderr=subprocess.PIPE, stdout=subprocess.PIPE)\n",
    "        if proc.returncode != 0:\n",
    "            log_error(tile_id, 'add_dem_projection', proc.stderr.decode())\n",
    "    \n",
    "    return tile_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To create a hillshade layer, we'll first, generate hillshade tiles from the bare earth model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dask.delayed\n",
    "def make_hillshade(tile_id):\n",
    "    odir = os.path.join(interim, 'hillshade_tiles')\n",
    "    outfile = os.path.join(odir, tile_id + '.tif')\n",
    "    \n",
    "    if not os.path.exists(outfile):\n",
    "        if not has_error(tile_id):\n",
    "            try: \n",
    "                proc = las.las2dem(i=os.path.join(processed, 'points', tile_id + '.laz'),\n",
    "                       odir=odir,\n",
    "                       otif=True, # create tiles as GeoTiffs\n",
    "                       hillshade=True,\n",
    "                       keep_class=2, # keep ground-classified returns only\n",
    "                       step=1, # resolution of output raster, in units of lidar data\n",
    "                       thin_with_grid=1, # use a 0.5 x 0.5 resolution for creating the TIN for the DEM\n",
    "                       extra_pass=True) # uses two passes over data to execute DEM creation more efficiently\n",
    "            \n",
    "            except PipelineError as e:\n",
    "                log_error(tile_id, 'make_hillshade', e.message)\n",
    "    else: # output file already exists\n",
    "        pass\n",
    "        \n",
    "    return tile_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dask.delayed\n",
    "def add_hillshade_projection(tile_id):\n",
    "    basename = tile_id + '.tif'\n",
    "    infile = os.path.join(interim, 'hillshade_tiles', basename)\n",
    "    \n",
    "    if not has_error(tile_id):\n",
    "        proc = subprocess.run(['rio', 'edit-info', '--crs', 'EPSG:{}'.format(target_epsg), infile],\n",
    "                              stderr=subprocess.PIPE, stdout=subprocess.PIPE)\n",
    "    \n",
    "        if proc.returncode != 0:\n",
    "                log_error(tile_id, 'add_hillshade_projection', proc.stderr.decode())\n",
    "    \n",
    "    return tile_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Identify building footprints\n",
    "First start by building shapefiles showing building boundaries in each buffered tile."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dask.delayed\n",
    "def make_building_footprint_tiles(tile_id):\n",
    "    odir = os.path.join(interim, 'building_tiles')\n",
    "    outfile = os.path.join(odir, tile_id + '.shp')\n",
    "    \n",
    "    if not os.path.exists(outfile):\n",
    "        if not has_error(tile_id):\n",
    "            try:\n",
    "                proc = las.lasboundary(i=os.path.join(processed, 'points', tile_id + '.laz'),\n",
    "                                       keep_class=6, # use only building-classified points\n",
    "                                       disjoint=True, # compute separate polygons for each building\n",
    "                                       concavity=1, # map concave boundary if edge length >= 1m\n",
    "                                       oshp=True,\n",
    "                                       odir=odir)\n",
    "            except PipelineError as e:\n",
    "                log_error(tile_id, 'make_building_footprint_tiles', e.message)\n",
    "    else: # output file already exists\n",
    "        pass\n",
    "        \n",
    "    return tile_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate shapefiles showing the bounding box of each (unbuffered) tile that we'll use to remove buildings that fall in the buffered area."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dask.delayed\n",
    "def unbuffered_tile_boundary(tile_id):\n",
    "    odir = os.path.join(interim, 'tile_boundaries')\n",
    "    outfile = os.path.join(odir, tile_id + '.shp')\n",
    "    \n",
    "    if not os.path.exists(outfile):\n",
    "        if not has_error(tile_id):\n",
    "            try:\n",
    "                proc = las.lasboundary(i=os.path.join(processed, 'points', tile_id + '.laz'),\n",
    "                                       odir=odir,\n",
    "                                       oshp=True,\n",
    "                                       use_tile_bb=True)\n",
    "            except PipelineError as e:\n",
    "                log_error(tile_id, 'unbuffered_tile_boundary', e.message)\n",
    "    else: # output file already exists\n",
    "        pass\n",
    "        \n",
    "    return tile_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each shapefile containing polygons of the building boundaries, we'll use the `clean_buffer_polys` function from `pyFIRS.utils` to remove polygons from a tile if their centroid falls in the buffered area of the tile."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dask.delayed\n",
    "def remove_building_buffers(tile_id, *args):\n",
    "    if type(tile_id) == list:\n",
    "        tile_id = tile_id[0]\n",
    "\n",
    "    basename = tile_id + '.shp'\n",
    "    infile = os.path.join(interim, 'building_tiles', basename)\n",
    "    tile_shp = os.path.join(interim, 'tile_boundaries', basename)\n",
    "    odir = os.path.join(processed, 'vectors', 'building_tiles')\n",
    "    outfile = os.path.join(odir, basename)\n",
    "    \n",
    "    if not os.path.exists(outfile):\n",
    "        if not has_error(tile_id):\n",
    "            try:\n",
    "                clean_buffer_polys(infile,\n",
    "                                   tile_shp,\n",
    "                                   odir=odir,\n",
    "                                   simp_tol=3,\n",
    "                                   simp_topol=True)\n",
    "            except Exception as e:\n",
    "                log_error(tile_id, 'remove_building_buffers', e.message)\n",
    "    else: # output file already exists\n",
    "        pass\n",
    "        \n",
    "    return tile_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Create a Canopy Height Model\n",
    "We're going to switch use a FUSION command line tool to generate a Canopy Height Models (CHMs). \n",
    "\n",
    "### Using FUSION's `canopymodel` to generate CHMs\n",
    "`FUSION` wants to have ground models formatted as .dtm files, for CHM development and for estimating other canopy metrics. Let's generate these ground models first using a 1-meter x-y resolution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dask.delayed\n",
    "def make_ground_dtm(tile_id):\n",
    "    infile = os.path.join(processed, 'points', tile_id + '.laz')\n",
    "    odir = os.path.join(interim, 'dtm_ground_tiles')\n",
    "    outfile = os.path.join(odir, tile_id + '.dtm')\n",
    "    \n",
    "    if not os.path.exists(outfile):\n",
    "        if not has_error(tile_id):\n",
    "            try:\n",
    "                proc = fus.gridsurfacecreate(surfacefile=outfile,\n",
    "                                             cellsize=1,\n",
    "                                             xyunits='M',\n",
    "                                             zunits='M',\n",
    "                                             coordsys=1, # in UTM\n",
    "                                             zone='10N',\n",
    "                                             horizdatum=2, # NAD83\n",
    "                                             vertdatum=2, # NAVD88\n",
    "                                             datafile=infile,\n",
    "                                             las_class=2, # keep only ground-classified points\n",
    "                                             odir=odir) # will make sure output directory is created if doesn't already exist\n",
    "            except PipelineError as e:\n",
    "                log_error(tile_id, 'make_ground_dtm', e.message)\n",
    "    else: # output file already exists\n",
    "        pass\n",
    "                \n",
    "    return tile_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dask.delayed\n",
    "def make_canopy_model(tile_id):\n",
    "    infile = os.path.join(processed, 'points', tile_id + '.laz')\n",
    "    odir = os.path.join(interim, 'chm_tiles')\n",
    "    outfile = os.path.join(odir, tile_id + '.dtm')\n",
    "    \n",
    "    if not os.path.exists(outfile):\n",
    "        if not has_error(tile_id):\n",
    "            try:\n",
    "                proc = fus.canopymodel(surfacefile=outfile,\n",
    "                                       cellsize=0.5, # in meters\n",
    "                                       xyunits='M',\n",
    "                                       zunits='M',\n",
    "                                       coordsys=1, # in UTM\n",
    "                                       zone='10N', # not in UTM\n",
    "                                       horizdatum=2, # NAD83\n",
    "                                       vertdatum=2, # NAVD88\n",
    "                                       datafiles=infile,\n",
    "                                       ground=os.path.join(interim, 'dtm_ground_tiles', tile_id + '.dtm'),\n",
    "                                       median=3, # median smoothing in 3x3 kernel\n",
    "                                       las_class=(1,2,5), # keep only ground, unclassified, and high veg points\n",
    "                                       outlier=(-1,110),\n",
    "                                       asc=True, # also output in ascii format\n",
    "                                       odir=odir) # will make sure output directory is created if doesn't already exist\n",
    "                \n",
    "            except PipelineError as e:\n",
    "                        log_error(tile_id, 'make_canopy_model', e.message)\n",
    "    else: # output file already exists\n",
    "        pass\n",
    "                \n",
    "    return tile_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert the ascii files that `canopymodel` generated into GeoTiffs, specifying their projection. Then cleanup the files `canopymodel` generated that we don't care about."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dask.delayed\n",
    "def asc2tif(tile_id):\n",
    "    infile = os.path.join(interim, 'chm_tiles', tile_id + '.asc')\n",
    "    outfile = os.path.join(interim, 'chm_tiles', tile_id + '.tif')\n",
    "    \n",
    "    if not os.path.exists(outfile):\n",
    "        if not has_error(tile_id):\n",
    "            try:\n",
    "                convert_project(infile, '.tif', 'EPSG:{}'.format(target_epsg))\n",
    "            except Exception as e:\n",
    "                log_error(tile_id, 'asc2tif', e.message)\n",
    "    else: # output file already exists\n",
    "        pass\n",
    "    \n",
    "    return tile_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clip the canopy height model tiles to remove overlapping areas that were from tile buffering to avoid edge effects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dask.delayed\n",
    "def remove_canopy_model_buffer(tile_id, *args):\n",
    "    if type(tile_id) == list:\n",
    "        tile_id = tile_id[0]\n",
    "        \n",
    "    infile = os.path.join(interim, 'chm_tiles', tile_id + '.tif')\n",
    "    in_shp = os.path.join(interim, 'tile_boundaries', tile_id + '.shp')\n",
    "    odir = os.path.join(processed, 'rasters', 'chm_tiles')\n",
    "    outfile = os.path.join(odir, tile_id + '.tif')\n",
    "    \n",
    "    if not os.path.exists(outfile):\n",
    "        if not has_error(tile_id):\n",
    "            try:\n",
    "                clip_tile_from_shp(infile, \n",
    "                                   in_shp, \n",
    "                                   odir,\n",
    "                                   buffer=10)\n",
    "            \n",
    "            except Exception as e:\n",
    "                log_error(tile_id, 'remove_canopy_model_buffer', e.message)\n",
    "    else: # output file already exists\n",
    "        pass\n",
    "    \n",
    "    return tile_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dask.delayed\n",
    "def remove_dem_buffer(tile_id, *args):\n",
    "    if type(tile_id) == list:\n",
    "        tile_id = tile_id[0]\n",
    "        \n",
    "    infile = os.path.join(interim, 'dem_tiles', tile_id + '.tif')\n",
    "    in_shp = os.path.join(interim, 'tile_boundaries', tile_id + '.shp')\n",
    "    odir = os.path.join(processed, 'rasters', 'dem_tiles')\n",
    "    outfile = os.path.join(odir, tile_id + '.tif')\n",
    "    \n",
    "    if not os.path.exists(outfile):\n",
    "        if not has_error(tile_id):\n",
    "            try:\n",
    "                clip_tile_from_shp(infile, \n",
    "                                   in_shp, \n",
    "                                   odir,\n",
    "                                   buffer=10)\n",
    "            \n",
    "            except Exception as e:\n",
    "                log_error(tile_id, 'remove_dem_buffer', e.message)\n",
    "    else: # output file already exists\n",
    "        pass\n",
    "    \n",
    "    return tile_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dask.delayed\n",
    "def remove_hillshade_buffer(tile_id, *args):\n",
    "    if type(tile_id) == list:\n",
    "        tile_id = tile_id[0]\n",
    "        \n",
    "    infile = os.path.join(interim, 'hillshade_tiles', tile_id + '.tif')\n",
    "    in_shp = os.path.join(interim, 'tile_boundaries', tile_id + '.shp')\n",
    "    odir = os.path.join(processed, 'rasters', 'hillshade_tiles')\n",
    "    outfile = os.path.join(odir, tile_id + '.tif')\n",
    "    \n",
    "    if not os.path.exists(outfile):\n",
    "        if not has_error(tile_id):\n",
    "            try:\n",
    "                clip_tile_from_shp(infile, \n",
    "                                   in_shp, \n",
    "                                   odir,\n",
    "                                   buffer=10)\n",
    "            \n",
    "            except Exception as e:\n",
    "                log_error(tile_id, 'remove_hillshade_buffer', e.message)\n",
    "    else: # output file already exists\n",
    "        pass\n",
    "    \n",
    "    return tile_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dask.delayed\n",
    "def tile_done(tile_id, *args, **kwargs):\n",
    "    if type(tile_id) == list:\n",
    "        tile_id = tile_id[0]\n",
    "        \n",
    "    outfile = os.path.join(interim, 'finished', tile_id + '.txt')\n",
    "    os.makedirs(os.path.dirname(outfile), exist_ok=True)\n",
    "    \n",
    "    with open(outfile, '+a') as f:\n",
    "        f.write('{}'.format(tile_id))\n",
    "    \n",
    "    return tile_id\n",
    "\n",
    "@dask.delayed\n",
    "def tiles_done(*args, **kwargs):\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hand-build the computational graph\n",
    "Define the recipe for computations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tile_ids = set(fname(tile) for tile in\n",
    "                         glob.glob(os.path.join(interim, 'retiled', '*.laz'))\n",
    "                        )\n",
    "\n",
    "failed_tiles = set(fname(tile) for tile in \n",
    "                   glob.glob(os.path.join(interim, 'failed', '*.txt'))\n",
    "                  )\n",
    "\n",
    "finished_tiles = set(fname(tile) for tile in \n",
    "                   glob.glob(os.path.join(interim, 'finished', '*.txt'))\n",
    "                  )\n",
    "\n",
    "tiles_to_process = list(tile_ids - failed_tiles - finished_tiles)\n",
    "\n",
    "print('Found {:,d} tiles to process'.format(len(tiles_to_process)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dsk = {}\n",
    "for tile in tile_ids:\n",
    "    # pre-processing of lidar point clouds\n",
    "    dsk['deduped-{}'.format(tile)]=(remove_duplicate_points, tile)\n",
    "    dsk['normalized-{}'.format(tile)]=(height_above_ground, 'deduped-{}'.format(tile))\n",
    "    dsk['denoised-{}'.format(tile)]=(denoise, 'normalized-{}'.format(tile)) \n",
    "    dsk['redenoised-{}'.format(tile)]=(denoise, 'denoised-{}'.format(tile))\n",
    "    dsk['classified-{}'.format(tile)]=(classify, 'redenoised-{}'.format(tile))\n",
    "    dsk['bbox-{}'.format(tile)]=(unbuffered_tile_boundary, 'classified-{}'.format(tile))\n",
    "    \n",
    "    # make derivative vector and raster products\n",
    "    dsk['dem-{}'.format(tile)]=(make_dem, 'classified-{}'.format(tile))\n",
    "    dsk['ground_dtm-{}'.format(tile)]=(make_ground_dtm, 'classified-{}'.format(tile))\n",
    "    dsk['canopy-{}'.format(tile)]=(make_canopy_model, 'ground_dtm-{}'.format(tile))\n",
    "    dsk['hillshade-{}'.format(tile)]=(make_hillshade, 'classified-{}'.format(tile))\n",
    "    dsk['buildings_buffered-{}'.format(tile)]=(make_building_footprint_tiles, 'classified-{}'.format(tile))\n",
    "    \n",
    "    # post-processing of derivative products\n",
    "    dsk['projected_dem-{}'.format(tile)]=(add_dem_projection, 'dem-{}'.format(tile))\n",
    "    dsk['projected_hillshade-{}'.format(tile)]=(add_hillshade_projection, 'hillshade-{}'.format(tile))\n",
    "    dsk['buildings_unbuffered-{}'.format(tile)]=(remove_building_buffers, \n",
    "                                                 ['buildings_buffered-{}'.format(tile), 'bbox-{}'.format(tile)])\n",
    "    dsk['canopy_tif-{}'.format(tile)] = (asc2tif, 'canopy-{}'.format(tile))\n",
    "    dsk['canopy_clipped-{}'.format(tile)]=(remove_canopy_model_buffer, ['canopy_tif-{}'.format(tile), 'bbox-{}'.format(tile)])\n",
    "    dsk['dem_clipped-{}'.format(tile)]=(remove_dem_buffer, ['projected_dem-{}'.format(tile), 'bbox-{}'.format(tile)])\n",
    "    dsk['hillshade_clipped-{}'.format(tile)]=(remove_hillshade_buffer, ['projected_hillshade-{}'.format(tile), 'bbox-{}'.format(tile)])\n",
    "    \n",
    "    # a catch-all when a tile is finished processing\n",
    "    dsk['tile_done-{}'.format(tile)]=(tile_done, ['unbuffered-{}'.format(tile),\n",
    "                                                  'dem_clipped-{}'.format(tile),\n",
    "                                                  'hillshade_clipped-{}'.format(tile),\n",
    "                                                  'buildings_unbuffered-{}'.format(tile),\n",
    "                                                  'canopy_clipped-{}'.format(tile)])\n",
    "    \n",
    "dsk['tiles_done'] = (tiles_done, ['tile_done-{}'.format(tile) for tile in tile_ids])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The computational pipeline, including dependencies of each step in the pipeline, can be specified using a dictionary. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "example_tile_graph = c.get(dsk, 'tile_done-{}'.format(tiles_to_process[0]))\n",
    "example_tile_graph.visualize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get Dask to determine how to get to the last state of the tile-processing pipeline, building a computational graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tiles_graph = c.get(dsk, 'tiles_done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tiles_results = c.compute(tiles_graph) # this might take a while..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "progress(tiles_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "failed = glob.glob(os.path.join(interim, 'failed', '*.txt'))\n",
    "finished = [os.path.basename(tile).split('.')[0] for tile in glob.glob(os.path.join(interim, 'finished', '*.txt'))]\n",
    "\n",
    "summary = '''\n",
    "Processing Summary\n",
    "-------------------\n",
    "{:>5,d} tiles in acquisition\n",
    "{:>5,d} tiles previously finished in acquisition\n",
    "\n",
    "{:>5,d} tiles being processed in this run\n",
    "{:>5,d} tiles from this run finished\n",
    "\n",
    "{:>5,d} tiles failed\n",
    "'''.format(len(tile_ids), \n",
    "           len(finished_tiles),\n",
    "           len(tiles_to_process), \n",
    "           len(finished) - (len(tile_ids) - len(tiles_to_process)), \n",
    "           len(failed))\n",
    "\n",
    "total_percent_unfinished = int(70 * (1-len(finished)/len(tile_ids)))\n",
    "total_percent_finished = int(70 * len(finished)/len(tile_ids))\n",
    "total_percent_failed = int(70 * len(failed)/len(tile_ids))\n",
    "\n",
    "this_run_unfinished = int(70 - 70*(len(finished) - (len(tile_ids) - len(tiles_to_process))) / len(tiles_to_process))\n",
    "this_run_finished = int(70*(len(finished) - (len(tile_ids) - len(tiles_to_process))) / len(tiles_to_process))\n",
    "\n",
    "print(summary)\n",
    "print('|' + '=' * this_run_finished + \n",
    "      ' '* this_run_unfinished + \n",
    "      '!' * total_percent_failed + \n",
    "      '|  {:.1%} this run'.format(\n",
    "          (len(finished) - (len(tile_ids) - len(tiles_to_process))) / len(tiles_to_process)))\n",
    "print('|' + '=' * total_percent_finished + \n",
    "      ' ' * total_percent_unfinished + \n",
    "      '!' * total_percent_failed + \n",
    "      '|  {:.1%} total'.format(\n",
    "          len(finished)/len(tile_ids)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for filename in failed:\n",
    "    with open(filename) as f:\n",
    "        print([line for line in f.readlines() if line.rstrip() != ''])\n",
    "    print('----------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# c.cancel(tiles_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# c.close()\n",
    "# cluster.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pyFIRS]",
   "language": "python",
   "name": "conda-env-pyFIRS-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
