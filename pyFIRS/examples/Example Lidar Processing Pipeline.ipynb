{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This workflow follows the framework of two tutorials on lidar [pre-processing](https://rapidlasso.com/2013/10/13/tutorial-lidar-preparation/) and [information extraction](https://rapidlasso.com/2013/10/20/tutorial-derivative-production/) published by Martin Isenburg. It assumes that your lidar data is in tiles and has ground returns already classified. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, shutil, glob\n",
    "import geopandas as gpd, pandas as pd\n",
    "import rasterio\n",
    "from matplotlib import pyplot as plt\n",
    "from pyFIRS.utils import lastools, formatters\n",
    "import ipyparallel as ipp\n",
    "from tqdm import tqdm_notebook\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "las_src = 'C:/LAStools/bin/'\n",
    "las = lastools.useLAStools(las_src)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Specify some key parameters for the processing pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# where the raw lidar data is currently stored\n",
    "raw_tiles = 'C:/Swinomish_Lidar/data/source/2016_Lidar/points/*.laz'\n",
    "workdir = os.path.abspath('C:/Swinomish_Lidar/processed_on_FORD')\n",
    "\n",
    "num_cores=4 # how many cores to use for parallel processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take a look at the format of the lidar data provided by the vendor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "lasinfo (180812) report for 'C:/Swinomish_Lidar/data/source/2016_Lidar/points\\q48122C5203.laz'\n",
      "reporting all LAS header entries:\n",
      "  file signature:             'LASF'\n",
      "  file source ID:             0\n",
      "  global_encoding:            1\n",
      "  project ID GUID data 1-4:   00000000-0000-0000-6557-747300004157\n",
      "  version major.minor:        1.2\n",
      "  system identifier:          'Quantum Spatial'\n",
      "  generating software:        'LasMonkey 2.2.6'\n",
      "  file creation day/year:     88/2017\n",
      "  header size:                227\n",
      "  offset to point data:       2392\n",
      "  number var. length records: 3\n",
      "  point data format:          1\n",
      "  point data record length:   33\n",
      "  number of point records:    113643\n",
      "  number of points by return: 112643 994 6 0 0\n",
      "  scale factor x y z:         0.01 0.01 0.01\n",
      "  offset x y z:               0 0 0\n",
      "  min x y z:                  1145300.79 1115326.59 -325.85\n",
      "  max x y z:                  1147258.13 1115803.65 201.64\n",
      "variable length header record 1 of 3:\n",
      "  reserved             0\n",
      "  user ID              'LASF_Projection'\n",
      "  record ID            2112\n",
      "  length after header  1079\n",
      "  description          'WKT Projection'\n",
      "    WKT OGC COORDINATE SYSTEM:\n",
      "    COMPD_CS[\"NAD83(HARN) / Washington South (ftUS) + NAVD88 height (ftUS)\",PROJCS[\"NAD83(HARN) / Washington South (ftUS)\",GEOGCS[\"NAD83(HARN)\",DATUM[\"NAD83 (High Accuracy Reference Network)\",SPHEROID[\"GRS 1980\",6378137,298.257222101,AUTHORITY[\"EPSG\",\"7019\"]],TOWGS84[0,0,0,0,0,0,0],AUTHORITY[\"EPSG\",\"6152\"]],PRIMEM[\"Greenwich\",0,AUTHORITY[\"EPSG\",\"8901\"]],UNIT[\"degree\",0.0174532925199433,AUTHORITY[\"EPSG\",\"9122\"]],AUTHORITY[\"EPSG\",\"4152\"]],PROJECTION[\"Lambert_Conformal_Conic_2SP\"],PARAMETER[\"standard_parallel_1\",47.33333333333334],PARAMETER[\"standard_parallel_2\",45.83333333333334],PARAMETER[\"latitude_of_origin\",45.33333333333334],PARAMETER[\"central_meridian\",-120.5],PARAMETER[\"false_easting\",1640416.667],PARAMETER[\"false_northing\",0],UNIT[\"US survey foot\",0.3048006096012192,AUTHORITY[\"EPSG\",\"9003\"]],AXIS[\"X\",EAST],AXIS[\"Y\",NORTH],AUTHORITY[\"EPSG\",\"2927\"]],VERT_CS[\"NAVD88 height (ftUS)\",VERT_DATUM[\"North American Vertical Datum 1988\",2005,AUTHORITY[\"EPSG\",\"5103\"]],UNIT[\"US survey foot\",0.3048006096012192,AUTHORITY[\"EPSG\",\"9003\"]],AXIS[\"Up\",UP],AUTHORITY[\"EPSG\",\"6360\"]]]\n",
      "variable length header record 2 of 3:\n",
      "  reserved             0\n",
      "  user ID              'lascompatible'\n",
      "  record ID            22204\n",
      "  length after header  156\n",
      "  description          'by LAStools of rapidlasso GmbH'\n",
      "variable length header record 3 of 3:\n",
      "  reserved             0\n",
      "  user ID              'LASF_Spec'\n",
      "  record ID            4\n",
      "  length after header  768\n",
      "  description          'by LAStools of rapidlasso GmbH'\n",
      "    Extra Byte Descriptions\n",
      "      data type: 4 (short), name \"LAS 1.4 scan angle\", description: \"additional attributes\", scale: 0.006, offset: 0 (not set)\n",
      "      data type: 1 (unsigned char), name \"LAS 1.4 extended returns\", description: \"additional attributes\", scale: 1 (not set), offset: 0 (not set)\n",
      "      data type: 1 (unsigned char), name \"LAS 1.4 classification\", description: \"additional attributes\", scale: 1 (not set), offset: 0 (not set)\n",
      "      data type: 1 (unsigned char), name \"LAS 1.4 flags and channel\", description: \"additional attributes\", scale: 1 (not set), offset: 0 (not set)\n",
      "LASzip compression (version 2.4r1 c2 50000): POINT10 2 GPSTIME11 2 BYTE 2\n",
      "reporting minimum and maximum for all LAS point record entries ...\n",
      "  X           114530079  114725813\n",
      "  Y           111532659  111580365\n",
      "  Z              -32585      20164\n",
      "  intensity        1391      65535\n",
      "  return_number       1          3\n",
      "  number_of_returns   1          3\n",
      "  edge_of_flight_line 0          0\n",
      "  scan_direction_flag 0          0\n",
      "  classification      1         10\n",
      "  scan_angle_rank   -31         22\n",
      "  user_data           0          1\n",
      "  point_source_ID 10136      10139\n",
      "  gps_time 142354713.607532 142355674.165875\n",
      "number of first returns:        112643\n",
      "number of intermediate returns: 6\n",
      "number of last returns:         112644\n",
      "number of single returns:       111650\n",
      "overview over number of returns of given pulse: 111650 1975 18 0 0 0 0\n",
      "histogram of classification of points:\n",
      "           30363  unclassified (1)\n",
      "            9268  ground (2)\n",
      "            8744  noise (7)\n",
      "           64753  water (9)\n",
      "             515  rail (10)\n",
      " +-> flagged as withheld:  8773\n",
      "\n"
     ]
    }
   ],
   "source": [
    "vendor_tiles = glob.glob(raw_tiles)\n",
    "las.lasinfo(i=vendor_tiles[0], \n",
    "            echo=True);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up the workspace "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define data handling directories\n",
    "raw, interim, processed = os.path.join(workdir,'raw'), os.path.join(workdir,'interim'), os.path.join(workdir,'processed')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Get the raw data into our working directory\n",
    "First, move the tiles over to our working directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done moving tiles into working directory.\n",
      "CPU times: user 12 ms, sys: 8 ms, total: 20 ms\n",
      "Wall time: 2min 7s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "las.las2las(i=raw_tiles,\n",
    "            odir=raw,\n",
    "            drop_withheld=True, # drop any points flagged as withheld by vendor\n",
    "            drop_class=(7,18), # drop any points classified as noise by vendor\n",
    "            olaz=True,\n",
    "            cores=num_cores)\n",
    "print('Done moving tiles into working directory.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, create spatial indexes for the input files to allow fast spatial queries (which we'll use for adding buffers)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done adding spatial indexes.\n",
      "CPU times: user 12 ms, sys: 4 ms, total: 16 ms\n",
      "Wall time: 1min 7s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "infiles = os.path.join(raw,'*.laz')\n",
    "\n",
    "las.lasindex(i=infiles, \n",
    "             cores=num_cores)\n",
    "\n",
    "print(\"Done adding spatial indexes.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we'll retile the data to add buffers for avoiding edge effects during processing.\n",
    "\n",
    "**THERE ARE ARGUMENTS IN THE FOLLOWING COMMAND THAT DEPEND UPON THE UNITS OF THE DATA.**\n",
    "\n",
    "If your data are in meters, you should change these parameters, or consider reprojecting the data to a projection that is in feet when you copy the source data into our working directory using `las2las` command at the top of this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done retiling and adding buffers. Created 76 tiles.\n",
      "CPU times: user 28 ms, sys: 16 ms, total: 44 ms\n",
      "Wall time: 3min 26s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "infiles = os.path.join(raw, '*.laz')\n",
    "odir = os.path.join(interim, 'retiled')\n",
    "\n",
    "las.lastile(i=infiles,\n",
    "            tile_size=3500, # assumes units are in feet... if using meters, change to 1000 or comment out\n",
    "            buffer=100, # assumes units are in feet... if using meters, change to 25\n",
    "            flag_as_withheld=True,\n",
    "            olaz=True,\n",
    "            odir=odir,\n",
    "            cores=num_cores)\n",
    "\n",
    "new_tiles = glob.glob(os.path.join(odir,'*.laz'))\n",
    "print('Done retiling and adding buffers. Created {} tiles.'.format(len(new_tiles)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# in case you need to save on storage space\n",
    "# clean out the raw directory now that you have retiled data to work with\n",
    "# shutil.rmtree(raw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Classify points in the lidar point cloud\n",
    "Remove noise and identify high vegetation and buildings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done denoising tiles.\n",
      "CPU times: user 20 ms, sys: 16 ms, total: 36 ms\n",
      "Wall time: 3min 56s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "infiles=os.path.join(interim, 'retiled', '*.laz')\n",
    "odir = os.path.join(interim, 'denoised')\n",
    "\n",
    "las.lasnoise(i=infiles, \n",
    "             remove_noise=True,\n",
    "             odir=odir, \n",
    "             olaz=True, \n",
    "             cores=num_cores) # use parallel processing\n",
    "\n",
    "print('Done denoising tiles.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, calculate the height aboveground for each point for use in classifying them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done calculating height above ground.\n",
      "CPU times: user 120 ms, sys: 116 ms, total: 236 ms\n",
      "Wall time: 6min 57s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "infiles=os.path.join(interim, 'denoised', '*.laz')\n",
    "odir = os.path.join(interim, 'lasheight')\n",
    "\n",
    "las.lasheight(i=infiles,\n",
    "              odir=odir, \n",
    "              olaz=True, \n",
    "              cores=num_cores) # use parallel processing\n",
    "\n",
    "print('Done calculating height above ground.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we'll classify points (that haven't already been classified into meaningful categories) as building or high vegetation that meet certain criteria for 'planarity' or 'ruggedness'. \n",
    "\n",
    "**THERE ARE ARGUMENTS IN THE FOLLOWING COMMAND THAT DEPEND UPON THE UNITS OF THE DATA.**\n",
    "\n",
    "If your data are in meters, you should change these parameters, or consider reprojecting the data to a projection that is in feet when you copy the source data into our working directory using `las2las` command at the top of this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done classifying lidar tiles.\n",
      "CPU times: user 268 ms, sys: 240 ms, total: 508 ms\n",
      "Wall time: 23min 3s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "infiles = os.path.join(interim, 'lasheight', '*.laz')\n",
    "odir = os.path.join(interim, 'classified')\n",
    "\n",
    "las.lasclassify(i=infiles,\n",
    "                odir=odir,\n",
    "                olaz=True,\n",
    "                step=5, # if your data are in meters, the LAStools default is 2.0\n",
    "                planar=0.5, # if your data are in meters, the LAStools default is 0.1\n",
    "                rugged=1, # if your data are in meters, the LAStools default is 0.4\n",
    "                ignore_class=(2,9,10,11,13,14,15,16,17), # ignore points already classified meaningfully\n",
    "                cores=num_cores) # use parallel processing\n",
    "\n",
    "print('Done classifying lidar tiles.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, remove buffer points from the classified tiles and put the clean tiles in the processed folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done trimming classified lidar tiles.\n",
      "CPU times: user 12 ms, sys: 8 ms, total: 20 ms\n",
      "Wall time: 2min 16s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "infiles = os.path.join(interim, 'classified', '*.laz')\n",
    "odir = os.path.join(processed, 'points')\n",
    "\n",
    "las.las2las(i=infiles,\n",
    "            odir=odir,\n",
    "            olaz=True,\n",
    "            drop_withheld=True, # remove points in tile buffers that were flagged as withhled with lastile\n",
    "            set_user_data=0, # remove height aboveground calculated using lasheight\n",
    "            cores=num_cores)\n",
    "\n",
    "print('Done trimming classified lidar tiles.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Produce a shapefile showing the layout of the tiles of points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Produced a shapefile overview of clean tile boundaries.\n",
      "CPU times: user 8 ms, sys: 8 ms, total: 16 ms\n",
      "Wall time: 4.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "infiles = os.path.join(processed, 'points', '*.laz')\n",
    "odir = os.path.join(processed, 'vectors')\n",
    "\n",
    "las.lasboundary(i=infiles,\n",
    "                odir=odir,\n",
    "                o='tiles.shp',\n",
    "                oshp=True,\n",
    "                use_bb=True, # use bounding box of tiles\n",
    "                overview=True,\n",
    "                labels=True,\n",
    "                cores=num_cores) # use parallel processing\n",
    "\n",
    "print('Produced a shapefile overview of clean tile boundaries.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# remove intermediate lidar files if you want to reclaim storage space\n",
    "# shutil.rmtree(os.path.join(interim, 'retiled'))\n",
    "# shutil.rmtree(os.path.join(interim, 'denoised'))\n",
    "# shutil.rmtree(os.path.join(interim, 'lasheight'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Generate a bare earth Digital Elevation Model\n",
    "Generate tiles of the bare earth model. This assumes that there are already ground-classified points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done producing bare earth tiles.\n",
      "CPU times: user 28 ms, sys: 8 ms, total: 36 ms\n",
      "Wall time: 3min 48s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "infiles = os.path.join(interim, 'classified', '*.laz')\n",
    "odir = os.path.join(processed, 'rasters', 'DEM_tiles')\n",
    "\n",
    "las.las2dem(i=infiles,\n",
    "            odir=odir,\n",
    "            obil=True, # create tiles as .bil rasters\n",
    "            keep_class=2, # keep ground-classified returns only\n",
    "            thin_with_grid=1, # use a 1 x 1 resolution for creating the TIN for the DEM\n",
    "            extra_pass=True, # uses two passes over data to execute DEM creation more efficiently\n",
    "            use_tile_bb=True, # remove buffers from tiles\n",
    "            cores=num_cores) \n",
    "\n",
    "print('Done producing bare earth tiles.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Merge the bare earth tiles into a single DEM formatted as a GeoTiff."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done producing merged DEM GeoTiff.\n",
      "CPU times: user 4 ms, sys: 8 ms, total: 12 ms\n",
      "Wall time: 3min 31s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "infiles = os.path.join(processed, 'rasters', 'DEM_tiles', '*.bil')\n",
    "outfile = os.path.join(processed, 'rasters', 'dem.tif')\n",
    "\n",
    "las.lasgrid(i=infiles,\n",
    "            merged=True,\n",
    "            o=outfile)\n",
    "\n",
    "print('Done producing merged DEM GeoTiff.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To create a hillshade layer, we'll first, generate hillshade tiles from the bare earth model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done producing hillshade bare earth tiles.\n",
      "CPU times: user 24 ms, sys: 16 ms, total: 40 ms\n",
      "Wall time: 3min 43s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "infiles = os.path.join(interim, 'classified', '*.laz')\n",
    "odir = os.path.join(processed, 'rasters', 'hillshade_tiles')\n",
    "\n",
    "las.las2dem(i=infiles,\n",
    "            odir=odir,\n",
    "            obil=True, # create tiles as .bil rasters\n",
    "            cores=num_cores,\n",
    "            hillshade=True,\n",
    "            keep_class=2, # keep ground-classified returns only\n",
    "            thin_with_grid=1, # use a 0.5 x 0.5 resolution for creating the TIN for the DEM\n",
    "            extra_pass=True, # uses two passes over data to execute DEM creation more efficiently\n",
    "            use_tile_bb=True) # remove buffers from tiles\n",
    "\n",
    "print('Done producing hillshade bare earth tiles.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now merge the hillshade tiles into a single raster formatted as GeoTiff."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done producing merged hillshade GeoTiff.\n",
      "CPU times: user 4 ms, sys: 12 ms, total: 16 ms\n",
      "Wall time: 2min 18s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "infiles = os.path.join(processed, 'rasters', 'hillshade_tiles', '*.bil')\n",
    "outfile = os.path.join(processed, 'rasters', 'hillshade.tif')\n",
    "\n",
    "las.lasgrid(i=infiles,\n",
    "            merged=True,\n",
    "            o=outfile)\n",
    "\n",
    "print('Done producing merged hillshade GeoTiff.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Identify building footprints\n",
    "First start by building shapefiles showing building boundaries in each buffered tile."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done producing building footprints in buffered tiles.\n",
      "CPU times: user 12 ms, sys: 16 ms, total: 28 ms\n",
      "Wall time: 1min 15s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "infiles = os.path.join(interim, 'classified', '*.laz')\n",
    "odir = os.path.join(interim, 'building_tiles')\n",
    "\n",
    "las.lasboundary(i=infiles,\n",
    "                odir=odir,\n",
    "                keep_class=6, # use only building-classified points\n",
    "                disjoint=True, # compute separate polygons for each building\n",
    "                concavity=3, # map concave boundary if edge length >= 3ft\n",
    "                cores=num_cores)\n",
    "\n",
    "print('Done producing building footprints in buffered tiles.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate shapefiles showing the bounding box of each (unbuffered) tile that we'll use to remove buildings that fall in the buffered area."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done producing boundaries of unbuffered tiles.\n",
      "CPU times: user 0 ns, sys: 8 ms, total: 8 ms\n",
      "Wall time: 5.74 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "infiles = os.path.join(processed, 'points', '*.laz')\n",
    "odir = os.path.join(interim, 'tile_boundaries')\n",
    "\n",
    "las.lasboundary(i=infiles,\n",
    "                odir=odir,\n",
    "                oshp=True,\n",
    "                use_tile_bb=True,\n",
    "                cores=num_cores)\n",
    "\n",
    "print('Done producing boundaries of unbuffered tiles.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each shapefile containing polygons of the building boundaries, we'll use the `clean_tile` function to remove polygons from a tile if their centroid falls in the buffered area of the tile."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done producing building footprints in cleaned (unbuffered) tiles.\n",
      "CPU times: user 11.4 s, sys: 4 ms, total: 11.4 s\n",
      "Wall time: 12.1 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "building_tiles = glob.glob(os.path.join(interim, 'building_tiles', '*.shp'))\n",
    "odir = os.path.join(processed, 'vectors', 'building_tiles')\n",
    "\n",
    "for poly_shp in building_tiles:\n",
    "    fname = os.path.basename(poly_shp)\n",
    "    tile_shp = os.path.join(interim, 'tile_boundaries', fname)\n",
    "    lastools.clean_tile(poly_shp, tile_shp, odir, simp_tol=3, simp_topol=True)\n",
    "\n",
    "print('Done producing building footprints in cleaned (unbuffered) tiles.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Merge the cleaned tiles of building footprints together into a single shapefile. We'll use geopandas to concatenate all the polygons together into a single geodataframe and then write out to a new shapefile."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done merging tiles of building footprints into a single shapefile.\n",
      "CPU times: user 4.29 s, sys: 12 ms, total: 4.3 s\n",
      "Wall time: 4.32 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "building_tiles = glob.glob(os.path.join(processed, 'vectors', 'building_tiles', '*.shp'))\n",
    "# create a list of geodataframes containing the tiles of building footprints\n",
    "gdflist = [gpd.read_file(tile) for tile in building_tiles]\n",
    "# merge them all together\n",
    "merged = gpd.GeoDataFrame(pd.concat(gdflist, ignore_index=True))\n",
    "# using pandas concat caused us to lose projection information, so let's add that back in\n",
    "merged.crs = gdflist[0].crs\n",
    "# and write the merged data to a new shapefile\n",
    "merged.to_file(os.path.join(processed,'vectors','buildings.shp'))\n",
    "\n",
    "print('Done merging tiles of building footprints into a single shapefile.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Create a Canopy Height Model\n",
    "`pyFIRS` includes a method for generating pit free Canopy Height Models following the approach laid out by Martin Isenburg in this [blog post](https://rapidlasso.com/2014/11/04/rasterizing-perfect-canopy-height-models-from-lidar/)--which you can call using `las.pitfree(...)`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This implementation of `las.pitfree` for canopy height modeling chains together a sequence of LAStools command line tools to generate a CHM for a single tile of lidar data. Because this function processes a single tile at a time, we can't take advantage of LAStools native multi-core processing capabilities for parallel processing. To implement the `las.pitfree` command on multiple cores, we'll use `ipyparallel` to process the jobs asynchronously.\n",
    "\n",
    "For this to work, you'll need to launch a parallel computing cluster. If you have ipyparallel installed in the computing environment that was used to launch this notebook (which may be different from the kernel you're using to execute it), you should be able to start a parallel computing cluster by switching to the \"IPython Clusters\" tab of the \"Home\" tab that was created when you called `jupyter notebook` from the console. \n",
    "\n",
    "Alternatively, you can also issue a call from the command line to fire up a cluster of workers. You will need to have your virtual environment activated (`activate envname` on Windows or `source active envname` on other OSs), and then call:\n",
    "\n",
    "```> ipcluster start -n #``` where # is the number of cores you want to use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1, 2, 3]\n",
      "importing subprocess on engine(s)\n",
      "importing os on engine(s)\n",
      "importing fusion from pyFIRS.utils on engine(s)\n",
      "importing lastools from pyFIRS.utils on engine(s)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<AsyncResult: _push>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rc = ipp.Client() # a client for controlling the workers\n",
    "print(rc.ids) # how many workers?\n",
    "\n",
    "dv = rc[:] # create a direct view of the workers\n",
    "v = rc.load_balanced_view() # create a load-balanced view of the workers\n",
    "\n",
    "# import the relevant packages to all the workers\n",
    "with dv.sync_imports():\n",
    "    import subprocess\n",
    "    import os\n",
    "    from pyFIRS.utils import lastools\n",
    "    \n",
    "# push some objects to the workers\n",
    "dv.push(dict(raw=raw, interim=interim, processed=processed, las_src=las_src))\n",
    "\n",
    "# instantiate the useLAStools class on the workers \n",
    "# so they can each execute LAStools commands\n",
    "%px las = lastools.useLAStools(las_src) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_pitfree(infile):\n",
    "    odir = os.path.join(interim, 'chm_pitfree')\n",
    "    return las.pitfree(lasfile=infile,\n",
    "                       outdir=odir,\n",
    "                       units='ft',\n",
    "                       cleanup=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['C:\\\\Swinomish_Lidar\\\\processed_on_FORD\\\\interim\\\\classified\\\\1130500_1134000.laz',\n",
       " 'C:\\\\Swinomish_Lidar\\\\processed_on_FORD\\\\interim\\\\classified\\\\1130500_1137500.laz',\n",
       " 'C:\\\\Swinomish_Lidar\\\\processed_on_FORD\\\\interim\\\\classified\\\\1130500_1141000.laz']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tiles_to_run = glob.glob(os.path.join(interim, 'classified', '*.laz'))\n",
    "tiles_to_run[0:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Started batch processing.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9f8a2acb67e74576a0177869625a18df",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Progress', max=76), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# start asynchronous batch with load-balanced view\n",
    "res = v.map_async(run_pitfree, tiles_to_run)\n",
    "print('Started batch processing.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if you want to track progress while the asynchronous batch is running...\n",
    "# this will lock the notebook until the batch is completed.\n",
    "jobs_done = res.progress\n",
    "with tqdm_notebook(total=len(res), initial=jobs_done, desc='Progress', unit='tile') as pbar:\n",
    "    while not res.ready():\n",
    "        new_progress = res.progress - jobs_done\n",
    "        jobs_done += new_progress\n",
    "        pbar.update(new_progress)\n",
    "        time.sleep(0.5)\n",
    "    \n",
    "    # once jobs are completed (i.e., res.ready() returns True)\n",
    "    # update the progress bar one last time\n",
    "    pbar.update(len(res)-jobs_done)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pyFIRS]",
   "language": "python",
   "name": "conda-env-pyFIRS-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
