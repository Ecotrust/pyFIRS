{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This workflow follows the framework of two tutorials on lidar [pre-processing](https://rapidlasso.com/2013/10/13/tutorial-lidar-preparation/) and [information extraction](https://rapidlasso.com/2013/10/20/tutorial-derivative-production/) published by Martin Isenburg. It assumes that your lidar data is in tiles and has ground returns already classified. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, shutil, glob\n",
    "import geopandas as gpd, pandas as pd\n",
    "import rasterio\n",
    "from matplotlib import pyplot as plt\n",
    "from pyFIRS.utils import lastools, fusion\n",
    "import ipyparallel as ipp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "las = lastools.useLAStools('/storage/lidar/LAStools/bin/')\n",
    "fus = fusion.useFUSION('/storage/lidar/FUSION/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Specify some key parameters for the processing pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# where the raw lidar data is currently stored\n",
    "raw_tiles = '/storage/lidar/Swinomish_Lidar_2016/source/*.laz'\n",
    "workdir = os.path.abspath('/storage/lidar/Swinomish_Lidar_2016')\n",
    "\n",
    "num_cores=32 # how many cores to use for parallel processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take a look at the format of the lidar data provided by the vendor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "lasinfo (180812) report for '/storage/lidar/Swinomish_Lidar_2016/source/q48122D5415.laz'\r\n",
      "reporting all LAS header entries:\r\n",
      "  file signature:             'LASF'\r\n",
      "  file source ID:             0\r\n",
      "  global_encoding:            1\r\n",
      "  project ID GUID data 1-4:   00000000-0000-0000-6557-747300004157\r\n",
      "  version major.minor:        1.2\r\n",
      "  system identifier:          'Quantum Spatial'\r\n",
      "  generating software:        'LasMonkey 2.2.6'\r\n",
      "  file creation day/year:     88/2017\r\n",
      "  header size:                227\r\n",
      "  offset to point data:       2392\r\n",
      "  number var. length records: 3\r\n",
      "  point data format:          1\r\n",
      "  point data record length:   33\r\n",
      "  number of point records:    36190794\r\n",
      "  number of points by return: 17005960 12142658 5370083 1418786 228637\r\n",
      "  scale factor x y z:         0.01 0.01 0.01\r\n",
      "  offset x y z:               0 0 0\r\n",
      "  min x y z:                  1151361.60 1124700.02 -253.61\r\n",
      "  max x y z:                  1154513.99 1129337.94 380.54\r\n",
      "variable length header record 1 of 3:\r\n",
      "  reserved             0\r\n",
      "  user ID              'LASF_Projection'\r\n",
      "  record ID            2112\r\n",
      "  length after header  1079\r\n",
      "  description          'WKT Projection'\r\n",
      "    WKT OGC COORDINATE SYSTEM:\r\n",
      "    COMPD_CS[\"NAD83(HARN) / Washington South (ftUS) + NAVD88 height (ftUS)\",PROJCS[\"NAD83(HARN) / Washington South (ftUS)\",GEOGCS[\"NAD83(HARN)\",DATUM[\"NAD83 (High Accuracy Reference Network)\",SPHEROID[\"GRS 1980\",6378137,298.257222101,AUTHORITY[\"EPSG\",\"7019\"]],TOWGS84[0,0,0,0,0,0,0],AUTHORITY[\"EPSG\",\"6152\"]],PRIMEM[\"Greenwich\",0,AUTHORITY[\"EPSG\",\"8901\"]],UNIT[\"degree\",0.0174532925199433,AUTHORITY[\"EPSG\",\"9122\"]],AUTHORITY[\"EPSG\",\"4152\"]],PROJECTION[\"Lambert_Conformal_Conic_2SP\"],PARAMETER[\"standard_parallel_1\",47.33333333333334],PARAMETER[\"standard_parallel_2\",45.83333333333334],PARAMETER[\"latitude_of_origin\",45.33333333333334],PARAMETER[\"central_meridian\",-120.5],PARAMETER[\"false_easting\",1640416.667],PARAMETER[\"false_northing\",0],UNIT[\"US survey foot\",0.3048006096012192,AUTHORITY[\"EPSG\",\"9003\"]],AXIS[\"X\",EAST],AXIS[\"Y\",NORTH],AUTHORITY[\"EPSG\",\"2927\"]],VERT_CS[\"NAVD88 height (ftUS)\",VERT_DATUM[\"North American Vertical Datum 1988\",2005,AUTHORITY[\"EPSG\",\"5103\"]],UNIT[\"US survey foot\",0.3048006096012192,AUTHORITY[\"EPSG\",\"9003\"]],AXIS[\"Up\",UP],AUTHORITY[\"EPSG\",\"6360\"]]]\r\n",
      "variable length header record 2 of 3:\r\n",
      "  reserved             0\r\n",
      "  user ID              'lascompatible'\r\n",
      "  record ID            22204\r\n",
      "  length after header  156\r\n",
      "  description          'by LAStools of rapidlasso GmbH'\r\n",
      "variable length header record 3 of 3:\r\n",
      "  reserved             0\r\n",
      "  user ID              'LASF_Spec'\r\n",
      "  record ID            4\r\n",
      "  length after header  768\r\n",
      "  description          'by LAStools of rapidlasso GmbH'\r\n",
      "    Extra Byte Descriptions\r\n",
      "      data type: 4 (short), name \"LAS 1.4 scan angle\", description: \"additional attributes\", scale: 0.006, offset: 0 (not set)\r\n",
      "      data type: 1 (unsigned char), name \"LAS 1.4 extended returns\", description: \"additional attributes\", scale: 1 (not set), offset: 0 (not set)\r\n",
      "      data type: 1 (unsigned char), name \"LAS 1.4 classification\", description: \"additional attributes\", scale: 1 (not set), offset: 0 (not set)\r\n",
      "      data type: 1 (unsigned char), name \"LAS 1.4 flags and channel\", description: \"additional attributes\", scale: 1 (not set), offset: 0 (not set)\r\n",
      "LASzip compression (version 2.4r1 c2 50000): POINT10 2 GPSTIME11 2 BYTE 2\r\n",
      "reporting minimum and maximum for all LAS point record entries ...\r\n",
      "  X           115136160  115451399\r\n",
      "  Y           112470002  112933794\r\n",
      "  Z              -25361      38054\r\n",
      "  intensity         115      65535\r\n",
      "  return_number       1          7\r\n",
      "  number_of_returns   1          7\r\n",
      "  edge_of_flight_line 0          0\r\n",
      "  scan_direction_flag 0          0\r\n",
      "  classification      1          7\r\n",
      "  scan_angle_rank   -32         32\r\n",
      "  user_data           0          1\r\n",
      "  point_source_ID 10140      10144\r\n",
      "  gps_time 142356073.975475 142357327.605288\r\n",
      "number of first returns:        17005960\r\n",
      "number of intermediate returns: 7043859\r\n",
      "number of last returns:         17000747\r\n",
      "number of single returns:       4859772\r\n",
      "WARNING: for return 4 real number of points by return (1418871) is different from header entry (1418786).\r\n",
      "WARNING: there are 23021 points with return number 6\r\n",
      "WARNING: there are 1564 points with return number 7\r\n",
      "overview over number of returns of given pulse: 4859772 13544679 11856335 4761909 1028308 128771 11020\r\n",
      "histogram of classification of points:\r\n",
      "        33259991  unclassified (1)\r\n",
      "         2630049  ground (2)\r\n",
      "          300754  noise (7)\r\n",
      " +-> flagged as withheld:  1877840\r\n",
      "\n"
     ]
    }
   ],
   "source": [
    "vendor_tiles = glob.glob(raw_tiles)\n",
    "las.lasinfo(i=vendor_tiles[0], echo=True);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up the workspace "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define data handling directories\n",
    "raw, interim, processed = os.path.join(workdir,'raw'), os.path.join(workdir,'interim'), os.path.join(workdir,'processed')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Get the raw data into our working directory\n",
    "First, move the tiles over to our working directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done moving tiles into working directory.\n",
      "CPU times: user 12 ms, sys: 8 ms, total: 20 ms\n",
      "Wall time: 2min 7s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "las.las2las(i=raw_tiles,\n",
    "            odir=raw,\n",
    "            drop_withheld=True, # drop any points flagged as withheld by vendor\n",
    "            drop_class=(7,18), # drop any points classified as noise by vendor\n",
    "            olaz=True,\n",
    "            cores=num_cores)\n",
    "print('Done moving tiles into working directory.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, create spatial indexes for the input files to allow fast spatial queries (which we'll use for adding buffers)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done adding spatial indexes.\n",
      "CPU times: user 12 ms, sys: 4 ms, total: 16 ms\n",
      "Wall time: 1min 7s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "infiles = os.path.join(raw,'*.laz')\n",
    "\n",
    "las.lasindex(i=infiles, \n",
    "             cores=num_cores)\n",
    "\n",
    "print(\"Done adding spatial indexes.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we'll retile the data to add buffers for avoiding edge effects during processing.\n",
    "\n",
    "**THERE ARE ARGUMENTS IN THE FOLLOWING COMMAND THAT DEPEND UPON THE UNITS OF THE DATA.**\n",
    "\n",
    "If your data are in meters, you should change these parameters, or consider reprojecting the data to a projection that is in feet when you copy the source data into our working directory using `las2las` command at the top of this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done retiling and adding buffers. Created 76 tiles.\n",
      "CPU times: user 28 ms, sys: 16 ms, total: 44 ms\n",
      "Wall time: 3min 26s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "infiles = os.path.join(raw, '*.laz')\n",
    "odir = os.path.join(interim, 'retiled')\n",
    "\n",
    "las.lastile(i=infiles,\n",
    "            tile_size=3500, # assumes units are in feet... if using meters, change to 1000 or comment out\n",
    "            buffer=100, # assumes units are in feet... if using meters, change to 25\n",
    "            flag_as_withheld=True,\n",
    "            olaz=True,\n",
    "            odir=odir,\n",
    "            cores=num_cores)\n",
    "\n",
    "new_tiles = glob.glob(os.path.join(odir,'*.laz'))\n",
    "print('Done retiling and adding buffers. Created {} tiles.'.format(len(new_tiles)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# in case you need to save on storage space\n",
    "# clean out the raw directory now that you have retiled data to work with\n",
    "# shutil.rmtree(raw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Classify points in the lidar point cloud\n",
    "Remove noise and identify high vegetation and buildings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done denoising tiles.\n",
      "CPU times: user 20 ms, sys: 16 ms, total: 36 ms\n",
      "Wall time: 3min 56s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "infiles=os.path.join(interim, 'retiled', '*.laz')\n",
    "odir = os.path.join(interim, 'denoised')\n",
    "\n",
    "las.lasnoise(i=infiles, \n",
    "             remove_noise=True,\n",
    "             odir=odir, \n",
    "             olaz=True, \n",
    "             cores=num_cores) # use parallel processing\n",
    "\n",
    "print('Done denoising tiles.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, calculate the height aboveground for each point for use in classifying them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done calculating height above ground.\n",
      "CPU times: user 120 ms, sys: 116 ms, total: 236 ms\n",
      "Wall time: 6min 57s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "infiles=os.path.join(interim, 'denoised', '*.laz')\n",
    "odir = os.path.join(interim, 'lasheight')\n",
    "\n",
    "las.lasheight(i=infiles,\n",
    "              odir=odir, \n",
    "              olaz=True, \n",
    "              cores=num_cores) # use parallel processing\n",
    "\n",
    "print('Done calculating height above ground.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we'll classify points (that haven't already been classified into meaningful categories) as building or high vegetation that meet certain criteria for 'planarity' or 'ruggedness'. \n",
    "\n",
    "**THERE ARE ARGUMENTS IN THE FOLLOWING COMMAND THAT DEPEND UPON THE UNITS OF THE DATA.**\n",
    "\n",
    "If your data are in meters, you should change these parameters, or consider reprojecting the data to a projection that is in feet when you copy the source data into our working directory using `las2las` command at the top of this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done classifying lidar tiles.\n",
      "CPU times: user 268 ms, sys: 240 ms, total: 508 ms\n",
      "Wall time: 23min 3s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "infiles = os.path.join(interim, 'lasheight', '*.laz')\n",
    "odir = os.path.join(interim, 'classified')\n",
    "\n",
    "las.lasclassify(i=infiles,\n",
    "                odir=odir,\n",
    "                olaz=True,\n",
    "                step=5, # if your data are in meters, the LAStools default is 2.0\n",
    "                planar=0.5, # if your data are in meters, the LAStools default is 0.1\n",
    "                rugged=1, # if your data are in meters, the LAStools default is 0.4\n",
    "                ignore_class=(2,9,10,11,13,14,15,16,17), # ignore points already classified meaningfully\n",
    "                cores=num_cores) # use parallel processing\n",
    "\n",
    "print('Done classifying lidar tiles.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, remove buffer points from the classified tiles and put the clean tiles in the processed folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done trimming classified lidar tiles.\n",
      "CPU times: user 12 ms, sys: 8 ms, total: 20 ms\n",
      "Wall time: 2min 16s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "infiles = os.path.join(interim, 'classified', '*.laz')\n",
    "odir = os.path.join(processed, 'points')\n",
    "\n",
    "las.las2las(i=infiles,\n",
    "            odir=odir,\n",
    "            olaz=True,\n",
    "            drop_withheld=True, # remove points in tile buffers that were flagged as withhled with lastile\n",
    "            set_user_data=0, # remove height aboveground calculated using lasheight\n",
    "            cores=num_cores)\n",
    "\n",
    "print('Done trimming classified lidar tiles.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Produce a shapefile showing the layout of the tiles of points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Produced a shapefile overview of clean tile boundaries.\n",
      "CPU times: user 8 ms, sys: 8 ms, total: 16 ms\n",
      "Wall time: 4.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "infiles = os.path.join(processed, 'points', '*.laz')\n",
    "odir = os.path.join(processed, 'vectors')\n",
    "\n",
    "las.lasboundary(i=infiles,\n",
    "                odir=odir,\n",
    "                o='tiles.shp',\n",
    "                oshp=True,\n",
    "                use_bb=True, # use bounding box of tiles\n",
    "                overview=True,\n",
    "                labels=True,\n",
    "                cores=num_cores) # use parallel processing\n",
    "\n",
    "print('Produced a shapefile overview of clean tile boundaries.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# remove intermediate lidar files if you want to reclaim storage space\n",
    "# shutil.rmtree(os.path.join(interim, 'retiled'))\n",
    "# shutil.rmtree(os.path.join(interim, 'denoised'))\n",
    "# shutil.rmtree(os.path.join(interim, 'lasheight'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Generate a bare earth Digital Elevation Model\n",
    "Generate tiles of the bare earth model. This assumes that there are already ground-classified points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done producing bare earth tiles.\n",
      "CPU times: user 28 ms, sys: 8 ms, total: 36 ms\n",
      "Wall time: 3min 48s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "infiles = os.path.join(interim, 'classified', '*.laz')\n",
    "odir = os.path.join(processed, 'rasters', 'DEM_tiles')\n",
    "\n",
    "las.las2dem(i=infiles,\n",
    "            odir=odir,\n",
    "            obil=True, # create tiles as .bil rasters\n",
    "            keep_class=2, # keep ground-classified returns only\n",
    "            thin_with_grid=1, # use a 1 x 1 resolution for creating the TIN for the DEM\n",
    "            extra_pass=True, # uses two passes over data to execute DEM creation more efficiently\n",
    "            use_tile_bb=True, # remove buffers from tiles\n",
    "            cores=num_cores) \n",
    "\n",
    "print('Done producing bare earth tiles.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Merge the bare earth tiles into a single DEM formatted as a GeoTiff."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done producing merged DEM GeoTiff.\n",
      "CPU times: user 4 ms, sys: 8 ms, total: 12 ms\n",
      "Wall time: 3min 31s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "infiles = os.path.join(processed, 'rasters', 'DEM_tiles', '*.bil')\n",
    "outfile = os.path.join(processed, 'rasters', 'dem.tif')\n",
    "\n",
    "las.lasgrid(i=infiles,\n",
    "            merged=True,\n",
    "            o=outfile)\n",
    "\n",
    "print('Done producing merged DEM GeoTiff.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To create a hillshade layer, we'll first, generate hillshade tiles from the bare earth model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done producing hillshade bare earth tiles.\n",
      "CPU times: user 24 ms, sys: 16 ms, total: 40 ms\n",
      "Wall time: 3min 43s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "infiles = os.path.join(interim, 'classified', '*.laz')\n",
    "odir = os.path.join(processed, 'rasters', 'hillshade_tiles')\n",
    "\n",
    "las.las2dem(i=infiles,\n",
    "            odir=odir,\n",
    "            obil=True, # create tiles as .bil rasters\n",
    "            cores=num_cores,\n",
    "            hillshade=True,\n",
    "            keep_class=2, # keep ground-classified returns only\n",
    "            thin_with_grid=1, # use a 0.5 x 0.5 resolution for creating the TIN for the DEM\n",
    "            extra_pass=True, # uses two passes over data to execute DEM creation more efficiently\n",
    "            use_tile_bb=True) # remove buffers from tiles\n",
    "\n",
    "print('Done producing hillshade bare earth tiles.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now merge the hillshade tiles into a single raster formatted as GeoTiff."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done producing merged hillshade GeoTiff.\n",
      "CPU times: user 4 ms, sys: 12 ms, total: 16 ms\n",
      "Wall time: 2min 18s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "infiles = os.path.join(processed, 'rasters', 'hillshade_tiles', '*.bil')\n",
    "outfile = os.path.join(processed, 'rasters', 'hillshade.tif')\n",
    "\n",
    "las.lasgrid(i=infiles,\n",
    "            merged=True,\n",
    "            o=outfile)\n",
    "\n",
    "print('Done producing merged hillshade GeoTiff.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Identify building footprints\n",
    "First start by building shapefiles showing building boundaries in each buffered tile."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done producing building footprints in buffered tiles.\n",
      "CPU times: user 12 ms, sys: 16 ms, total: 28 ms\n",
      "Wall time: 1min 15s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "infiles = os.path.join(interim, 'classified', '*.laz')\n",
    "odir = os.path.join(interim, 'building_tiles')\n",
    "\n",
    "las.lasboundary(i=infiles,\n",
    "                odir=odir,\n",
    "                keep_class=6, # use only building-classified points\n",
    "                disjoint=True, # compute separate polygons for each building\n",
    "                concavity=3, # map concave boundary if edge length >= 3ft\n",
    "                cores=num_cores)\n",
    "\n",
    "print('Done producing building footprints in buffered tiles.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate shapefiles showing the bounding box of each (unbuffered) tile that we'll use to remove buildings that fall in the buffered area."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done producing boundaries of unbuffered tiles.\n",
      "CPU times: user 0 ns, sys: 8 ms, total: 8 ms\n",
      "Wall time: 5.74 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "infiles = os.path.join(processed, 'points', '*.laz')\n",
    "odir = os.path.join(interim, 'tile_boundaries')\n",
    "\n",
    "las.lasboundary(i=infiles,\n",
    "                odir=odir,\n",
    "                oshp=True,\n",
    "                use_tile_bb=True,\n",
    "                cores=num_cores)\n",
    "\n",
    "print('Done producing boundaries of unbuffered tiles.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each shapefile containing polygons of the building boundaries, we'll use the `clean_tile` function to remove polygons from a tile if their centroid falls in the buffered area of the tile."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done producing building footprints in cleaned (unbuffered) tiles.\n",
      "CPU times: user 11.4 s, sys: 4 ms, total: 11.4 s\n",
      "Wall time: 12.1 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "building_tiles = glob.glob(os.path.join(interim, 'building_tiles', '*.shp'))\n",
    "odir = os.path.join(processed, 'vectors', 'building_tiles')\n",
    "\n",
    "for poly_shp in building_tiles:\n",
    "    fname = os.path.basename(poly_shp)\n",
    "    tile_shp = os.path.join(interim, 'tile_boundaries', fname)\n",
    "    lastools.clean_tile(poly_shp, tile_shp, odir, simp_tol=3, simp_topol=True)\n",
    "\n",
    "print('Done producing building footprints in cleaned (unbuffered) tiles.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Merge the cleaned tiles of building footprints together into a single shapefile. We'll use geopandas to concatenate all the polygons together into a single geodataframe and then write out to a new shapefile."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done merging tiles of building footprints into a single shapefile.\n",
      "CPU times: user 4.29 s, sys: 12 ms, total: 4.3 s\n",
      "Wall time: 4.32 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "building_tiles = glob.glob(os.path.join(processed, 'vectors', 'building_tiles', '*.shp'))\n",
    "# create a list of geodataframes containing the tiles of building footprints\n",
    "gdflist = [gpd.read_file(tile) for tile in building_tiles]\n",
    "# merge them all together\n",
    "merged = gpd.GeoDataFrame(pd.concat(gdflist, ignore_index=True))\n",
    "# using pandas concat caused us to lose projection information, so let's add that back in\n",
    "merged.crs = gdflist[0].crs\n",
    "# and write the merged data to a new shapefile\n",
    "merged.to_file(os.path.join(processed,'vectors','buildings.shp'))\n",
    "\n",
    "print('Done merging tiles of building footprints into a single shapefile.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Create a Canopy Height Model\n",
    "Although `pyFIRS` includes a method for generating pit free Canopy Height Models following the approach laid out by Martin Isenburg in this [blog post](https://rapidlasso.com/2014/11/04/rasterizing-perfect-canopy-height-models-from-lidar/)--which you can call using `las.pitfree(...)`--this approach is very time-consuming and doesn't (yet) scale well if you're trying to process a whole lidar acquisition. \n",
    "\n",
    "We'll first normalize the point clouds to have z values represent height above ground."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done normalizing tiles.\n",
      "CPU times: user 52 ms, sys: 24 ms, total: 76 ms\n",
      "Wall time: 6min 58s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "infiles = os.path.join(interim, 'classified', '*.laz')\n",
    "odir = os.path.join(interim, 'normalized_chm')\n",
    "\n",
    "las.lasheight(i=infiles,\n",
    "              odir=odir,\n",
    "              olaz=True,\n",
    "              replace_z=True,\n",
    "              keep_class=(1,2,3,4,5), # keep only ground, unclassified, and vegetation points\n",
    "              cores=num_cores)\n",
    "\n",
    "print('Done normalizing tiles.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll borrow from the pit free canopy modeling workflow logic to 'splat' the lidar points before processing them into a surface model.\n",
    "\n",
    "**THERE ARE ARGUMENTS IN THE FOLLOWING COMMAND THAT DEPEND UPON THE UNITS OF THE DATA.**\n",
    "\n",
    "If your data are in meters, you should change these parameters, or consider reprojecting the data to a projection that is in feet when you copy the source data into our working directory using `las2las` command at the top of this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done splatting.\n",
      "CPU times: user 44 ms, sys: 12 ms, total: 56 ms\n",
      "Wall time: 3min 41s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "infiles = os.path.join(interim, 'normalized_chm', '*.laz')\n",
    "odir = os.path.join(interim, 'splatted_chm')\n",
    "\n",
    "las.lasthin(i=infiles,\n",
    "            odir=odir,\n",
    "            olaz=True,\n",
    "            highest=True,\n",
    "            subcircle=0.3, # if your data are in meters, the LAStools default is 0.1\n",
    "            cores=num_cores)\n",
    "\n",
    "print('Done splatting.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll use a median filter over the highest hits to generate a Canopy Height Model using the `canopymodel` command line tool from the FUSION software package. Because FUSION doesn't offer multi-core options for it's command line tools, we'll use `ipyparallel` to process the jobs asynchronously."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/pyFIRS/lib/python3.6/site-packages/ipyparallel/client/client.py:459: RuntimeWarning: \n",
      "            Controller appears to be listening on localhost, but not on this machine.\n",
      "            If this is true, you should specify Client(...,sshserver='you@Ford')\n",
      "            or instruct your controller to listen on an external IP.\n",
      "  RuntimeWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31]\n",
      "importing subprocess on engine(s)\n",
      "importing os on engine(s)\n",
      "importing pyFIRS on engine(s)\n",
      "importing fusion from pyFIRS.utils on engine(s)\n"
     ]
    },
    {
     "ename": "CompositeError",
     "evalue": "one or more exceptions from call to method: remote_import\n[0:apply]: ModuleNotFoundError: No module named 'pyFIRS.utils'\n[1:apply]: ModuleNotFoundError: No module named 'pyFIRS.utils'\n[2:apply]: ModuleNotFoundError: No module named 'pyFIRS.utils'\n[3:apply]: ModuleNotFoundError: No module named 'pyFIRS.utils'\n.... 28 more exceptions ...",
     "output_type": "error",
     "traceback": [
      "[0:apply]: ",
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)\u001b[0;32m<string>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/pyFIRS/lib/python3.6/site-packages/ipyparallel/client/view.py\u001b[0m in \u001b[0;36mremote_import\u001b[0;34m(name, fromlist, level)\u001b[0m",
      "\u001b[1;32m    427\u001b[0m             \u001b[0;32mimport\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m",
      "\u001b[1;32m    428\u001b[0m             \u001b[0muser_ns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mglobals\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m",
      "\u001b[0;32m--> 429\u001b[0;31m             \u001b[0mmod\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m__import__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfromlist\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfromlist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlevel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m",
      "\u001b[0m\u001b[1;32m    430\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mfromlist\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m",
      "\u001b[1;32m    431\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfromlist\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'pyFIRS.utils'",
      "",
      "[1:apply]: ",
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)\u001b[0;32m<string>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/pyFIRS/lib/python3.6/site-packages/ipyparallel/client/view.py\u001b[0m in \u001b[0;36mremote_import\u001b[0;34m(name, fromlist, level)\u001b[0m",
      "\u001b[1;32m    427\u001b[0m             \u001b[0;32mimport\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m",
      "\u001b[1;32m    428\u001b[0m             \u001b[0muser_ns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mglobals\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m",
      "\u001b[0;32m--> 429\u001b[0;31m             \u001b[0mmod\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m__import__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfromlist\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfromlist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlevel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m",
      "\u001b[0m\u001b[1;32m    430\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mfromlist\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m",
      "\u001b[1;32m    431\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfromlist\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'pyFIRS.utils'",
      "",
      "[2:apply]: ",
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)\u001b[0;32m<string>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/pyFIRS/lib/python3.6/site-packages/ipyparallel/client/view.py\u001b[0m in \u001b[0;36mremote_import\u001b[0;34m(name, fromlist, level)\u001b[0m",
      "\u001b[1;32m    427\u001b[0m             \u001b[0;32mimport\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m",
      "\u001b[1;32m    428\u001b[0m             \u001b[0muser_ns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mglobals\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m",
      "\u001b[0;32m--> 429\u001b[0;31m             \u001b[0mmod\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m__import__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfromlist\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfromlist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlevel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m",
      "\u001b[0m\u001b[1;32m    430\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mfromlist\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m",
      "\u001b[1;32m    431\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfromlist\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'pyFIRS.utils'",
      "",
      "[3:apply]: ",
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)\u001b[0;32m<string>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/pyFIRS/lib/python3.6/site-packages/ipyparallel/client/view.py\u001b[0m in \u001b[0;36mremote_import\u001b[0;34m(name, fromlist, level)\u001b[0m",
      "\u001b[1;32m    427\u001b[0m             \u001b[0;32mimport\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m",
      "\u001b[1;32m    428\u001b[0m             \u001b[0muser_ns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mglobals\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m",
      "\u001b[0;32m--> 429\u001b[0;31m             \u001b[0mmod\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m__import__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfromlist\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfromlist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlevel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m",
      "\u001b[0m\u001b[1;32m    430\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mfromlist\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m",
      "\u001b[1;32m    431\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfromlist\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'pyFIRS.utils'",
      "",
      "... 28 more exceptions ..."
     ]
    }
   ],
   "source": [
    "rc = ipp.Client() # a client for controlling the workers\n",
    "print(rc.ids) # how many workers?\n",
    "\n",
    "dv = rc[:] # create a direct view of the workers\n",
    "v = rc.load_balanced_view() # create a load-balanced view of the workers\n",
    "\n",
    "# import the relevant packages to all the workers\n",
    "with dv.sync_imports():\n",
    "    import subprocess\n",
    "    import os\n",
    "    import pyFIRS\n",
    "    from pyFIRS.utils import fusion\n",
    "    \n",
    "#%px fus = fusion.useFUSION('/storage/lidar/FUSION') # let the workers know how to call FUSION tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_chm(infile):\n",
    "    \"A wrapper function to execute canopymodel on an input file using canned parameters.\"\n",
    "    basename = os.path.basename(infile).split('.')[0]\n",
    "    odir = '/storage/lidar/Swinomish_Lidar_2016/interim/chm'\n",
    "    outfile = os.path.join(odir,basename + '.dtm')\n",
    "    \n",
    "    return fus.canopymodel(surfacefile=outfile,\n",
    "                           cellsize=1.64042, # half-meter resolution\n",
    "                           xyunits='F',\n",
    "                           zunits='F',\n",
    "                           coordsys=2,\n",
    "                           zone=0,\n",
    "                           horizdatum=2,\n",
    "                           vertdatum=2,\n",
    "                           datafiles=infile,\n",
    "                           median=3,\n",
    "                           peaks=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "fus.canopymodel?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pyFIRS]",
   "language": "python",
   "name": "conda-env-pyFIRS-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
