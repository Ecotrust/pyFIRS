{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/storage/anaconda3/envs/pyFIRS/lib/python3.6/site-packages/tqdm/autonotebook/__init__.py:14: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  \" (e.g. in jupyter console)\", TqdmExperimentalWarning)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "import glob\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import dask\n",
    "from dask.distributed import Client, progress, LocalCluster\n",
    "from pyFIRS.wrappers import lastools, fusion\n",
    "from pyFIRS.utils import clean_dir, clean_buffer_polys, clip_tile_from_shp, convert_project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting up parallel computing using `dask.distributed`\n",
    "`LAStools` offers native multi-core processing as an optional argument (`cores`) supplied to its command-line tools. `FUSION` command line tools do not. To enable parallel processing of `FUSION` commands, we'll use `dask.distributed` to schedule the processing of tiles in asynchronous parallel batches. This approach also offers us the ability to track progress using a progress bar.\n",
    "\n",
    "You'll first need to launch a parallel computing cluster. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster=LocalCluster(scheduler_port=7001, diagnostics_port=7002)\n",
    "c = Client(cluster)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point, you should also be able to view an interactive dashboard on port 7002. If you're executing this on a remote server, you'll need to set up port forward so you can view the dashboard on your local machine's browser. Once you've done that, or if you're processing on your own machine, you can view the dashboard at [http://localhost:7002/status](http://localhost:7002/status)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Enough already, let's get to work with some lidar data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll define where we can find the binary executables for LAStools and FUSION command line tools."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "las = lastools.useLAStools('/storage/lidar/LAStools/bin')\n",
    "fus = fusion.useFUSION('/storage/lidar/FUSION/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll create working directories for raw (imported with modest clean-up from source files), interim, and processed data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# where the imported lidar data is currently stored\n",
    "workdir = os.path.abspath('/storage/lidar/olc_metro_2014')\n",
    "\n",
    "# the coordinate reference system we'll be working with\n",
    "target_epsg = 26910 # utm 10 N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define data handling directories\n",
    "raw, interim, processed = os.path.join(workdir,'raw'), os.path.join(workdir,'interim'), os.path.join(workdir,'processed')\n",
    "layers = os.path.join(interim, 'layers')\n",
    "\n",
    "num_cores = len(c.ncores()) # identify how many workers we have\n",
    "\n",
    "# push our working directories and wrapper classes to the workers on the cluster as well\n",
    "c.scatter([src_dir, raw, interim, processed, layers, las, fus, target_epsg, num_cores], broadcast=True);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Retile the data to add buffers for avoiding edge effects during processing.\n",
    "\n",
    "In practice, executing the `lastile` command on individual tiles in parallel is likely to corrupt your output files. I suspect this is because the dynamic re-tiling of input files means that many output tiles are likely to require inputs from multiple input files, and that parallel processing outside of LAStools may result in collisions writing data from multiple inputs to these output tiles. So, for this case, we'll let `lastile` handle the parallelism under the hood. We won't have a progress bar, but this shouldn't take more than 5-10 minutes per ~100 tiles (with vendor tile size ~1000x1000m with 4-8 pts/m2).\n",
    "\n",
    "**THERE ARE ARGUMENTS IN THE FOLLOWING COMMAND THAT DEPEND UPON THE UNITS OF THE DATA.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "tile_proc = las.lastile(i=os.path.join(raw, '*.laz'),\n",
    "                        tile_size=1000, # in units of lidar data\n",
    "                        buffer=25, # assumes units are in meters\n",
    "                        flag_as_withheld=True, # flag buffer points as \"withheld\", enables handling with other LAStools\n",
    "                        extra_pass=True, # if outputting to LAZ format, can help avoid memory limits\n",
    "                        full_bb=True,\n",
    "                        olas=True,\n",
    "                        odir=os.path.join(interim, 'retiled'),\n",
    "                        cores=num_cores);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to confirm that the point cloud files you've retiled haven't been corrupted (i.e., they still match valid LAS format specifications), you can use the following function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dask.delayed\n",
    "def validate(infile): # the function we'll map to a list of inputs\n",
    "    odir, basename = os.path.split(infile)\n",
    "    tile_id = basename.split('.')[0]\n",
    "    outfile = os.path.join(odir, tile_id + \".xml\")\n",
    "    if os.path.exists(outfile):\n",
    "        pass\n",
    "    else:\n",
    "        proc = las.lasvalidate(i=infile,\n",
    "                               o=outfile)\n",
    "    return tile_id\n",
    "\n",
    "infiles = glob.glob(os.path.join(interim, 'retiled', '*.las'))\n",
    "val_results = c.persist([validate(file) for file in infiles])\n",
    "progress(val_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check to see if any of the files failed validation\n",
    "# if a file shows up here, inspect the .xml file and see whether the corruption is a concern\n",
    "# for example, in this acquisition, the corruption issue is GPS time, which we aren't worried about\n",
    "corrupted = [val_results[i].result().args[3] for i in range(len(val_results)) if 'fail' in val_results[i].result().stderr.decode()]\n",
    "corrupted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Classify points in the lidar point cloud"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the original tiles delivered by the vendor included overlapping edges, our retiling may result in duplicated points in the new tiles from overlapping edges of vendor-provided input tiles. In the next step, we will ensure that only one point with unique (X,Y,Z) coordinates are retained in the point cloud data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dask.delayed\n",
    "def remove_dupes(tile_id):\n",
    "    proc = las.lasduplicate(i=os.path.join(interim, 'retiled', tile_id + '.las'),\n",
    "                            unique_xyz=True,\n",
    "                            olay=True,\n",
    "                            olaydir=layers,\n",
    "                            odir=layers)\n",
    "    return tile_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll remove points that are isolated as likely noise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dask.delayed\n",
    "def denoise(tile_id):    \n",
    "    proc = las.lasnoise(i=os.path.join(interim, 'retiled', tile_id + '.las'),\n",
    "                        remove_noise=True,\n",
    "                        ilaydir=layers,\n",
    "                        olay=True,\n",
    "                        olaydir=layers)\n",
    "    return tile_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, calculate the height aboveground for each point for use in classifying them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dask.delayed\n",
    "def hag(tile_id):\n",
    "    proc = las.lasheight(i=os.path.join(interim, 'retiled', tile_id + '.las'),\n",
    "                         ilaydir=layers,\n",
    "                         olay=True,\n",
    "                         olaydir=layers)\n",
    "    return tile_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we'll classify points as building or high vegetation that meet certain criteria for 'planarity' or 'ruggedness'. \n",
    "\n",
    "**THERE ARE ARGUMENTS IN THE FOLLOWING COMMAND THAT DEPEND UPON THE UNITS OF THE DATA.**\n",
    "\n",
    "If your data are in meters, you should change these parameters, or consider reprojecting the data to a projection that is in feet when you copy the source data into our working directory using the `import_tiles` / `las2las` command at the top of this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dask.delayed\n",
    "def classify(tile_id):\n",
    "    proc = las.lasclassify(i=os.path.join(interim, 'retiled', tile_id + '.las'),\n",
    "                           ilaydir=layers,\n",
    "                           step=2.0, # if your data are in meters\n",
    "                           planar=0.1, # if your data are in meters\n",
    "                           rugged=0.4, # if your data are in meters\n",
    "                           olas=True,\n",
    "                           odir=os.path.join(interim, 'classified'))\n",
    "    return tile_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll now remove the points in the buffered area of each tile and put the clean tiles in the processed folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dask.delayed\n",
    "def dropwithheld(tile_id):\n",
    "    outfile = os.path.join(processed, 'points', tile_id + '.las')\n",
    "    if not os.path.exists(outfile):\n",
    "        proc = las.las2las(i=os.path.join(interim, 'classified', tile_id + '.las'),\n",
    "                           drop_withheld=True, # remove points flagged as withhled\n",
    "                           set_user_data=0, # remove height aboveground calculated using lasheight\n",
    "                           olaz=True, # compress outputs to LAZ format\n",
    "                           odir=os.path.join(processed, 'points'))\n",
    "    else:\n",
    "        pass\n",
    "    \n",
    "    return tile_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll produce a shapefile showing the layout of the tiles as a single shapefile. This is a single process that takes a few seconds to run, so no need to distribute it using `dask`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dask.delayed\n",
    "def tiles_overview(*args, **kwargs):\n",
    "    odir = os.path.join(processed, 'vectors')\n",
    "    \n",
    "    if os.path.exists(os.path.join(processed, 'vectors', 'tiles.shp')):\n",
    "        pass\n",
    "    else:\n",
    "        proc = las.lasboundary(i=os.path.join(processed, 'points', '*.laz'),\n",
    "                               use_bb=True, # use bounding box of tiles\n",
    "                               overview=True,\n",
    "                               labels=True,\n",
    "                               cores=num_cores, # use parallel processing\n",
    "                               oshp=True,\n",
    "                               o=os.path.join(processed, 'vectors', 'tiles.shp'))\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Generate a bare earth Digital Elevation Model\n",
    "Generate tiles of the bare earth model. This assumes that there are already ground-classified points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dask.delayed\n",
    "def make_dem(tile_id):\n",
    "    proc = las.las2dem(i=os.path.join(interim, 'classified', tile_id + '.las'),\n",
    "                       odir=os.path.join(processed, 'rasters', 'DEM_tiles'),\n",
    "                       otif=True, # create tiles as GeoTiff rasters\n",
    "                       keep_class=2, # keep ground-classified returns only\n",
    "                       step=1, # resolution of output raster, in units of lidar data\n",
    "                       thin_with_grid=1, # use a 1 x 1 resolution for creating the TIN for the DEM\n",
    "                       extra_pass=True, # uses two passes over data to execute DEM creation more efficiently\n",
    "                       use_tile_bb=True) # remove buffers from tiles\n",
    "    return tile_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dask.delayed\n",
    "def add_crs_dem(tile_id):\n",
    "    basename = tile_id + '.tif'\n",
    "    infile = os.path.join(processed, 'rasters', 'DEM_tiles', basename)\n",
    "    \n",
    "    proc = subprocess.run(['rio', 'edit-info', '--crs', 'EPSG:{}'.format(target_epsg), infile],\n",
    "                   stderr=subprocess.PIPE, stdout=subprocess.PIPE)\n",
    "    \n",
    "    return tile_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To create a hillshade layer, we'll first, generate hillshade tiles from the bare earth model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dask.delayed\n",
    "def make_hillshade(tile_id):\n",
    "    proc = las.las2dem(i=os.path.join(interim, 'classified', tile_id + '.las'),\n",
    "                       odir=os.path.join(processed, 'rasters', 'hillshade_tiles'),\n",
    "                       otif=True, # create tiles as GeoTiffs\n",
    "                       hillshade=True,\n",
    "                       keep_class=2, # keep ground-classified returns only\n",
    "                       step=1, # resolution of output raster, in units of lidar data\n",
    "                       thin_with_grid=1, # use a 0.5 x 0.5 resolution for creating the TIN for the DEM\n",
    "                       extra_pass=True, # uses two passes over data to execute DEM creation more efficiently\n",
    "                       use_tile_bb=True) # remove buffers from tiles\n",
    "    return tile_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dask.delayed\n",
    "def add_crs_hill(tile_id):\n",
    "    basename = tile_id + '.tif'\n",
    "    infile = os.path.join(processed, 'rasters', 'hillshade_tiles', basename)\n",
    "    \n",
    "    proc = subprocess.run(['rio', 'edit-info', '--crs', 'EPSG:{}'.format(target_epsg), infile],\n",
    "                          stderr=subprocess.PIPE, stdout=subprocess.PIPE)\n",
    "    \n",
    "    return tile_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Identify building footprints\n",
    "First start by building shapefiles showing building boundaries in each buffered tile."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dask.delayed\n",
    "def bldg_tiles(tile_id):\n",
    "    proc = las.lasboundary(i=os.path.join(interim, \"retiled\", tile_id + '.las'),\n",
    "                           ilaydir=layers,\n",
    "                           keep_class=6, # use only building-classified points\n",
    "                           disjoint=True, # compute separate polygons for each building\n",
    "                           concavity=1, # map concave boundary if edge length >= 1m\n",
    "                           oshp=True,\n",
    "                           odir=os.path.join(interim, 'building_tiles'))\n",
    "    return tile_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate shapefiles showing the bounding box of each (unbuffered) tile that we'll use to remove buildings that fall in the buffered area."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dask.delayed\n",
    "def bbox_shp(tile_id):\n",
    "    basename = tile_id + '.laz'\n",
    "    infile = os.path.join(processed, 'points', basename)\n",
    "    odir = os.path.join(interim, 'tile_boundaries')\n",
    "    \n",
    "    if os.path.exists(os.path.join(odir, basename)):\n",
    "        pass\n",
    "    else:\n",
    "        proc = las.lasboundary(i=infile,\n",
    "                               odir=odir,\n",
    "                               oshp=True,\n",
    "                               use_tile_bb=True)\n",
    "    return tile_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each shapefile containing polygons of the building boundaries, we'll use the `clean_buffer_polys` function from `pyFIRS.utils` to remove polygons from a tile if their centroid falls in the buffered area of the tile."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dask.delayed\n",
    "def clean_bldgs(tile_id, *args):\n",
    "    if type(tile_id) == list:\n",
    "        tile_id = tile_id[0]\n",
    "    basename = tile_id + '.shp'\n",
    "    infile = os.path.join(interim, 'building_tiles', basename)\n",
    "    tile_shp = os.path.join(interim, 'tile_boundaries', basename)\n",
    "    odir = os.path.join(processed, 'vectors', 'building_tiles')\n",
    "    \n",
    "    if os.path.exists(os.path.join(odir, basename)):\n",
    "        pass\n",
    "    else:\n",
    "        clean_buffer_polys(infile,\n",
    "                           tile_shp,\n",
    "                           odir=odir,\n",
    "                           simp_tol=3,\n",
    "                           simp_topol=True)\n",
    "    return tile_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Create a Canopy Height Model\n",
    "We're going to switch use a FUSION command line tool to generate a Canopy Height Models (CHMs). \n",
    "\n",
    "### Using FUSION's `canopymodel` to generate CHMs\n",
    "`FUSION` wants to have ground models formatted as .dtm files, for CHM development and for estimating other canopy metrics. Let's generate these ground models first using a 1-meter x-y resolution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dask.delayed\n",
    "def groundDTMs(tile_id):\n",
    "    infile = os.path.join(interim, 'classified', tile_id + '.las')\n",
    "    odir = os.path.join(interim, 'dtm_ground_tiles')\n",
    "    outfile = os.path.join(odir, tile_id + '.dtm')\n",
    "    \n",
    "    if os.path.exists(os.path.join(odir, tile_id + '.laz')):\n",
    "        pass\n",
    "    else:\n",
    "        proc = fus.gridsurfacecreate(surfacefile=outfile,\n",
    "                                     cellsize=1,\n",
    "                                     xyunits='M',\n",
    "                                     zunits='M',\n",
    "                                     coordsys=1, # in UTM\n",
    "                                     zone='10N',\n",
    "                                     horizdatum=2, # NAD83\n",
    "                                     vertdatum=2, # NAVD88\n",
    "                                     datafile=infile,\n",
    "                                     las_class=2, # keep only ground-classified points\n",
    "                                     odir=odir) # will make sure output directory is created if doesn't already exist\n",
    "    return tile_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dask.delayed\n",
    "def canopymodel(tile_id):\n",
    "    infile = os.path.join(interim, 'classified', tile_id + '.las')\n",
    "    odir = os.path.join(interim, 'chm_tiles')\n",
    "    outfile = os.path.join(odir, tile_id + '.dtm')\n",
    "    \n",
    "    if os.path.exists(os.path.join(odir, tile_id + '.dtm')):\n",
    "        pass\n",
    "    else:\n",
    "        proc = fus.canopymodel(surfacefile=outfile,\n",
    "                               cellsize=0.5, # in meters\n",
    "                               xyunits='M',\n",
    "                               zunits='M',\n",
    "                               coordsys=1, # in UTM\n",
    "                               zone='10N', # not in UTM\n",
    "                               horizdatum=2, # NAD83\n",
    "                               vertdatum=2, # NAVD88\n",
    "                               datafiles=infile,\n",
    "                               ground=os.path.join(interim, 'dtm_ground_tiles', tile_id + '.dtm'),\n",
    "                               median=3, # median smoothing in 3x3 kernel\n",
    "                               las_class=(1,2,5), # keep only ground, unclassified, and high veg points\n",
    "                               asc=True, # also output in ascii format\n",
    "                               odir=odir) # will make sure output directory is created if doesn't already exist\n",
    "    return tile_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert the ascii files that `canopymodel` generated into GeoTiffs, specifying their projection. Then cleanup the files `canopymodel` generated that we don't care about."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dask.delayed\n",
    "def asc2tif(tile_id):\n",
    "    infile = os.path.join(interim, 'chm_tiles', tile_id + '.asc')\n",
    "    \n",
    "    if os.path.exists(os.path.join(interim, 'chm_tiles', tile_id + '.tif')):\n",
    "        pass\n",
    "    else:\n",
    "        convert_project(infile, '.tif', 'EPSG:{}'.format(target_epsg))\n",
    "    \n",
    "    return tile_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clip the canopy height model tiles to remove overlapping areas that were from tile buffering to avoid edge effects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dask.delayed\n",
    "def clip(tile_id, *args):\n",
    "    if type(tile_id) == list:\n",
    "        tile_id = tile_id[0]\n",
    "        \n",
    "    infile = os.path.join(interim, 'chm_tiles', tile_id + '.tif')\n",
    "    in_shp = os.path.join(interim, 'tile_boundaries', tile_id + '.shp')\n",
    "    odir = os.path.join(processed, 'rasters', 'chm_tiles')\n",
    "    \n",
    "    if os.path.exists(os.path.join(odir, tile_id + '.tif')):\n",
    "        pass\n",
    "    else:\n",
    "        clip_tile_from_shp(infile, in_shp, odir)\n",
    "    \n",
    "    return tile_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dask.delayed\n",
    "def tile_done(tile_id, *args, **kwargs):\n",
    "    return tile_id\n",
    "\n",
    "@dask.delayed\n",
    "def tiles_done(*args, **kwargs):\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hand-build the computational graph\n",
    "Define the recipe for computations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tile_ids = [os.path.basename(file).split('.')[0] for file in glob.glob(os.path.join(interim, 'retiled', '*.las'))]\n",
    "print('Found {:,d} tiles to process'.format(len(tile_ids)))\n",
    "print(tile_ids[0:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The computational pipeline, including dependencies of each step in the pipeline, can be specified using a dictionary. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dsk = {}\n",
    "for tile in tile_ids:\n",
    "    dsk['dedupe-{}'.format(tile)]=(remove_dupes, tile)\n",
    "    dsk['denoise-{}'.format(tile)]=(denoise, 'dedupe-{}'.format(tile))\n",
    "    dsk['normalize-{}'.format(tile)]=(hag, 'denoise-{}'.format(tile))\n",
    "    dsk['classify-{}'.format(tile)]=(classify, 'normalize-{}'.format(tile))\n",
    "    dsk['drop-{}'.format(tile)]=(dropwithheld, 'classify-{}'.format(tile))\n",
    "    dsk['bbox-{}'.format(tile)]=(bbox_shp, 'drop-{}'.format(tile))\n",
    "    dsk['dem-{}'.format(tile)]=(make_dem, 'classify-{}'.format(tile))\n",
    "    dsk['prj_dem-{}'.format(tile)]=(add_crs_dem, 'dem-{}'.format(tile))\n",
    "    dsk['hill-{}'.format(tile)]=(make_hillshade, 'classify-{}'.format(tile))\n",
    "    dsk['prj_hill-{}'.format(tile)]=(add_crs_hill, 'hill-{}'.format(tile))\n",
    "    dsk['bldgs_buff-{}'.format(tile)]=(bldg_tiles, 'classify-{}'.format(tile))\n",
    "    dsk['bldgs_clean-{}'.format(tile)]=(clean_bldgs, ['bldgs_buff-{}'.format(tile), 'bbox-{}'.format(tile)])\n",
    "    dsk['ground_dtm-{}'.format(tile)]=(groundDTMs, 'classify-{}'.format(tile))\n",
    "    dsk['canopy-{}'.format(tile)]=(canopymodel, 'ground_dtm-{}'.format(tile))\n",
    "    dsk['canopy_tif-{}'.format(tile)] = (asc2tif, 'canopy-{}'.format(tile))\n",
    "    dsk['canopy_clip-{}'.format(tile)]=(clip, ['canopy_tif-{}'.format(tile), 'bbox-{}'.format(tile)])\n",
    "    dsk['tile_done-{}'.format(tile)]=(tile_done, ['denoise-{}'.format(tile), \n",
    "                                                  'normalize-{}'.format(tile), \n",
    "                                                  'classify-{}'.format(tile), \n",
    "                                                  'drop-{}'.format(tile),\n",
    "                                                  'bbox-{}'.format(tile),\n",
    "                                                  'dem-{}'.format(tile),\n",
    "                                                  'prj_dem-{}'.format(tile),\n",
    "                                                  'hill-{}'.format(tile),\n",
    "                                                  'prj_hill-{}'.format(tile),\n",
    "                                                  'bldgs_buff-{}'.format(tile),\n",
    "                                                  'bldgs_clean-{}'.format(tile),\n",
    "                                                  'ground_dtm-{}'.format(tile),\n",
    "                                                  'canopy-{}'.format(tile),\n",
    "                                                  'canopy_tif-{}'.format(tile),\n",
    "                                                  'canopy_clip-{}'.format(tile)])\n",
    "    \n",
    "dsk['tiles_done'] = (tiles_done, ['tile_done-{}'.format(tile) for tile in tile_ids])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processing_steps = ['dedupe', 'denoise', 'normalize', 'classify', 'drop', 'bbox', \n",
    "                    'dem', 'prj_dem', 'hill', 'prj_hill', 'bldgs_buff', \n",
    "                    'bldgs_clean', 'ground_dtm', 'canopy', 'canopy_tif', 'canopy_clip']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "example_tile_graph = c.get(dsk, 'tile_done-{}'.format(tile_ids[0]))\n",
    "example_tile_graph.visualize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example_tile_results = c.compute(example_tile_graph) # this might take a while...\n",
    "# progress(example_tile_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get Dask to determine how to get to the last state of the tile-processing pipeline, building a computational graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tiles_graph = c.get(dsk, 'tiles_done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tiles_results = c.compute(tiles_graph) # this might take a while...\n",
    "progress(tiles_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# get rid of the .tfw and .kml files that LAStools generates\n",
    "clean_dir(os.path.join(processed, 'rasters', 'DEM_tiles'), ['.tfw', '.kml'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# get rid of the .tfw and .kml files that LAStools generates\n",
    "clean_dir(os.path.join(processed, 'rasters', 'hillshade_tiles'), ['.tfw', '.kml'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# get rid of the .asc and .dtm files that FUSION generates\n",
    "clean_dir(os.path.join(interim, 'chm_tiles'), ['.asc', '.dtm'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Merge tiled derivative outputs together\n",
    "Merge all the tiled GeoTiffs and Shapefiles into single overview files."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Merge the bare earth tiles into a single GeoTiff."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dask.delayed\n",
    "def merge_dem(*args, **kwargs):\n",
    "    infiles = os.path.join(processed, 'rasters', 'DEM_tiles', '*.tif')\n",
    "    outfile = os.path.join(processed, 'rasters', 'dem.tif')\n",
    "    \n",
    "    if os.path.exists(outfile):\n",
    "        return\n",
    "    else:\n",
    "        return subprocess.run(['rio', 'merge', *glob.glob(infiles), outfile, '--co', 'compress=LZW',\n",
    "                              '--co', 'tiled=true', '--co', 'blockxsize=256', '--co', 'blockysize=256'],\n",
    "                              stderr=subprocess.PIPE, stdout=subprocess.PIPE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now merge the hillshade tiles into a single raster formatted as GeoTiff."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dask.delayed\n",
    "def merge_hillshade(*args, **kwargs):\n",
    "    infiles = os.path.join(processed, 'rasters', 'hillshade_tiles', '*.tif')\n",
    "    outfile = os.path.join(processed, 'rasters', 'hillshade.tif')\n",
    "\n",
    "    if os.path.exists(outfile):\n",
    "        return\n",
    "    else:\n",
    "        return subprocess.run(['rio', 'merge', *glob.glob(infiles), outfile, '--co', 'compress=LZW',\n",
    "                              '--co', 'tiled=true', '--co', 'blockxsize=256', '--co', 'blockysize=256'],\n",
    "                              stderr=subprocess.PIPE, stdout=subprocess.PIPE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Merge the trimmed canopy height model tiles into a single raster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dask.delayed\n",
    "def merge_chm(*args, **kwargs):\n",
    "    infiles = os.path.join(processed, 'rasters', 'chm_tiles', '*.tif')\n",
    "    outfile = os.path.join(processed, 'rasters', 'chm.tif')\n",
    "    \n",
    "    if os.path.exists(outfile):\n",
    "        pass\n",
    "    else:\n",
    "        proc = subprocess.run(['rio', 'merge', *glob.glob(infiles), outfile, '--co', 'compress=LZW',\n",
    "                              '--co', 'tiled=true', '--co', 'blockxsize=256', '--co', 'blockysize=256'],\n",
    "                              stderr=subprocess.PIPE, stdout=subprocess.PIPE)\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Merge the cleaned tiles of building footprints together into a single shapefile. We'll use `geopandas` to concatenate all the polygons together into a single geodataframe and then write out to a new shapefile."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dask.delayed\n",
    "def merge_bldgs(*args, **kwargs):\n",
    "    \n",
    "    if os.path.exists(os.path.join(processed,'vectors','buildings.shp')):\n",
    "        pass\n",
    "    else:\n",
    "        building_tiles = glob.glob(os.path.join(processed, 'vectors', 'building_tiles', '*.shp'))\n",
    "        # create a list of geodataframes containing the tiles of building footprints\n",
    "        gdflist = [gpd.read_file(tile) for tile in building_tiles]\n",
    "        # merge them all together\n",
    "        merged = gpd.GeoDataFrame(pd.concat(gdflist, ignore_index=True))\n",
    "        # using pandas' concat caused us to lose projection information, so let's add that back in\n",
    "        merged.crs = gdflist[0].crs\n",
    "        # and write the merged data to a new shapefile\n",
    "        merged.to_file(os.path.join(processed,'vectors','buildings.shp'))\n",
    "\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A single state that will depend upon the completion of the merged rasters and vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dask.delayed\n",
    "def merge_done(*args, **kwargs):\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# building the computation receipe\n",
    "merge_dsk = {}\n",
    "merge_dsk['tiles_over'] = (tiles_overview, ['tiles_done'])\n",
    "merge_dsk['merge_bldgs'] = (merge_bldgs, ['tiles_done'])\n",
    "merge_dsk['merge_hill'] = (merge_hillshade, ['tiles_done'])\n",
    "merge_dsk['merge_dem'] = (merge_dem, ['tiles_done'])\n",
    "merge_dsk['merge_chm'] = (merge_chm, ['tiles_done'])\n",
    "merge_dsk['merge_done']=(merge_done, ['tiles_over', 'merge_bldgs', 'merge_hill', 'merge_dem', 'merge_chm'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merge_graph = c.get(merge_dsk, 'merge_done') # build the computation graph\n",
    "merge_results = c.compute(merge_graph) # this might take a while...\n",
    "progress(merge_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# c.cancel(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c.close()\n",
    "cluster.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pyFIRS]",
   "language": "python",
   "name": "conda-env-pyFIRS-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
