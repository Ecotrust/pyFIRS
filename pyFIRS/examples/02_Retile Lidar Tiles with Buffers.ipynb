{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import multiprocessing\n",
    "from pyFIRS.wrappers import lastools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "WORKDIR = '/storage/lidar/hoh-river_2012/'\n",
    "RAW = os.path.join(WORKDIR, 'raw')\n",
    "INTERIM = os.path.join(WORKDIR, 'interim')\n",
    "NUM_CORES = 32\n",
    "POINT_CAPACITY = 50000000 # maximum number of points to allow in each tile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "las = lastools.useLAStools('/storage/lidar/LAStools/bin')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retile the data to add buffers for avoiding edge effects during processing.\n",
    "\n",
    "In practice, executing the `lastile` command on individual tiles in parallel is likely to corrupt your output files. I suspect this is because the dynamic re-tiling of input files means that many output tiles are likely to require inputs from multiple input files, and that parallel processing outside of LAStools may result in collisions writing data from multiple inputs to these output tiles. So, for this case, we'll let `lastile` handle the parallelism under the hood. We won't have a progress bar, but this shouldn't take more than 5-10 minutes per ~100 tiles (with vendor tile size ~1000x1000m with 4-8 pts/m2).\n",
    "\n",
    "**THERE ARE ARGUMENTS IN THE FOLLOWING COMMAND THAT DEPEND UPON THE UNITS OF THE DATA.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "INFILE_STR = os.path.join(RAW, '*.laz')\n",
    "INFILES = glob.glob(INFILE_STR)\n",
    "ODIR = os.path.join(INTERIM, 'retiled')\n",
    "print('Retiling {:,d} tiles'.format(len(INFILES)))\n",
    "\n",
    "# do the processing\n",
    "tile_proc = las.lastile(i=INFILE_STR,\n",
    "                        tile_size=1000, # in units of lidar data\n",
    "                        buffer=50, # assumes units are in meters\n",
    "                        flag_as_withheld=True, # flag buffer points as \"withheld\"\n",
    "                        refine_tiling=POINT_CAPACITY,\n",
    "                        olaz=True,\n",
    "                        odir=ODIR,\n",
    "                        cores=NUM_CORES);\n",
    "\n",
    "print('First pass done. Now moving to adaptive tiling.')\n",
    "\n",
    "# refine the tiles to ensure no tile contains more than desired # of points\n",
    "tile_proc2 = las.lastile(i=os.path.join(ODIR, '*_1000.laz'),\n",
    "                        flag_as_withheld=True,\n",
    "                        refine_tiles=POINT_CAPACITY,\n",
    "                        olaz=True,\n",
    "                        cores=NUM_CORES);\n",
    "\n",
    "print('Done with first pass, moving onto second.')\n",
    "# refine the tiles to ensure no tile contains more than desired # of points\n",
    "tile_proc3 = las.lastile(i=os.path.join(ODIR, '*_500.laz'),\n",
    "                        flag_as_withheld=True, # flag buffer points as \"withheld\"\n",
    "                        refine_tiles=POINT_CAPACITY,\n",
    "                        olaz=True,\n",
    "                        cores=NUM_CORES);\n",
    "\n",
    "print('Done with second pass, moving onto third.')\n",
    "# refine the tiles to ensure no tile contains more than desired # of points\n",
    "tile_proc4 = las.lastile(i=os.path.join(ODIR, '*_250.laz'),\n",
    "                        flag_as_withheld=True, # flag buffer points as \"withheld\"\n",
    "                        refine_tiles=POINT_CAPACITY,\n",
    "                        olaz=True,\n",
    "                        cores=NUM_CORES);\n",
    "\n",
    "print('Done.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ODIR = os.path.join(INTERIM, 'retiled')\n",
    "tiles_1000 = ['_'.join(os.path.basename(x).split('_')[0:2]) for x in glob.glob(os.path.join(ODIR, '*_1000.laz'))]\n",
    "tiles_500 = ['_'.join(os.path.basename(x).split('_')[0:2]) for x in glob.glob(os.path.join(ODIR, '*_500.laz'))]\n",
    "tiles_250 = ['_'.join(os.path.basename(x).split('_')[0:2]) for x in glob.glob(os.path.join(ODIR, '*_250.laz'))]\n",
    "redundant_tiles = [x for x in tiles_1000 if x in tiles_500] + [x for x in tiles_1000 if x in tiles_250]\n",
    "redundant_tiles = [os.path.join(ODIR,x+'_1000.laz') for x in redundant_tiles]\n",
    "\n",
    "print('Produced {:,d} original tiles'.format(len(tiles_1000)))\n",
    "print('Removing {:,d} redundant original tiles covered by smaller tiles'.format(len(redundant_tiles)))\n",
    "\n",
    "with multiprocessing.Pool(NUM_CORES) as p:\n",
    "    p.map(os.remove, redundant_tiles)\n",
    "\n",
    "OUTFILES = glob.glob(os.path.join(ODIR, '*.laz'))\n",
    "print('Produced {:,d} new tiles, each with <= {:,d} points'.format(len(OUTFILES), POINT_CAPACITY))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
